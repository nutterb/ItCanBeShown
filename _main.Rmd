--- 
title: "It Can Be Shown"
subtitle: "Notes on Statistical Theory"
author: "Benjamin Nutter"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
bookdown::gitbook:
  config:
    toc:
      collapse: section
      scroll_highlight: true
      before: null
      after: null
documentclass: book
bibliography: Biblio.bib
biblio-style: "apalike"
link-citations: yes
github-repo: nutterb/ItCanBeShown
nocite: | 
  @Hogg2005a, @Law2000a, @Wacke2002a
description: "Notes on the Development of Statistical Theory"
---

```{r, warning = FALSE, message = FALSE, echo = FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
library(pixiedust)
library(RColorBrewer)
library(StudyPlanning)
library(tidyr)

palette <- brewer.pal(9, "PRGn")
pallette_green <- rev(brewer.pal(7, "Greens")[3:5])
```



# Introduction

There is one phrase that makes me cringe every time I see it.  It's a phrase that embodies feelings of frustration, inadequacy, and failure to understand.  That phrase:

> It can be shown

Everytime I read that phrase, I would look at the subsequent result and think "Really? It can?"

This book is a collection of notes that I've put together to avoid having to feel that way in the future.  It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory.  Most imporantly, it spells even the smallest steps along each development so that I don't have to worry about solving it again in the future.

You won't find much in the way of application.  There are no exercises.  There is only minimal explanation.  My intent is to show development of statistical theory and nothing else. 

<!--chapter:end:index.Rmd-->

# Analysis of Variance

## One-Way Design

### Decomposition of Sums of Squares

$$\begin{align*}
SS_{Total}
	&= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{++})^2 \\
	&= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+} + \bar x_{i+} -
	        x_{++})^2 \\
  &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} 
		\big[ (x_{ij} - \bar x{i+}) + (\bar x_{i+} - \bar x_{++}) \big]^2 \\
	&= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} 
		\big[ (\bar x_{i+} - \bar x_{++}) + (x_{ij} - \bar x{i+}) \big]^2 \\
  &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} 
      \big[ (\bar x_{i+} - \bar x_{++})^2
		  + 2(\bar x_{i+} - \bar x_{++})(x_{ij} - \bar x_{i+})
		  + (x_{ij} - \bar x_{i+})^2 \big] \\
  &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (\bar x_{i+} - \bar x_{++})^2
	  	+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} 2(\bar x_{i+} 
	  	- \bar x_{++})(x_{ij} - \bar x_{i+})
		  + \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
	  	+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++})
			\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})
		  + \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
  		+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++})
	  		\bigg(\sum\limits_{j=1}^{n_i} x_{ij} - \sum\limits_{j=1}^{n_i}\bar x_{i+}\bigg) 
	  	 + \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++})
			(x_{i+} - n_i \bar x_{i+})
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++})
			\big(x_{i+} - n_i \frac{x_{i+}}{n_i}\big)
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++})
			(x_{i+} - x_{i+})
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ 2\sum\limits_{i=1}^{a} n_i (\bar x_{i+} - \bar x_{++}) \cdot 0
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ 0
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2 \\
  &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2\\
\end{align*}$$

The components are commonly referred to as

$$
SS_{Factor}
	= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2 
$$

and

$$
SS_{Error}
	= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{i+})^2
$$

Notice that $SS_{Factor}$ compares the factor means to the overall mean, and it can be said that $SS_{Factor}$ measures the variation _between_ factors.  $SS_{Error}$ compares each observation to the overall mean, and can be said to describe the variation _within_ factors.

When $n_1 = n_2 = \cdots n_i = n$, the design is said to be balanced.

## Computational Formulas

$SS_{Total}$ and $SS_{Factor}$ can be simplified for convenient computation.

$$\begin{align*}
SS_{Total} 
	        &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} (x_{ij} - \bar x_{++})^2 \\
  ^{[1]}  &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{n_i} x_{ij}^2 - 
                x_{++} \sum\limits_{j=1}^{n_i}\frac{1}{n_i}\\
\end{align*}$$

> 1. See Theorem \@ref(computational-formula-population-variance)

$$\begin{align*}
SS_{Factor}
	        &= \sum\limits_{i=1}^{a} n_i(\bar x_{i+} - \bar x_{++})^2  \\
  ^{[1]}  &= \sum\limits_{i=1}^{a}\frac{\bar x_{i+}^2}{n_i} 
                - \bar x_{++} \sum\limits_{i=1}^{a}\frac{1}{n_i}
\end{align*}$$

> 1. See Theorem \@ref(computational-formula-population-variance)

$SS_{Error}$ does not simplify to a convenient form, but  
$$\begin{align*}
SS_{Total} 
                       &= SS_{Factor} + SS_{Error}  \\
\Rightarrow SS_{Error} &= SS_{Total} - SS_{Factor}
\end{align*}$$




## Randomized Complete Block Design

Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable.  To implement this design, suppose we have $a$ treatments we want to compare, and $b$ blocks.  We may analyze the data by use of the sums of squares, similar to the one-way design.

### Decomposition of Sums of Squares

$$\begin{align*}
SS_{Total}
	    &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}(x_{ij} - \bar x_{++})^2 \\
	    	    &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}(x_{ij} + \bar x_{i+} - \bar x_{i+} 
		      + \bar x_{+ j} - \bar x_{+ j} + \bar x_{++} - \bar x_{+ +} - \bar x_{++})^2 \\
      &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[ (\bar x_{i+} - \bar x_{++})
    		+ (\bar x_{+ j} - \bar x_{++})
    		+ (x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++}) \big]^2 \\
      &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[
    		(\bar x_{i+} - \bar x_{++})^2
    		+ 2(\bar x_{i+} - \bar x_{++})(\bar x_{+ j} - \bar x_{++}) \\
      &	\ \ \ \ + 2(\bar x_{i+} - \bar x_{++})(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})
    		+ (\bar x_{+ j} - \bar x_{++})^2 \\
      & \ \ \ \ + 2(\bar x_{+ j} - \bar x_{++})
                   (x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++}) \\
    	& \ \ \ \	+ (x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2 \big] \\
      &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[
    		    (\bar x_{i+} - \bar x_{++})^2
    		    + (\bar x_{+ j} - \bar x_{++})^2
    		    + (x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2 \\
    	& \ \ \ \ + 2(\bar x_{i+} - \bar x_{++})(\bar x_{+ j} - \bar x_{++})
    		    + 2(\bar x_{i+} - \bar x_{++})(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++}) \\
    	& \ \ \ \ 
            + 2(\bar x_{+ j} - \bar x_{++})(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++}) \big] \\
      ^{[1]} &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[
    		(\bar x_{i+} - \bar x_{++})^2
    		+ (\bar x_{+ j} - \bar x_{++})^2
    		+ (x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2
        + 0 + 0 + 0 \big] \\
      &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b} (\bar x_{i+} - \bar x_{++})^2
    		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b} (\bar x_{+ j} - \bar x_{++})^2
        + \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}
    			(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2 \\
      &= b \sum\limits_{i=1}^{a} (\bar x_{i+} - \bar x_{++})^2
    		+ a \sum\limits_{j=1}^{b} (\bar x_{+ j} - \bar x_{++})^2
        + \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}
    			(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2
\end{align*}$$

> 1. It is shown that the cross products are equal to zero in Section \@ref(anova-rcbd-cross-products)

These terms are commonly referred to as 
$$\begin{align*}
SS_{Factor}
	&= b \sum\limits_{i=1}^{a} (\bar x_{i+} - \bar x_{++})^2 \\
SS_{Block}
	&= a \sum\limits_{j=1}^{b} (\bar x_{+ j} - \bar x_{++})^2 \\
SS_{Error}
	&= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}
		(x_{ij} - \bar x_{i+} - \bar x_{+ j} + \bar x_{++})^2
\end{align*}$$


### Computational Formulae

$SS_{Total}$, $SS_{Factor}$, and $SS_{Block}$ can all be simplified for convenient computation.

$$\begin{align*}
SS_{Total} 
          &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}(x_{ij} - \bar x_{++})^2 \\
  ^{[1]}  &= \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b} x_{ij}^2 - \frac{x_{++}}{ab}\\
\\
SS_{Factor} 
          &= b \sum\limits_{i=1}^{a} (\bar x_{i+} - \bar x_{++})^2 \\
  ^{[1]}  &= \frac{1}{b}\sum\limits_{i=1}^{a}x_{i+}^2 - \frac{x_{++}^2}{ab} \\
\\
SS_{Block} 
          &= a \sum\limits_{j=1}^{b} (\bar x_{+ j} - \bar x_{++})^2 \\
  ^{[1]}  &= \frac{1}{a}\sum\limits_{j=1}^{b} x_{+ j}^2 - \frac{x_{++}^2}{ab}
\end{align*}$$

> 1. See Theorem \@ref(computational-formula-population-variance)

$SS_{Error}$ does not simplify to any convenient form, but may be calculated from the other terms as  
$SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}$


### RCBD Cross Products {#anova-rcbd-cross-products}

The cross products of the RCBD design

$$\begin{align*}
2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{i+}-\bar x_{++})
						(\bar x_{+ j}-\bar x{++}) & \\
\ \ \ \ + 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ j}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++}) & \\
\ \ \ \ + 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ i}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++})
	&= 0
\end{align*}$$

_Proof:_

$$
2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{i+}-\bar x_{++})
						(\bar x_{+ j}-\bar x{++}) 
	+ 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ j}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++}) \\
\ \ \ \ 	+ 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ i}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++})\\
\ \  = 2\bigg(\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{i+}-\bar x_{++})
						(\bar x_{+ j}-\bar x{++})
	+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ j}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++}) \\
\ \ \ \ 	+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}	(\bar x_{+ i}-\bar x_{++})
							(x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++})\bigg)\\
\ \  = 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[
		(\bar x_{i+}-\bar x_{++}) (\bar x_{+ j}-\bar x{++})
		+ (\bar x_{+ j}-\bar x_{++}) (x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++}) \\
\ \ \ \ 	+ (\bar x_{+ i}-\bar x_{++}) (x_{ij} + \bar x_{i+} + \bar x_{+ j} - \bar x_{++}) \big] \\
\ \  = 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\big[
		\bar x_{i+}\bar x_{+ j} - \bar x_{i+}\bar x_{++} 
			- \bar x_{+ j}\bar x_{++} + \bar x_{++}^2 \\
\ \ \ \ 	+ x_{ij}\bar x_{+ j} - \bar x_{i +}\bar x_{+ j} - \bar x_{+ j}^2 + \bar x_{+ j}\bar x_{++}
			- x_{ij}\bar x_{++} + \bar x_{i+}\bar x_{++}
			+ \bar x_{+ j}\bar x_{++} - \bar x_{++}^2 \\
\ \ \ \ 	+ x_{ij}\bar x_{+ j} - \bar x_{i +}^2 - \bar x_{i+}\bar x_{+ j} + \bar x_{+ j}\bar x_{++}
			- x_{ij}\bar x_{++} + \bar x_{i+}\bar x_{++}
			+ \bar x_{+ j}\bar x_{++} - \bar x_{++}^2 \big] \\
\ \  = 2\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}(
		-\bar x_{++}^2 - \bar x_{i+}^2 - \bar x_{+ j}^2
		+ x_{ij}\bar x_{i+} + x_{ij}\bar x_{+ j} - 2 x_{ij}\bar x_{++} - \bar x_{i+}\bar x_{+ j} \\
\ \ \ \ 	+ 2\bar x_{i+}\bar x_{++} + 2\bar x_{+ j}\bar x_{++} ) \\
\ \ = 2\bigg(-\sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{++}^2
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{i+}^2
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}x_{ij}\bar x_{i+}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}x_{ij}\bar x_{+ j} \\ 
\ \ \ \	- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2 x_{ij}\bar x_{++} 
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{i+}\bar x_{+ j}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{i+}\bar x_{++} 
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg) \\
\ \  = 2\bigg( \frac{ab\bar x_{++}^2}{a^2b^2}
		- \frac{b}{b^2}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{a}{a^2}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}x_{ij}\bar x_{i+}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}x_{ij}\bar x_{+ j}\\ 
\ \ \ \ 	- \frac{2\bar x{++}^2}{ab} 
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{i+}\bar x_{+ j}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{i+}\bar x_{++} 
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg)\\
\ \ ^{[1]} =2\bigg( \frac{\bar x_{++}^2}{ab}
		- \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}x_{ij}\bar x_{+ j}\\ 
\ \ \ \ 	- \frac{2\bar x{++}^2}{ab} 
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{i+}\bar x_{+ j}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{i+}\bar x_{++} 
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg)\\
\ \ ^{[2]} = 2\bigg( \frac{\bar x_{++}^2}{ab}
		- \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2\\ 
\ \ \ \ 	- \frac{2\bar x{++}^2}{ab} 
		- \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}\bar x_{i+}\bar x_{+ j}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{i+}\bar x_{++} 
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg)\\
\ \ ^{[3]} = 2\bigg( \frac{\bar x_{++}^2}{ab}
		- \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2\\ 
\ \ \ \ 	- \frac{2\bar x{++}^2}{ab} 
		- \frac{\bar x_{++}}{ab}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{i+}\bar x_{++} 
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg) \\
\ \ ^{[4]} = 2\bigg( \frac{\bar x_{++}^2}{ab}
		- \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
$$

$$
\ \ \ \ - \frac{2\bar x{++}^2}{ab} 
		- \frac{\bar x_{++}}{ab}
		+ \frac{2\bar x_{++}^2}{ab}
		+ \sum\limits_{i=1}^{a}\sum\limits_{j=1}^{b}2\bar x_{+ j}\bar x_{++} \bigg)\\
\ \ ^{[5]} = 2\bigg( \frac{\bar x_{++}^2}{ab}
		- \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		- \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2\\ 
\ \ \ \ 	- \frac{2\bar x{++}^2}{ab} 
		- \frac{\bar x_{++}}{ab}
		+ \frac{2\bar x_{++}^2}{ab}
		+ \frac{2\bar x_{++}^2}{ab} \bigg)\\
\ \  = 2\bigg(\frac{4\bar x_{++}^2}{ab} - \frac{4\bar x_{++}^2}{ab}
		+ \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2 - \frac{1}{b}\sum\limits_{i=1}^{a}\bar x_{i+}^2
		+ \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2 - \frac{1}{a}\sum\limits_{j=1}^{b}\bar x_{+ j}^2 \bigg)\\
\ \  = 2(0 + 0 + 0) \\
	= 2(0) \\
	= 0 
$$

> 1. See Summation Theorem \@ref(summation-theorem-7)
> 2. See Summation Theorem \@ref(summation-theorem-8)
> 3. See Summation Theorem \@ref(summation-theorem-4)
> 4. See Summation Theorem \@ref(summation-theorem-5)
> 5. See Summation Theorem \@ref(summation-theorem-6)

Using the theorems in Chapter \@ref{summation-chapter} it is can be shown that each of the three cross products is equal to zero.  However, the physical tedium of reducing each cross product is much greater than the approach taken above.

<!--chapter:end:ANOVA.Rmd-->

# Bernoulli Distribution

## Probability Mass Function

A random variable is said to have a Bernoulli Distribution with parameter $p$ if its probability mass function is:
\[p(x)=\left\{ 
		\begin{array}{ll}
			\pi^x(1-\pi)^{1-x},	& x=0,1\\
			0 		& \mathrm{otherwise}
		\end{array} 
	\right. \]
Where  $\pi$ is the probability of a success.

## Cumulative Mass Function

\label{Bernoulli1}
\[P(x)=\left\{
		\begin{array}{lll}
			0   & x<0\\
			1-\pi & x=0\\
			1   & 1\leq x
		\end{array}
	\right. \]
	

```{r Bernoulli_Distribution, echo = FALSE, fig.path= 'figures/', fig.cap = 'The graphs on the left and right show a Bernoulli Probability Distribution and Cumulative Distribution Function, respectively, with $\\pi=.4$.  Note that this is identical to a Binomial Distribution with parameters $n=1$ and $\\pi=.4$.'}
Bernoulli <- 
  data.frame(x = 0:1) %>%
  mutate(pmf = dbinom(x, size = 1, p = 0.4),
         cmf = pbinom(x, size = 1, p = 0.4)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             c("pmf", "cmf"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Bernoulli,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob), 
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:1) + 
  ylab("P(x)") + 
  theme_bw()
```

## Expected Values

$$ 
\begin{align*}
E(X) 
  &= \sum\limits_{i=0}^{1} x\cdot p(x) \\
	&= \sum\limits_{i=0}^{1} x \cdot \pi^{x} (1-\pi)^{1-x}\\
  &= 0 \cdot \pi^{0} (1-\pi)^{1-0} + 1 \cdot \pi^{1} (1-\pi)^{1-1}\\
  &= 0 + \pi (1-\pi)^{0}\\
	&= \pi\\
	\\
	\\
E(X^{2}) 
  &= \sum\limits_{i=0}^{1} x^2 \cdot p(x)\\
  &= \sum\limits_{i=0}^{1} x^{2} \cdot \pi^x (1-\pi)^{1-x}\\
  &= \sum\limits_{i=0}^{1} 0^{2} \cdot \pi^0 (1-\pi)^{1-0} + 1^2 \cdot \pi^1 (1-\pi)^{1-1}\\
  &= 0 \cdot 1 \cdot 1 + 1 \cdot \pi \cdot 1 \\
	&= 0 + \pi\\
 	&= \pi\\
 	\\
 	\\
\mu	&= E(X) = \pi\\
  \\
  \\
\sigma^2 
  &= E(X^2) - E(X)^2 \\
  &= \pi-\pi^2 \\
  &= \pi(1-\pi)
\end{align*}
$$

## Moment Generating Function

$$\begin{align*}
M_{X}(t)
	&= E(e^{tX}) \\
	&= \sum\limits_{i=0}^{1} e^{tx} p(x) \\
	&= \sum\limits_{i=0}^{1} e^{tx} \pi^{x} (1-\pi)^{1-x}\\
	&= e^{t0} \pi^0 (1-\pi)^{1-0} + e^t \pi^t (1-\pi)^{1-1}\\
	&= (1-\pi) + e^t \pi\\
	&=\pi e^t + (1-\pi) \\
	\\
	\\
M^{(1)}_X(t) &= \pi e^t\\
  \\
  \\
M^{(2)}_X(t) &= \pi e^t\\
  \\
  \\
E(X)
	&=M^{(1)}_X(0)\\
	&= \pi e^0\\
	&= \pi e^0\\
	&= \pi\\
	\\
	\\
E(X^2)
	&= M^{(2)}_X(0)\\
	&= \pi e^0\\
	&= \pi\\
	\\
	\\
\mu
  &= E(X)\\
	&= \pi\\
  \\
  \\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \pi - \pi^2 \\
	&= \pi (1-\pi)
\end{align*}
$$


## Theorems for the Bernoulli Distribution

### Validity of the Distribution

$$\sum\limits_{x=0}^{1}\pi^x(1-\pi)^{1-x}=1$$

_Proof:_

$$\begin{align*}
\sum\limits_{x=0}^{1} \pi^x (1-\pi)^{1-x}
	&= \pi^0 (1-\pi)^1 + \pi^1 (1-\pi)^0 \\
	&= (1-\pi) + \pi \\
	&= 1
\end{align*}$$


### Sum of Bernoulli Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.\\
Then $Y\sim$ Binomial$(n,\pi)$

_Proof:_
$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_n}) \\
	&= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n}) \\
  &= (\pi e^t+(1-\pi)) (\pi e^t+(1-\pi)) \cdots (\pi e^t+(1-\pi)) \\
	&= (\pi e^t+(1-\pi))^n
\end{align*}$$

Which is the moment generating function of a Binomial random variable with parameters $n$ and $\pi$.  Thus, $Y\sim$ Binomial$(n,\pi)$. 

<!--chapter:end:Bernoulli_Distribution.Rmd-->

# Binomial Distribution

## Probability Mass Function
A random variable is said to follow a Binomial distribution with parameters $n$ and $\pi$ if its probability mass function is:

\[p(x)=
	\left\{
		\begin{array}{ll}
			{n \choose x} \pi^x (1-\pi)^{n-x},	& x=0,1,2,\ldots,n\\
			0 				& \mathrm{otherwise}
		\end{array}
	\right. \]
	
Where $n$ is the number of trials performed and $\pi$ is the probability of a success on each individual trial.

## Cumulative Mass Function

\[ P(x)=
	\left\{
		\begin{array} {lll}
			0							& x<0\\
			\sum\limits_{i=0}^{x} {n \choose i} \pi^i (1-\pi)^{n-i} 	& 0 \leq x=0,1,2,\ldots,n\\
			1 							& n\leq x
		\end{array}
	\right. \]
	
A recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:

$$\begin{align*} 
F(x+1)
	&= {n\choose x+1} \pi^{x+1} (1-\pi)^{n-(x+1)} \\
	&= \frac{n!}{(x+1)!(n-(x+1))!} \pi^{x+1} (1-\pi)^{n-(x+1)} \\
  &= \frac{n!}{(x+1)!(n-x-1)!} \pi^{x+1} (1-\pi)^{n-x-1} \\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} \pi \cdot \pi^x \frac{(1-\pi)^{n-x}}{(1-\pi)} \\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)!} \cdot \frac{\pi}{1-\pi} \pi^x (1-\pi)^{n-x} \\
  &= \frac{\pi}{1-\pi} \cdot \frac{n-x}{x+1} \cdot \frac{n!}{x!(n-x)!} \pi^x (1-\pi)^{n-x} \\
  &= \frac{\pi}{1-\pi} \cdot \frac{n-x}{x+1} \cdot {n\choose x} \pi^x (1-\pi)^{n-x} \\
	&= \frac{\pi}{1-\pi} \cdot \frac{n-x}{x+1} \cdot F(x)
\end{align*}$$

```{r Binomial_Distribution, echo = FALSE, fig.path = 'figures/', fig.cap = 'The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $n=10$ and $\\pi=.4$.'}
Binomial <- 
  data.frame(x = 0:10) %>%
  mutate(dbinom = dbinom(x, size = 10, p = 0.4),
         pbinom = pbinom(x, size = 10, p = 0.4)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             levels = c("dbinom", "pbinom"),
                             labels = c("Probability Mass", "Cumulative Mass")))

ggplot(data = Binomial,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob),
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:10) + 
  ylab("P(x)") + 
  theme_bw()
```

## Expected Values
$$\begin{align*}
E(X)
    	    &= \sum\limits_{x=0}^n x \cdot p(x) \\
    	    &= \sum\limits_{x=0}^n x {n\choose x} \pi^x (1-\pi)^{n-x} \\
	^{[1]}  &= \sum\limits_{x=0}^n x {n\choose x} \pi^x q^{n-x} \\
          &= 0 \cdot {n\choose 0}\pi^0q^n+1 \cdot {n\choose 1}\pi^1q^{n-1} 
    		        + \cdots + n{n\choose n}\pi^nq^{n-n}\\
     	    &= 0 + 1{n\choose 1}\pi^1q^{n-1} + 2{n\choose 2}\pi^2q^{n-2} 
    		        + \cdots + n{n\choose n}\pi^nq^{n-n}\\
    	    &= n\pi^1 q^{n-1} + n(n-1)\pi^2q^{n-2} + \cdots + n(n-1)\pi^{n-1}q^{n-(n-1)} + n \pi^n\\
    	    &= n\pi [q^{n-1} + (n-1)\pi q^{n-2} + \cdots + \pi^{n-1}]\\
        	&= n\pi \Big[{n-1\choose 0}\pi^0q^{n-1} + {n-1\choose 1}\pi^1q^{(n-1)-1}
        		+ \cdots + {n-1\choose n-1}\pi^{n-1}q^{(n-1)-(n-1)}\Big]\\
        	&= n\pi (\sum\limits_{x=0}^{n-1}{n-1\choose x}\pi^xq^{(n-1)-x}) \\
  ^{[2]}	&= n\pi(\pi+q)^{n-1} \\
  ^{[1]}	&= n\pi(\pi+(1-\pi))^{n-1} \\
    	    &= n\pi(\pi+1-\pi)^{n-1} \\
        	&= n\pi(1)^{n-1} \\
        	&= n\pi(1) \\
        	&= n\pi
\end{align*}$$

> 1. Let $q = (1 - \pi)$
> 2. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$

$$\begin{align*}
E(X^2)
	        &= \sum\limits_{x=0}^{n} x^2 p(x) \\
	        &= \sum\limits_{x=0}^{n} x^2 {n\choose x} \pi^x (1-\pi)^{n-x} \\
	^{[1]}  &= \sum\limits_{x=0}^{n} x^2 {n\choose x} \pi^x q^{n-x} \\
          &= 0^2 \frac{n!}{0!(n-0)!} \pi^0q^n + 1^2 \frac{n!}{1!(n-1)!} \pi^1q^{n-1}
		          + \cdots + n^2 \frac{n!}{n!(n-n)!} \pi^nq^{n-n} \\
        	&= 0 + 1 \frac{n!}{(n-1)!} \pi q^{n-1} + 2 \frac{n!}{1\cdot(n-2)!} \pi^2q^{n-2}
        		+ \cdots + n \frac{n!}{(n-1)!(n-n)!} \pi^n \\
          &= n\pi \Big[1 \frac{(n-1)!}{(n-1)!} \pi^0q^{n-1}
        		+ 2 \frac{(n-1)!}{1(n-2)!} \pi^2q^{n-2}
        		+ \cdots + n \frac{(n-1)!}{(n-1)!(n-n)!} \pi^{n-1}\Big] \\
          &= n\pi \Big[1 \frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} \pi^{1-1} q^{n-1} + 
                 	\cdots + n \frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} \pi^{n-1} q^{(n-1)-(n-1)}\Big] \\
          &= n\pi \sum\limits_{x=1}^{n} x {n-1\choose x-1} \pi^{x-1}1^{(n-1)-(x-1)} \\
  ^{[2]}  &= \sum\limits_{y=0}^{m} (y+1) {m \choose y} \pi^yq^{m-y} \\
          &= n\pi \Big[ \sum\limits_{y=0}^{m} y {m \choose y} \pi^yq^{m-y} + {m \choose y} \pi^yq^{m-y}\Big] \\
          &= n\pi \Big[ \sum\limits_{y=0}^{m} y {m \choose y} \pi^yq^{m-y} 
          		+ \sum\limits_{y=0}^{m} {m \choose y} \pi^yq^{m-y}\Big] \\
  ^{[3]}  &= n\pi(m\pi+1) \\
        	&= n\pi[(n-1)\pi+1] \\
        	&=n\pi(n\pi-\pi+1) \\
        	&=n^2\pi^2 - n\pi^2 + n\pi
\end{align*}$$
	
>1. $q = (1 - \pi)$
>2. Let $y = x - 1$ and $n = m + 1$  
>    $\Rightarrow$ $x = y + 1$ and $m = n - 1$
>3. $\sum\limits_{y=0}^{m}y{m \choose y}\pi^yq^{m-y}$
>		is of the form of the expected value of $Y$, and $E(Y)=m\pi=(n-1)\pi$.
>		$\sum\limits_{y=0}^{m}{m \choose y}\pi^yq^{m-y}$
>		is the sum of all probabilities over the domain of $Y$ which is 1.


$$\begin{align*}
\mu
	&= E(X) \\
	&= n\pi \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= n^2\pi^2 - n\pi^2 + n\pi - n^2\pi^2 \\
	&= -n\pi^2 + n\pi \\
	&= n\pi(-\pi-1) \\
  &= n\pi(1-\pi)
\end{align*}$$

## Moment Generating Function

$$
\begin{align*} 
M_X(t)
          &= E(e^{tX})=\sum\limits_{x=0}^{n}e^{tx}p(x) \\
        	&= \sum\limits_{x=0}^{n}e^{tx}{n\choose x}\pi^x(1-\pi)^{n-x} \\
          &= \sum\limits_{x=0}^{n}{n\choose x}e^{tx}\pi^x(1-\pi)^{n-x} \\
	        &= \sum\limits_{x=0}^{n}{n\choose x}(\pi e^{tx})^x(1-\pi)^{n-x} \\
  ^{[1]}  &= [(1-\pi)+\pi e^t]^n
\end{align*}
$$
	
> 1. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$

$$
\begin{align*}
M_X^{(1)}(t) &= n[(1 - \pi) + \pi e^t] ^ {n - 1} \pi e^t\\
\\
M_X^{(2)}(t) &= n[(1-\pi) + \pi e^t] ^ {n-1} \pi e^t + n(n-1)[(1-\pi) + \pi e^t] ^ {n-2}(\pi e^t)^2\\
             &= n\pi e^t[(1-\pi) + \pi e^t] ^ {n-1} + n(n-1)\pi e^{2t}[(1-\pi) + \pi e^t] ^ {n-2}\\
\\
\\
E(X)
  &= M_X^{(1)}(0) \\
  &= n[(1-\pi)+\pi e^0]^{n-1}\pi e^0 \\
  &= n[1-\pi+\pi^{n-1}\pi\\
  &= n(1)^{n-1}\pi
  &= n\pi\\
\\
\\
E(X^2)
  &= M_X^{(2)}(0) \\
  &= n\pi e^0 [(1-\pi) + \pi e^0]^{n-1} + n(n-2) \pi e^{2\cdot0}[(1-\pi) + \pi e^0]^{n-2} \\
  &= n\pi(1-\pi+\pi)^{n-2}+n(n-1)\pi^2(1-\pi+\pi^{n-2} \\
  &= n\pi (1)^{n-1} + n(n-1) \pi^2 (1)^{n-2} \\
  &= n\pi+n(n-1)\pi^2 \\
  &= n\pi+(n^2-n)\pi^2 \\
  &= n\pi + n^2 + n^2\pi^2 - n\pi^2 \\
\\
\\
\mu
  &= E(X) \\
  &= n\pi \\
\\
\\
\sigma^2
  &= E(X^2) - E(X)^2 \\
  &= n\pi + n^2\pi^2 - n\pi^2 - n^2\pi^2 \\
  &= n\pi - n\pi^2\\
  &= n\pi(1-\pi)
\end{align*}$$



## Maximum Likelihood Estimator

Since $n$ is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for $n$.  The mean and variance can both be estimated from the single parameter $\pi$.

Let $X$ be a Binomial random variable with parameter $\pi$ and $n$ outcomes $(x_1,x_2,\ldots,x_n)$.  Let $x_i=0$ for a failure and $x_i=1$ for a success.  In other words, $X$ is the sum of $n$ Bernoulli trials with equal probability of success and $X=\sum\limits_{i=1}^{n}x_i$.

### Likelihood Function
$$\begin{align*} 
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta) \\
	&= P(x_1|\theta) P(x_2|\theta) \cdots P(x_n|\theta) \\
  &= [\theta^{x_1}(1-\theta)^{1-x_1}] [\theta^{x_2}(1-\theta)^{1-x_2}] \cdots
        [\theta^{x_n}(1-\theta)^{1-x_n}]\\
  &= \exp_\theta\bigg\{\sum\limits_{i=1}^{n}x_i\bigg\}
        \exp_{(1-\theta)}\bigg\{n-\sum\limits_{i=1}^{n}x_i\bigg\} \\
  &= \theta^X(1-\theta)^{n-X}
\end{align*}$$

### Log-likelihood Function
$$\begin{align*}
\ell(\theta)
	&= \ln L(\theta) \\
	&= \ln\big(\theta^X(1-\theta)^{n-X}\big) \\
	&= X\ln(\theta)+(n-X)\ln(1-\theta)
\end{align*}$$

### MLE for p

$$\begin{align*} 
                \frac{d\ell(p)}{d \pi} 
                            &= \frac{X}{\pi}-\frac{n-X}{1-\pi} \\
\\
\\
                       0   &= \frac{X}{\pi}-\frac{n-X}{1-\pi} \\
  \Rightarrow  \frac{X}{\pi} &= \frac{n-X}{1-\pi} \\
  \Rightarrow  (1-\pi)X      &= \pi(n-X) \\
  \Rightarrow  X-\pi X        &= n\pi-\pi X \\
  \Rightarrow  X           &= n\pi \\
  \Rightarrow  \frac{X}{n} &= \pi \\
\end{align*}$$

So $\displaystyle \hat p = \frac{X}{n} = \frac{1}{n}\sum\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $\pi$.


## Theorems for the Binomial Distribution

### Validity of the Distribution

$$\begin{align*}
\sum\limits_{x=0}^n{n\choose x}p^x(1-p)^{n-x}	= 1
\end{align*}$$

_Proof:_

$$\begin{align*}
\sum\limits_{x=0}^n {n\choose x} \pi^x (1-\pi)^{n-x} \\
	^{[1]}  &= \big(\pi + (1-\pi)\big)^n \\
	        &= (1)^n \\
	        &= 1 
\end{align*}$$

> 1. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$


### Sum of Binomial Random Variables

Let $X_1,X_2,\ldots,X_k$ be independent random variables where $X_i$ comes from a Binomial distribution with parameters $n_i$ and $\pi$.  That is $X_i\sim(n_i,\pi)$.

Let $Y = \sum\limits_{i=1}{k} X_i$.  Then $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,\pi)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{t(x_1 + X_2 + \cdots + X_k)} \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_k}) \\ 
  &= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_k}) \\
  &= \prod\limits_{i=1}^{k} [(1-\pi)+\pi e^t]^{n_i} \\
	&= [(1-\pi)+\pi e^t]^{\sum\limits_{i=1}^{k}n_i}
\end{align*}$$

Which is the mgf of a Binomial random variable with parameters $\sum\limits_{i=1}^{k}n_i$ and $\pi$.  
Thus $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,\pi)$.

### Sum of Bernoulli Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $\pi$.  Let $Y = \sum\limits_{i=1}^{n}X_i$.  
Then $Y\sim$Binomial$(n,\pi)$

_Proof:_
$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_n}) \\
	&= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n}) \\
  &= (\pi e^t+(1-\pi))(\pi e^t+(1-\pi))\cdots (\pi e^t+(1-\pi)) \\
	&= (\pi e^t+(1-\pi))^n
\end{align*}$$

Which is the mgf of a Binomial random variable with parameters $n$ and $p$.  Thus, $Y\sim$ Binomial$(n,p)$. 

<!--chapter:end:Binomial_Distribution.Rmd-->

# Binomial Test

## Binomial Test

The binomial test is used to look for evidence that the proportion of a Binomial distributed random variable may differ from a hypothesized (or previously observed) value.

### Test Statistic

The test statistic for a binomial test is the observed frequency of experimental subjects that exhibit the trait of interest.  

### Definitions

Let $X$ be a random variable following a binomial distribution with parameters $n$ and $\pi$. Let $x$ be the observed frequency of experimental subjects exhibiting the trait of interest. 

### Hypotheses

The hypotheses for the Binomial test may take the following forms:

For a two-sided test:

$$\begin{align}
H_0: \pi = \pi_0 \\
H_a: \pi \neq \pi_0
\end{align}$$

For a one-sided test:

$$\begin{align}
H_0: \pi < \pi_0 \\
H_a: \pi \geq \pi_0
\end{align}$$

or 

$$\begin{align}
H_0: \pi > \pi_0 \\
H_a: \pi \leq \pi_0
\end{align}$$

### Decision Rule

The decision to reject the null hypothesis is made when the observed value of $x$ lies in the critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for $\alpha$, the Type I Error.

In a two-sided test, the upper bound is shared equally in both tails. Due to the discrete nature of the distribution, the total probability in the tails may not equal $\alpha$. The figures below depict examples of rejection regions for selected values of the Binomial distribution parameters. The decision rule is:

Reject $H_0$ if $x < Binomial(\alpha/2, n, \pi_0)$ or $x > Binomial(1 - \alpha/2, n, \pi_0)$

```{r, echo = FALSE, fig.path = 'figures/', fig.cap='The examples displayed use $n = 20$. For the top, middle, and bottom examples, $\\pi$ is set at 0.3, 0.5, and 0.75, respectively. Notice that in some cases, the rejection regions for $\\alpha = 0.10$ and $\\alpha = 0.05$ are identical.'}
DF <- 
  bind_rows(
    data.frame(x = 0:20,
               n = 20,
               p = .3),
    data.frame(x = 0:20,
               n = 20,
               p = .5),
    data.frame(x = 0:20,
               n = 20,
               p = .75)
  ) %>%
  mutate(dbinom = dbinom(x, n, p),
         pbinom = pbinom(x, n, p),
         reject_region = ifelse(pbinom < 0.005 | pbinom > 0.995,
                                "0.01",
                                ifelse(pbinom < 0.025 | pbinom > 0.975,
                                       "0.05",
                                       ifelse(pbinom < 0.05 | pbinom > .95,
                                              "0.10",
                                              "non-reject"))))

ggplot(data = DF,
       mapping = aes(x = x,
                     y = dbinom)) + 
  geom_bar(data = filter(DF, reject_region %in% "non-reject"),
           stat = "identity",
           color = "black",
           fill = "lightgray") + 
  geom_bar(data = filter(DF, !reject_region %in% "non-reject"),
           mapping = aes(fill = reject_region),
           stat = "identity",
           color = "black") +
  scale_fill_manual(values = rev(pallette_green)) + 
  labs(fill = "alpha") + 
  ylab("Probability") + 
  facet_grid(p ~ .)
```

In the one-sided test, $\alpha$ is placed in only one tail. The figures below depict examples of rejection regions for selected values of the Binomial distribution parameters. In each case, $\alpha$ is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is:

Reject $H_0$ when $x \leq Binomial(\alpha, n, \pi_0)$

For an upper tailed test, the decision rule is:

Reject $H_0$ when $x \geq Binomial(1 - \alpha, n, \pi_0)$

```{r, echo = FALSE, fig.path = 'figures/', fig.cap='The examples displayed use $n = 20$. For the top, middle, and bottom examples, $\\pi$ is set at 0.3, 0.5, and 0.75, respectively.'}
DF <- 
  bind_rows(
    data.frame(x = 0:20,
               n = 20,
               p = .3,
               side = "Left Tailed",
               stringsAsFactors = FALSE),
    data.frame(x = 0:20,
               n = 20,
               p = .5,
               side = "Left Tailed",
               stringsAsFactors = FALSE),
    data.frame(x = 0:20,
               n = 20,
               p = .75,
               side = "Left Tailed",
               stringsAsFactors = FALSE),
    data.frame(x = 0:20,
               n = 20,
               p = .3,
               side = "Right Tailed",
               stringsAsFactors = FALSE),
    data.frame(x = 0:20,
               n = 20,
               p = .5,
               side = "Right Tailed",
               stringsAsFactors = FALSE),
    data.frame(x = 0:20,
               n = 20,
               p = .75,
               side = "Right Tailed",
               stringsAsFactors = FALSE)
  ) %>%
  mutate(dbinom = dbinom(x, n, p),
         pbinom = pbinom(x, n, p),
         reject_region = ifelse(side == "Left Tailed",
                                ifelse(pbinom < 0.01,
                                       "0.01",
                                       ifelse(pbinom < 0.05,
                                              "0.05",
                                              ifelse(pbinom < 0.10,
                                                     "0.10",
                                                     "non-reject"))),
                                ifelse(pbinom > 0.99,
                                       "0.01",
                                       ifelse(pbinom > 0.95,
                                              "0.05",
                                              ifelse(pbinom > 0.90,
                                                     "0.10",
                                                     "non-reject")))))

ggplot(data = DF,
       mapping = aes(x = x,
                     y = dbinom)) + 
  geom_bar(data = filter(DF, reject_region %in% "non-reject"),
           stat = "identity",
           color = "black",
           fill = "lightgray") + 
  geom_bar(data = filter(DF, !reject_region %in% "non-reject"),
           mapping = aes(fill = reject_region),
           stat = "identity",
           color = "black") +
  scale_fill_manual(values = rev(pallette_green)) + 
  labs(fill = "alpha") + 
  ylab("Probability") + 
  facet_grid(p ~ side)
```

### Power

The derivations below make use of the following symbols:

* $x$: The observed frequency of experimental units exhibiting the trait of interest.
* $n$: The total number of experimental units.
* $\pi_0$: The proportion of the population that exhibits the trait of interest under the null hypothesis.
* $\pi_a$: The proportion of the population that exhibits the trait of interest under the alternative hypothesis.
* $\alpha$: The significance level.
* $\gamma(\pi)$: The power of the test for the parameter $\pi$.
* $Binomial(\alpha, n, \pi)$: A quantile of the Binomial distribution with a probability $\alpha$, and parameters $n$ and $\pi$.
* $C$: The critical region.

**Two Sided Test**

$$\begin{align}
\gamma(\pi_a) &= P_{\pi_a}(x \in C) \\
 &= P_{\pi_a}(Binomial(\alpha/2, n, \pi_0) \leq Binomial(\alpha / 2, n, \pi_a)) + \\
 & \ \ \ \ \ \ \ P_{\pi_a}(Binomial(1 - \alpha / 2, n, \pi_0) \geq Binomial(1 - \alpha / 2, n, \pi_a))
\end{align}$$

**Left Sided Test**

$$\begin{align}
\gamma(\pi_a) &= P_{\pi_a}(x \in C) \\
 &= P_{\pi_a}(Binomial(\alpha, n, \pi_0) \leq Binomial(\alpha, n, \pi_a)) 
\end{align}$$

**Right Sided Test**

$$\begin{align}
\gamma(\pi_a) &= P_{\pi_a}(x \in C) \\
 &= P_{\pi_a}(Binomial(1 - \alpha, n, \pi_0) \geq Binomial(1 - \alpha, n, \pi_a)) 
\end{align}$$

Since the Binomial distribution is discrete, the power curve has the interesting characteristic of not being monotonic.  It is sometimes described as having a "sawtooth" appearance. This behavior means that a larger sample size is not always preferred.  For example, in the following figure, a sample size of 10 has better power than a sample size of 12.

```{r, echo = FALSE, fig.path = 'figures/', fig.cap='Power for a Binomial test with $\\pi_0 = .15$ and $\\pi_a = 0.25$'}
BinomialPower <- 
  test_binomial(n = 2:30, 
                p0 = .15, 
                p1 = .25, 
                alpha= .05, 
                power = NULL,
                tail = 'right')

ggplot(data = BinomialPower,
       mapping = aes(x = n,
                     y = power)) + 
  geom_line(color = pallette_green[1]) + 
  ylab("Power") + 
  xlab("Sample Size")
```

<!--chapter:end:Binomial_Test.Rmd-->

# Binomial Theorem

The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions.  Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach.  Other useful theorems are provided at the end of this chapter.

## Traditional Proof

### Lemma: Pascal's rule

Let $n$ and $x$ be non-negative integers such that $x\leq n$.

Then ${n-1\choose x} + {n-1\choose x-1} = {n\choose x}$.

_Proof:_

$$\begin{align*}
{n-1\choose x} + {n-1\choose x-1}
	&= \frac{(n-1)!}{x!(n-1-x)!} + \frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\
  &= \frac{(n-1)!}{x!(n-x-1)!} + \frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\
  &= \frac{(n-1)!}{x!(n-x-1)!} + \frac{(n-1)!}{(x-1)!(n-x)!}\\
  &= \frac{(n-1)!}{x(x-1)!(n-x-1)!} + \frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\
  &= \frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!}
	  	+\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\
  &= \frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\
	&= \frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\
  &= \frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\
	&= \frac{n!}{x!(n-x)!} \\
	&= {n\choose x}
\end{align*}$$


### The Binomial Theorem {#binomial-theorem-traditional}

Let $a$ and $b$ be constants and let $n$ be any positive integer.  Then

$$(a+b)^n = \sum\limits_{x=0}^{n} {n\choose x} a^{n-x} b^x$$

_Proof:_

This proof is completed by mathematical induction.

Base Step: $n=1$

$$\begin{align*}
(a+b)^1
	&= \sum\limits_{x=0}^{1} {1\choose x} a^{1-x} b^x \\
	&= {1\choose 0} a^{1-0} b^0 + {1\choose 1} a^{1-1} b^1 \\
	&= 1\cdot a\cdot 1 + 1\cdot 1\cdot b \\
	&= a+b
\end{align*}$$

Inductive Step: Assume that the Theorem holds for $n$, and show it is true for $n+1$.

$$\begin{align*}
(a+b)^{n+1}
      	&= (a+b)(a+b)^n \\
      	&= a(a+b)^n + b(a+b)^n \\
        &= a(a^n + \sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x}b^x + b^n)
      		+ b(a^n + \sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x}b^x+b^n) \\
        &= (a^{n+1}+a\sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x}ab^x)
      		+ (a^nb+\sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x}b^x+b^{n+1}) \\
        &= (a^{n+1}+\sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x+1}ab^x)
      		+ (a^nb+\sum\limits_{x=1}^{n-1}{n\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\
^{[1]}  &= (a^{n+1}+\sum\limits_{x=1}^{n}a^{n-x+1}b^x)
	      	+ (\sum\limits_{x=0}^{n-1}{n\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\
^{[2]}  &= (a^{n+1}+\sum\limits_{x=1}^{n}{n\choose x}a^{n-x+1}b^x)
	      	+ \sum\limits_{x-1}^{n-1}{n\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\
^{[3]}  &= a^{n+1} + \sum\limits_{x+1}^{n}{n+1\choose x}a^{n-x+1}b^x + b^{n+1} \\
        &=a^{n+1}+\sum\limits_{x=1}^{n}{n+1\choose x}a^{(n+1)-x}b^x+b^{n+1} \\
^{[4]}	&= \sum\limits_{x=0}^{n+1}{n+1\choose x}a^{(n+1)-x}b^x
\end{align*}$$

This completes both the inductive step and the proof.

> 1. $ab^n={n\choose n}a^{n-n+1}b^n$ which is the term for $x=n$ in the first summation.  
>    $a^nb={n\choose 0}a^{n-0}b^1$ which is the term for $x=0$ in the second summation.
> 2. $\sum\limits_{x=0}^{n-1}{n\choose x}a^{n-x}b^{x+1} \\
      \ \ \ \ = \sum\limits_{x=1}^{n}{n\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\
      \ \ \ \ = \sum\limits_{x=1}^{n}{n\choose x-1}a^{n-x+1}b^x$
> 3. This step is made using Pascal's Rule with $n=n-1$.
> 4. $a^{n+1}={n+1\choose 0}a^{(n+1)-0}b^0$ which is the term for $x=0$ in the summation.  
>		 $\ \ b^{n+1}={n+1\choose n+1}a^{(n+1)-(n+1)}b^{n+1}$ which is the term for $x=n+1$ in the summation
      

## General Approach

### A Binomial Expansion Theorem

This theorem and its corrolary are provided by Brunette [@Bruneta].

For any positive integer $n$, let $B_n = (x_1+y_1) (x_2+y_2) \cdots (x_n+y_n)$.  In the expansion $B_n$, before combining possible like terms, the following are true:

i. There will be $2^n$ terms.
ii. Each of these terms will be a product of $n$ factors.
iii.  In each such product there will be one factor from each binomial (in $B_n$).
iv. Every such product of $n$ factors, one from each binomial, is represented in the expansion.

_Proof:_

Proof is done by induction.

For the case $n=1$, the result is clear.

Now assume that the theorem is true for a particular $n$ and consider $B_{n+1}$.

$$ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} $$

By the inductive assumption, $B_n = T_1 + T_2 + \cdots + T_{2^n}$ where each $T_i$ is a product of $n$ factors, one factor from each binomial.  It follows that every term in the expansion of $B_n+1$ is either of the type $T_ix_{n+1}$ or $T_iy_{n+1}$, for some $1\leq i \leq 2^n$.  But each term of either of the above types is clearly a product of $n+1$ factors with one factor coming from each binomial.  thus, if _(ii)_ and _(iii)_ are true for $B_n$, then they are true for $B_n+1$.

Next, by the inductive assumption, the expansion of $B_n$ is a sum of $2^n+2^n$ terms, i.e., $2^{n+1}$ terms.  This completes the inductive step for _(i)_.

Lastly, it remains for us to consider a product of the type $p_1 p_2 \cdots p_n p_{n+1}$ where, for each $1\leq i\leq n+1$, $p_i = x_i$ or $p_i = y_i$.  By the inductive hypothesis, $p_1 p_2 \cdots p_n$ is a term in the expansion of $B_n$.  If $p_{n+1} = x_{n+1}$, then $p_1 p_2 \cdots p_n p_{n+1}$ is a term in the expansion of $B_nx_{n+1}$, and so of $B_{n+1}$.  Likewise, if $p_{n+1}=y_{n+1}$, then $p_1 p_2 \cdots p_n p_{n+1}$ is a term in the expansion of $B_n y_{n+1}$, and so of $B_{n+1}$.  This completes the inductive step and the proof.


### Corollary: Binomial Theorem

Let $x$ and $y$ be constants and let $n$ be any positive integer.

Then $\displaystyle (x+y)^n = \sum\limits_{i=0}^{n} {n\choose i} x^{n-i} y^i\\$

_Proof:_

Since each term in the expansion will have $n$ terms, each term must follow the form $x^{n-i} y^i$ for $0 \leq i \leq n$, and in all, there are $2^n$ such terms.  For any given value of $i$, the number of terms of the form $x^{n-i}y^i$ is clearly the number of ways one can choose the $i$ factors of $y$ from the $n$ available binomials, i.e., ${n\choose i}$, which gives

$$(x+y)^n = \sum\limits_{i=0}^{n}{n\choose i} x^{n-i} y^i$$


## Other Theorems

### Theorem {#binomialtheorem-other-theorem-1}

$${N_1\choose 0}{N_2\choose n} + {N_1\choose 2}{N_2\choose n-1} + \cdots
		+ {N_1\choose n-1}{N_2\choose 1} + {N_1\choose n}{N_2\choose 0}
	= {N_1+N_2\choose n}$$

where $0 \leq n \leq N_1 + N_2$.

_Proof:_

Using the Binomial Theorem we establish

$$
(1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\
\Rightarrow [{N_1\choose 0}a^0+\cdots+{N_1\choose N_1}a^{N_1}]\cdot
		[{N_2\choose 0}a^0+\cdots+{N_2\choose N_2}a^{N_2}] \\
  \ \ \ \ ={N_1+N_2\choose 0}+{N_1+N_2\choose 1}a+\cdots
		+{N_1+N_2\choose N_1+N_2}a^{N_1+N_2}  
$$

Expanding the left side of the equation gives

$$
{N_1\choose 0}{N_2\choose 0} + {N_1\choose 0}{N_2\choose 1}a + \cdots
		+ {N_1\choose 0}{N_2\choose N_2}a^{N_2} + {N_1\choose 1}{N_2\choose 0}a \\
\ \ \ \ + \cdots + {N_1\choose 1}{N_2\choose N_2}a^{N_2+1}
		+ \cdots + {N_1\choose N_1}{N_2\choose 0}a^{N_1}
		+ {N_1\choose N_1}{N_2\choose 1}a^{N_1+1}  \\
\ \ \ \ + \cdots + {N_1\choose N_1}{N_2\choose N_2}a^{N_1+N_2} \\
= {N_1\choose 0}{N_2\choose 0}+{N_1\choose 0}{N_2\choose 1}a
		+ {N_1\choose 1}{N_2\choose 0}a \\
\ \ \ \ + {N_1\choose 0}{N_2\choose 2}a^2+{N_1\choose 1}{N_2\choose 1}a^2
		+ {N_1\choose 2}{N_2\choose 0}a^2 \\
\ \ \ \ + \cdots + {N_1\choose N_1}{N_2\choose N_2}a^{N_1+N_2}
$$

Notice that for any $n$ where $0 \leq n \leq N_1 + N_2$, the coefficient for $a^n$, found by combining like terms, is ${N_1\choose 0}{N_2\choose n} + {N_1\choose 1}{N_2\choose n-1} + \cdots+{N_1\choose n-1}{N_2\choose 1} + {N_1\choose 0}{N_2\choose n}$ and, by the equivalence of the first equation in the proof, is equal to the coefficient ${N_1 + N_2\choose n}$.


### Theorem {#binomialtheorem-other-theorem-2}
$$\frac{\sum\limits_{i=1}^{n}{N_1\choose i}{N_2\choose n-i}}{{N_1+N_2\choose n}} = 1$$ 

for $0 \leq n \leq N_1 + N_2$.\\

_Proof:_

Theorem \@ref(binomialtheorem-other-theorem-1) establishes the equality 

$$
{N_1\choose 0}{N_2\choose n}+{N_1\choose 2}{N_2\choose n-1} + \cdots
		+ {N_1\choose n-1}{N_2\choose 1}+{N_1\choose n}{N_2\choose 0}
	= {N_1+N_2\choose n} \\
\Rightarrow\sum\limits_{i=1}^{n}{N_1\choose i}{N_2\choose n-i}
	= {N_1+N_2\choose n} \\
\Rightarrow\frac{\sum\limits_{i=1}^{n} {N_1\choose i}{N_2\choose n-i}} {{N_1+N_2\choose n}}
	= 1
$$

<!--chapter:end:Binomial_Theorem.Rmd-->

# Chebychev's Theorem

## Chebychev's Theorem

In any finite set of numbers and for any real number $h > 1$, at least $(1 - \frac{1}{h^2}) \cdot 100\%$ of the numbers lie within $h$ standard deviations of the mean.  In other words, they lie within the interval $(\mu-h\cdot\sigma , \mu+h\cdot\sigma)$.

_Proof:_

For a set $\{x_1,x_2,\ldots,x_r,x_{r+1},\ldots,x_n\}$ where, by choice of labeling, $\{x_1,x_2,\ldots,x_r\}$ lie outside of $(\mu-h\cdot\sigma , \mu+h\cdot\sigma)$.  Also, $\{x_{r+1},\ldots,x_n\}$ are within the interval.  Under these conditions we know

$$|x_1-\mu| > h\sigma,\ |x_2-\mu| > h\sigma, \ldots,\ |x_r-\mu| > h\sigma$$

Squaring gives

$$(x_1-\mu)^2 > h^2\sigma^2,\ (x_2-\mu)^2 > h^2\sigma^2,\ldots,\ (x_r-\mu)^2 > h^2\sigma^2\\
\ \ \ \ \Rightarrow\sum\limits_{i=1}^{r}(x_1-\mu)^2 > \sum\limits_{i=1}^{r}h^2\sigma^2
	= rh^2\sigma^2
$$

Since all $(x_i-\mu)^2$ must necessarily be positive,

$$\begin{align*}
\sum\limits_{i=1}^{r}(x_i-\mu)^2 &< \sum\limits_{i=1}^{n}(x_i-\mu)^2 \\
\ \ \ \ \Rightarrow rh^2\sigma^2 &< \sum\limits_{i=1}^{n}(x_i-\mu)^2 \\
\ \ \ \ ^{[1]} \Rightarrow rh^2\sigma^2 &< n\sigma^2 \\
\ \ \ \ \Rightarrow rh^2 &< n \\
\ \ \ \ \Rightarrow\frac{r}{n} &< \frac{1}{h^2}
\end{align*}$$

> 1. $\sigma^2 = \frac{1}{n}\sum\limits_{i=1}^{n}(x_i-\mu)^2$  
>		 $\ \ \ \ \Rightarrow n\sigma^2 = \sum\limits_{i=1}^{n}(x_i-\mu)^2$

and $\frac{r}{n}$ is the fraction of numbers outside $(\mu-h\cdot\sigma , \mu+h\cdot\sigma)$.  By the law of complements, the fraction of numbers inside the interval is $1 - \frac{r}{n}$, which implies $1 - \frac{r}{n} > 1 - \frac{1}{h^2}$.  Thus, more than $(1-\frac{1}{h^2})\cdot 100\%$ of the points lie within $h$ standard deviations of the mean, or within the interval $(\mu-h\cdot\sigma , \mu+h\cdot\sigma)$. 


## Alternate Proof of Chebychev's Theorem

In any finite set of numbers and for any real number $h>1$, at least $(1-\frac{1}{h^2})\cdot 100\%$ of the numbers lie within $h$ standard deviations of the mean.  In other words, they lie within the interval $(\mu-h\cdot\sigma,\mu+h\cdot\sigma)$.\\

_Proof:_

The proof here is done for the discrete case, but is applicable also in the continuous case by replacing the summations with integrals (with integrals, the limits will be from $-\infty$ to $\infty$).

$$\begin{align*}
\sigma^2
	&= E(x-\mu)^2 \\
	&= \sum\limits_{y=0}^{\infty}(y-\mu)^2p(y) \\
  &= \sum\limits_{y=0}^{\mu-h\sigma}(y-\mu)^2p(y) + 
        \sum\limits_{y=\mu-h\sigma+1}^{\mu+h\sigma-1}(y-\mu)^2p(y)
		    + \sum\limits_{y=\mu+h\sigma}^{\infty}(y-\mu)^2p(y) \\
^{[1]} \Rightarrow \sigma^2 &\geq \sum\limits_{y=0}^{\mu-h\sigma}(y-\mu)^2p(y)
		+ \sum\limits_{y=\mu+h\sigma}^{\infty}(y-\mu)^2p(y)\\
\end{align*}$$

> 1. Since all the $(y-\mu)^2$ must be positive, removing the middle term will surely result in this inequality.

In both of these summations $y$ is outside the interval $(\mu-h\cdot\sigma , \mu+h\cdot\sigma)$, so

$$\begin{align*}
              |y-\mu|   &\geq   h\sigma \\
\Rightarrow (y-\mu^2)   &\geq   h^2\sigma^2 \\
\Rightarrow \sigma^2    &\geq   \sum\limits_{y=0}^{\mu-h\sigma}h^2\sigma^2p(y)
		+ \sum\limits_{\mu+h\sigma}^{\infty}h^2\sigma^2p(y) \\
\Rightarrow\sigma^2     &\geq   h^2\sigma^2\Big[\sum\limits_{y=0}^{\mu-h\sigma}p(y)
		+ \sum\limits_{\mu+h\sigma}^{\infty}p(y)\Big]
\end{align*}$$

The first summation is the sum of all probabilities that $y-\mu < h\sigma$, i.e. $P(y-\mu < h\sigma)$.  Likewise, the second summation is $P(y-\mu > h\sigma)$.

$$\begin{align*} 
\Rightarrow \sigma^2         &\geq   h^2\sigma^2[P(y-\mu<h\sigma) + P(y-\mu>h\sigma)] \\
\Rightarrow \sigma^2         &\geq   h^2\sigma^2[P(|y-\mu|>h\sigma)] \\
\Rightarrow \frac{1}{h^2}    &\geq   P(|y-\mu|>h\sigma) \\
\Rightarrow 1-\frac{1}{h^2}  &\leq   P(|y-\mu|>h\sigma)
\end{align*}$$


## Chebychev's Theorem for Absolute Deviation

This theorem is provided by Brunette [@Brunetb]

In any finite set of numbers, and for any real number $h > 1$, at least $1 - \frac{1}{h}$ of the numbers lie within $h$ absolute deviations of the mean, where the absolute deviation is defined $Ab = \frac{1}{n}\sum\limits_{i=1}{n}|x_i-\bar x|$.  In other words, $1-\frac{1}{h}$ of the numbers are in the interval $(\bar x-h\cdot Ab , \bar x+h\cdot Ab)$.

_Proof:_

For a set $\{x_1,x_2,\ldots,x_r,x_{r+1},\ldots,x_n\}$ where, by choice of labeling, $\{x_1,x_2,\ldots,x_r\}$ lie outside of $(\mu-h\cdot Ab , \mu+h\cdot Ab)$.  Also, $\{x_{r+1},\ldots,x_n\}$ are within the interval.  Accordingly,


$$h \cdot Ab \leq |x_1-\bar x| ,\ h \cdot Ab \leq |x_1-\bar x| ,\ldots ,\ h \cdot Ab 
    \leq |x_1-\bar x| $$
    
$$\begin{align*}
\Rightarrow r \cdot h \cdot Ab 
    &\leq \sum\limits_{i=1}^{r}|x_i-\bar x| \\
\Rightarrow r \cdot h \cdot Ab 
    &\leq \sum\limits_{i=1}^{n}|x_i-\bar x| \\
^{[1]} \Rightarrow r \cdot h \cdot Ab 
    &\leq n \cdot Ab\\
\Rightarrow \frac{r}{n} 
    &\leq \frac{1}{h}\\
\Rightarrow -\frac{r}{n} 
    &\geq -\frac{1}{h}\\
\Rightarrow 1-\frac{r}{n} 
    &\geq 1-\frac{1}{h}
\end{align*}$$

> 1. $Ab = \frac{1}{n}\sum\limits_{i=1}^{n}|x_i-\bar x|$  
>    $\Rightarrow n \cdot Ab = \sum\limits_{i=1}^{n}|x_i-\bar x|$ 

Now $\frac{r}{n}$ is the fraction of numbers outside the interval.  So $1-\frac{r}{n}$ is the fraction of numbers within $h$ absolute deviations of the mean, or within the interval $(\mu-h\cdot Ab , \mu+h\cdot Ab)$.


<!--chapter:end:Chebychev.Rmd-->

# Chi-Square Distribution

## Probability Distribution Function
A random variable $X$ is said to have a Chi-Square Distribution with parameter $\nu$ if its probability distribution function is

\[f(x) = \left\{
	\begin{array}{ll}
		\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
			{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}  & 0<x,\ 0<\nu\\
		0 & otherwise  
	\end{array} \right. \]

$\nu$ is commonly referred to as the _degrees of freedom_.

## Cumulative Distribution Function

The cumulative distribution function for the Chi-Square Distribution cannot be written in closed form.  It's integral form is expressed as
\[ F(x) = \left\{
	\begin{array}{ll}
		\displaystyle\int\limits_{0}^{x} \frac{t^{\frac{\nu}{2}-1}e^{-\frac{t}{2}}}
			{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} dt  & 0<x,\ 0<\nu\\\\
		0 & otherwise
	\end{array} \right.
\]


```{r ChiSquare_Distribution, echo = FALSE, fig.path = 'figures/', fig.cap = 'The graphs on the top and bottom depict the Chi-Square probability distribution and cumulative distribution functions, respectively, for $\\nu=4,7,10$.  As $\\nu$ gets larger, the distribution becomes flatter with thicker tails.'}
ChiSquare <- 
  expand.grid(x = seq(0, 20, by = .01),
              nu = c(4, 7, 10)) %>%
  mutate(dchisq = dchisq(x, df = nu),
         pchisq = pchisq(x, df = nu)) %>%
  gather(type, prob, -x, -nu) %>%
  mutate(nu = factor(nu),
         type = factor(type,
                       levels = c("dchisq",
                                  "pchisq"),
                       labels = c("Probability Distribution",
                                  "Cumulative Distribution")))

ggplot(data = ChiSquare,
       mapping = aes(x = x,
                     y = prob,
                     colour = nu)) + 
  geom_line() + 
  facet_grid(type ~ ., scales = "free_y") + 
  theme_bw() + 
  scale_colour_manual(values = palette[c(1, 3, 9)]) + 
  xlab("X") + 
  ylab("Probability")
```


## Expected Values
$$\begin{align*}
E(X)
      	&= \int\limits_{0}^{\infty}x\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}dx \\
      	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} 
      		\int\limits_{0}^{\infty}x\cdot x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx \\
      	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} 
      		\int\limits_{0}^{\infty}x^{\frac{\nu}{2}}e^{-\frac{x}{2}}dx  \\
^{[1]}  &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
		      \Big[\Gamma\Big(\frac{\nu}{2}+1\Big)2^{\frac{\nu}{2}+1}\Big] \\
      	&= \frac{\Gamma(\frac{\nu}{2}+1)2^{\frac{\nu}{2}+1}} 
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} \\
      	&= \frac{\frac{\nu}{2}\Gamma(\frac{\nu}{2})2^{\frac{\nu}{2}+1}} 
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} \\
      	&= \frac{2\nu}{2} \\
      	&= \nu
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		= \beta^\alpha\Gamma(\alpha)$

$$\begin{align*}
E(X^2)
      	&= \int\limits_{0}^{\infty}x^2\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}dx \\
      	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
      		\int\limits_{0}^{\infty}x^2\cdot x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx \\
      	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
	      	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}+1}e^{-\frac{x}{2}}dx \\
^{[1]}  &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
      		\Big[\Gamma(\frac{\nu}{2}+2)2^{\frac{\nu}{2}+2}\Big] \\
      	&= \frac{\Gamma\Big(\frac{\nu}{2}+2\Big)2^{\frac{\nu}{2}+2}}
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} \\
      	&= \frac{(\frac{\nu}{2}+1)\Gamma(\frac{\nu}{2}+1)2^{\frac{\nu}{2}+2}}
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} \\
        &= \frac{\Big(\frac{\nu}{2}+1\Big)\frac{\nu}{2}\Gamma(\frac{\nu}{2})2^{\frac{\nu}{2}+2}}
      		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})} \\
      	&= \Big(\frac{\nu}{2}+1\Big)\frac{\nu}{2}\cdot 2^2=2\Big(\frac{\nu}{2}+1\Big)\nu \\
      	&= (\nu+2)\nu=\nu^2+2\nu
\end{align*}$$
	
> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		= \beta^\alpha\Gamma(\alpha)$
		
$$\begin{align*}
\mu
	&= E(X) \\
	&= \nu \\
\\
\\
\sigma^2
	&= E(X^2)-E(X)^2 \\
	&= \nu^2+2\nu-\nu^2 \\
	&= 2\nu
\end{align*}$$



## Moment Generating Function

$$\begin{align*}
M_X(t)
        	&= E(e^{tX}) \\
        	&= \int\limits_{0}^{\infty}e^{tx}
        	  	\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
        		  {2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}dx \\
        	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}e^{tx}\cdot 
        			x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx \\
          &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        		  e^{tx}e^{-\frac{x}{2}}dx \\
        	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{tx-\frac{x}{2}}dx \\
          &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{\frac{2tx}{2}-\frac{x}{2}}dx \\
        	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{-\frac{2tx-x}{2}}dx \\
          &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{-x\frac{-2t+1}{2}}dx \\
        	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{-x\frac{1-2t}{2}}dx \\
          &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}
        	  	e^{\frac{-x}{\frac{2}{1-2t}}}dx \\
	^{[1]}  &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        		\Big[\Big(\frac{2}{1-2t}\Big)^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})\Big]\\
          &= \frac{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        	  	{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})(1-2t)^{\frac{\nu}{2}}} \\
        	&= \frac{1}{(1-2t)^{\frac{\nu}{2}}} \\
        	&= (1-2t)^{-\frac{\nu}{2}}
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		= \beta^\alpha\Gamma(\alpha)$

$$\begin{align*}
M_X^{(1)}(t)
	&= -\frac{\nu}{2}(1-2t)^{-\frac{\nu}{2}-1}(-2) \\
	&= \frac{2\nu}{2}(1-2t)^{-\frac{\nu}{2}-1} \\
	&= \nu(1-2t)^{-\frac{\nu}{2}-1} \\
\\
\\
M_X^{(2)}(t)
	&= (-\frac{\nu}{2}-1)\nu(1-2t)^{-\frac{\nu}{2}-2}(-2) \\
	&= (\frac{2\nu}{2}+2)\nu(1-2t)^{-\frac{\nu}{2}-2} \\
  &= (\nu+2)\nu)(1-2t)^{-\frac{\nu}{2}-2} \\
	&= (\nu^2+2\nu)(1-2t)^{-\frac{\nu}{2}-2}\\
\\
\\
M_X^{(1)}(0)
	&= \nu(1-2\cdot 0)^{-\frac{\nu}{2}-1} \\
	&= \nu(1-0)^{-\frac{\nu}{2}-1} \\
	&= \nu(1)^{-\frac{\nu}{2}-1} \\
	&= \nu \\
M_X^{(2)}(0)
	&= (\nu^2+2\nu)(1-2\cdot 0)^{-\frac{\nu}{2}-2} \\
	&= (\nu^2+2\nu)(1-0)^{-\frac{\nu}{2}-2} \\
  &= (\nu^2+2\nu)(1)^{-\frac{\nu}{2}-2} \\
	&= (\nu^2+2\nu) \\
\\
\\
E(X)
	&= M_X^{(1)}(0) \\
	&= \nu\\
\\
\\
E(X^2)
	&= M_X^{(2)}(0) \\
	&= (\nu^2+2\nu) \\
\\
\\
\mu
	&= E(X) \\
	&= \nu  \\
\sigma^2
	&= E(X^2)-E(X)^2 \\
	&= \nu^2+2\nu-\nu^2 \\
	&= 2\nu
\end{align*}$$



## Maximum Likelihood Function

Let $x_1,x_2,\ldots,x_n$ be a random sample from a Chi-square distribution with parameter $\nu$.

### Likelihood Function

$$\begin{align*}
L(\theta)
	&= f(x_1|\theta) f(x_2|\theta) \cdots f(x_n|\theta) \\
  &= \frac{x_1^{\nu/2-1}e^{-x_1/2}}{2^{\nu/2}\Gamma\big(\frac{\nu}{2}\big)}
		  \cdot \frac{x_2^{\nu/2-1}e^{-x_2/2}}{2^{\nu/2}\Gamma\big(\frac{\nu}{2}\big)}
		  \cdots \frac{x_n^{\nu/2-1}e^{-x_n/2}}{2^{\nu/2}\Gamma\big(\frac{\nu}{2}\big)} \\
  &= \prod\limits_{i=1}^{n}\frac{x_i^{\nu/2-1}e^{-x_i/2}}
      {2^{\nu/2}\Gamma\big(\frac{\nu}{2}\big)} \\
	&= \bigg(2^{\nu/2}\Gamma\Big(\frac{\nu}{2}\Big)\bigg)
		  \prod\limits_{i=1}^{n}x_i^{\nu/2-1}e^{-x_i/2} \\
  &= \bigg(2^{\nu/2}\Gamma\Big(\frac{\nu}{2}\Big)\bigg)
  		\cdot \exp\bigg\{ \sum\limits_{i=1}^{n}\frac{x_i}{2} \bigg\}
	  	\cdot \prod\limits_{i=1}^{n}x_i^{\nu/2-1} \\
  &= \bigg(2^{\nu/2}\Gamma\Big(\frac{\nu}{2}\Big)\bigg)
	  	\cdot \exp\bigg\{ \frac{1}{2}\sum\limits_{i=1}^{n}x_i \bigg\}
	  	\cdot \prod\limits_{i=1}^{n}x_i^{\nu/2-1}
\end{align*}$$

### Log-likelihood Function
$$\begin{align*}
\ell(\theta)
	&= \ln\big(L(\theta)\big) \\
	&= \ln\Bigg[ \bigg(2^{\nu/2}\Gamma\Big(\frac{\nu}{2}\Big)\bigg)
		\cdot \exp\bigg\{ \frac{1}{2}\sum\limits_{i=1}^{n}x_i \bigg\}
		\cdot \prod\limits_{i=1}^{n}x_i^{\nu/2-1} \Bigg] \\
  &= \ln\Bigg[ \bigg( 2^{\nu/2}\Gamma \Big( \frac{\nu}{2} \Big) \bigg) \Bigg]
		+ \ln\Bigg( \exp\bigg\{ \frac{1}{2}\sum\limits_{i=1}^{n}x_i \bigg\} \Bigg)
		+ \ln\bigg(\prod\limits_{i=1}^{n}x_i^{\nu/2-1}\bigg) \\
  &= -n \ln\bigg( 2^{\nu/2}\Gamma \Big( \frac{\nu}{2} \Big) \bigg)
		+ \frac{1}{2}\sum\limits_{i=1}^{n}x_i
		+ \bigg( \frac{\nu}{2}-1 \bigg) \ln\bigg( \prod\limits_{i=1}^{n}x_i \bigg) \\
  &= -n\bigg( \ln(2^{\nu/2}) + \Gamma\Big(\frac{\nu}{2}\Big) \bigg)
	  + \frac{1}{2}\sum\limits_{i=1}^{n}x_i
		+ \bigg( \frac{\nu}{2}-1 \bigg) \sum\limits_{i=1}^{n}\ln x_i \\
  &= -n\bigg(\frac{\nu}{2} \ln 2 + \ln \Gamma\Big( \frac{\nu}{2} \Big) \bigg)
		+ \frac{1}{2}\sum\limits_{i=1}^{n}x_i
		+ \bigg( \frac{\nu}{2}-1 \bigg) \sum\limits_{i=1}^{n}\ln x_i \\
  &= -\frac{n\nu}{2} \ln 2 - n\ln \Gamma\Big( \frac{\nu}{2} \Big)
		+ \frac{1}{2}\sum\limits_{i=1}^{n}x_i
		+ \bigg( \frac{\nu}{2}-1 \bigg) \sum\limits_{i=1}^{n}\ln x_i
\end{align*}$$


### MLE for $\nu$

$$\begin{align*}
\frac{d\ell}{d\nu}
	&= -\frac{n}{2} \ln 2 
		- \frac{n}{\Gamma\big(\frac{\nu}{2}\big)} \Gamma^\prime\Big(\frac{\nu}{2}\Big) \cdot \frac{1}{2}
		+ 0 + \frac{1}{2} \sum\limits_{i=1}^{n}\ln x_i \\
  &= -\frac{n}{2} \ln 2
		- \frac{n}{2\Gamma\big(\frac{\nu}{2}\big)} \Gamma^\prime\Big(\frac{\nu}{2}\Big)
		+ \frac{1}{2} \sum\limits_{i=1}^{n}\ln x_i \\
\\
\\
0 &= -\frac{n}{2} \ln 2
		- \frac{n}{2\Gamma\big(\frac{\nu}{2}\big)} \Gamma^\prime\Big(\frac{\nu}{2}\Big)
		+ \frac{1}{2} \sum\limits_{i=1}^{n}\ln x_i\\
\Rightarrow \frac{n}{2} \ln 2 - \frac{1}{2}\sum\limits_{i=1}^{n}\ln x_i
		&= -\frac{n}{2\Gamma\big(\frac{\nu}{2}\big)} \Gamma^\prime\Big(\frac{\nu}{2}\Big)\\
\Rightarrow n\ln 2 - \sum\limits_{i=1}^{n}\ln x_i
		&= -\frac{n}{\Gamma\big(\frac{\nu}{2}\big)} \Gamma^\prime\Big(\frac{\nu}{2}\Big)\\
\Rightarrow \frac{\sum\limits_{i=1}^{n}\ln x_i - n\ln 2}{n}
		&= \frac{\Gamma^\prime\big(\frac{\nu}{2}\big)}{\Gamma\big(\frac{\nu}{2}\big)}
\end{align*}$$

Due to the complexity of the Gamma function in this equation, no solution can be developed for $\nu$ in closed form.  Thus, we have to rely on numerical methods to obtain a solution to the equation and find the maximum likelihood estimator.



## Theorems for the Chi-Square Distribution

### Validity of the Distribution

$$ 
\int\limits_{0}^{\infty}\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}dx = 1
$$

_Proof:_

$$\begin{align*}
\int\limits_{0}^{\infty}\frac{x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}}
		{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}dx
        	&= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        		\int\limits_{0}^{\infty}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx \\
	^{[1]}  &= \frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}
        		\Big[2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})\Big] \\
          &= \frac{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}{2^{\frac{\nu}{2}}
              \Gamma(\frac{\nu}{2})} \\
        	&= 1
\end{align*}$$
	
> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		= \beta^\alpha\Gamma(\alpha)$


### Sum of Chi-Square Random Variables

Let $X_1 , X_2 , \ldots , X_n$ be independent Chi-Square random variables with parameter $\nu_i$, that is $X_i\sim\chi^2(\nu_i),\ i=1,2,\ldots,n$.

Suppose $Y = \sum\limits_{i=1}^{n}X_i$. Then $Y\sim\chi^2(\sum\limits_{i=1}^{n}\nu_i)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}=E(e^{t(X_1+X_2+\cdots+X_n}) \\
	&= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
	&= E(e^{tX_1})E(e^{tX_2})\cdots E(e^{tX_n}) \\
  &= (1-2t)^{-\frac{\nu_1}{2}}(1-2t)^{-\frac{\nu_2}{2}}\cdots
	  	(1-2t)^{-\frac{\nu_n}{2}} \\
	&= (1-2t)^{\sum\limits_{i=1}^{n}\nu_i}
\end{align*}$$

Which is the mgf of a Chi-Square random variable with parameter $\sum\limits_{i=1}^{n}\nu_i$.  
Thus $Y\sim\chi^2\bigg(\sum\limits_{i=1}^{n}\nu_i\bigg)$. 

### Multiple of a Chi-Square Random Variable

Let $X$ be a Chi-Square random variable with parameter $\nu$, that is $X\sim\chi^2(\nu),\ i=1,2,\ldots,n$.

Suppose $Y = c \cdot X$. Then $Y\sim Gamma(\frac{\nu}{2}, 2 \cdot c)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY})=E(e^{tcX}) \\
	&= (1-2tc)^{-\frac{\nu}{2}} \\
	&= (1-2c \cdot t)^{-\frac{\nu}{2}}
\end{align*}$$

Which is the mgf of a Gamma distributed variable with parameters $\alpha = 
\frac{\nu}{2}$ and $\beta = 2c$. Thus, $Y\sim Gamma(\frac{\nu}{2}, 2 \cdot c)$.

### Square of a Standard Normal Random Variable

If $Z\sim N(0,1)$, then $Z^2\sim\chi^2(1)$.

_Proof:_
$$\begin{align*}
M_{Z^2}(t)
      	&= E(e^{tZ^2}) \\
      	&= \int\limits_{-\infty}^{\infty}e^{tz^2}\frac{1}{\sqrt{2\pi}}
      		e^{-\frac{z^2}{2}}dz \\
      	&= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}e^{tz^2}
      		e^{-\frac{z^2}{2}}dz \\
        &= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}
      		e^{-\frac{z^2}{2}(-2t+1)}dz \\
      	&= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}
      		e^{-\frac{z^2}{2}(1-2t)}dz \\
^{[1]}  &= \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}
      		e^{-\frac{z^2}{2}(1-2t)}dz \\
^{[2]}	&= \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}e^{-u}
      		\frac{\sqrt{2}u^{-\frac{1}{2}}}{2(1-2t)^{\frac{1}{2}}}du \\
        &= \frac{2\sqrt{2}}{2\sqrt{2\pi}(1-2t)^{\frac{1}{2}}}
      		\int\limits_{0}^{\infty}e^{-u}u^{-\frac{1}{2}}du \\
      	&= \frac{2\sqrt{2}}{2\sqrt{2\pi}(1-2t)^{\frac{1}{2}}}
      		\int\limits_{0}^{\infty}u^{\frac{1}{2}-1}e^{-u}du \\
^{[3]}  &= \frac{1}{\sqrt{\pi}(1-2t)^{\frac{1}{2}}}\Gamma(\frac{1}{2}) \\
      	&= \frac{\sqrt{\pi}}{\sqrt{\pi}(1-2t)^{\frac{1}{2}}} \\
      	&= \frac{1}{(1-2t)^{\frac{1}{2}}}=(1-2t)^{-\frac{1}{2}} \\
\end{align*}$$

> 1. $\int\limits_{-\infty}^{\infty}f(x)dx
		= 2\int\limits_{0}^{\infty}f(x)dx$ when f(x) is an even function (\ref{Integration1.2})
> 2. Let $u=\frac{z^2}{2}(1-2t)  
>    \ \ \ \ \Rightarrow z=\frac{\sqrt{2}u^{\frac{1}{2}}}{(1-2t)^{\frac{1}{2}}}$  
>    \ \ \ \ So $dz=\frac{\sqrt{2}u^{-\frac{1}{2}}} {2(1-2t)^{\frac{1}{2}}}$
> 3. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$	
		
		
Which is the mgf of a Chi-Square random variable with 1 degree of freedom.  Thus $Z^2\sim\chi^2(1)$. 

<!--chapter:end:ChiSquare_Distribution.Rmd-->

# Combinations

### Lemma {#combinations-lemma}

A set of $n$ elements may be partitioned into $m$ distinct groups containing $k_1 , k_2 , \ldots , k_m$ objects, respectively, where each object appears in exactly one group and $\sum\limits_{i=1}^{m}k_i=n$, in $\displaystyle N={n\choose k_1k_2\ldots k_m}=\frac{n!}{k_1!k_2!\ldots k_m!}$ ways.\\

_Proof:_

$N$ is the number of ways all $n$ of the elements of the set can be arranged in $m$ groups where the order within each group is not important (i.e. rearrangements of elements in a group do not qualify as distinct groups).

The number of distinct arrangements of the $n$ elements in which the order of selection is important, $P_k^n$, is equal to $N$ multiplied by the number of ways each individual group of $k_i$ can be selected in which the order is important, i.e.

$$\begin{align*}
P_n^n 
                  &= N \cdot P_{k_1}^{k_1} P_{k_2}^{k_2} \cdots P_{k_m}^{k_m} \\
  \Rightarrow n!  &= N \cdot k_1! k_2! \cdots k_m! \\
  \Rightarrow N   &= \frac{n!}{k_1! k_2! \cdots k_m!}
\end{align*}$$

### Combinations Theorem {#combinations-theorem}

Given a set of $n$ elements, the number of possible ways to select a subset of size $k$, without regard to the order of their selection, is $\frac{n!}{k!(n-k)!}$.\\

_Proof:_

This theorem is a special case of the Lemma with $n=n$, $m=2$, $k_1=k$ and $k_2=n-k$.  thus,

$$\displaystyle N=\frac{n!}{k!(n-k)!}$$

The formula $\displaystyle \frac{n!}{k!(n-k)!}$ is denoted in a number of ways, depending on the author.  Denotations may be 
$C_k^n$, $_nC_k$, $C_{n,k}$, $C(n,k)$, and ${n\choose k}$.  
Throughout this book, the form ${n\choose k}$ is used and may be read "$n$ choose $k$ objects."


### Theorem {#combinations-theorem-1}

For any integer $a$ such that $0\leq a\leq k$,

$$
{n\choose k}
	= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}{n-a\choose k-a}
$$

_Proof:_

$$\begin{align*}
{n\choose k}	
	&= \frac{n!}{k!(n-k)!} \\
	&= \frac{n(n-1)!}{k(k-1)!(n-k)!} \\
	&= \frac{n(n-1)(n-2)!}{k(k-1)(k-2)!(n-k)!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)(n-a)!}{k(k-1)(k-2)\cdots(k-a+1)(k-a)!(n-k)!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}
		\cdot \frac{(n-a)!}{(k-a)!(n-k)!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}
		\cdot \frac{(n-a)!}{(k-a)!(n-a+a-k)!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}
	\cdot \frac{(n-a)!}{(k-a)![(n-a)+(a-k)]!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}
		\cdot \frac{(n-a)!}{(k-a)![(n-a)-(k-a)]!} \\
  &= \frac{n(n-1)(n-2)\cdots(n-a+1)}{k(k-1)(k-2)\cdots(k-a+1)}
		\cdot {n-a\choose k-a}
\end{align*}$$

<!--chapter:end:Combinations.Rmd-->

# Correlation (Pearson's)

Pearson's correlation coefficient of the variables $X$ and $Y$ is a measure of the linear relationship between $X$ and $Y$.  It is defined
\[\rho
	= \frac{Cov(X,Y)}{\sqrt{\sigma_X^2\cdot \sigma_Y^2}}\]

Notice that if $X$ and $Y$ are independent then $Cov(X,Y,)=0$ and $\rho=0$ and there is no linear relationship between the variables.

## Theorems on Pearson's Correlation

## Computational Formula for $\rho$

$$\rho
	= \frac{\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}(x_i-\mu_X)(y_j-\mu_Y)}
		{\sum\limits_{i=1}^{n}(x_i-\mu_X)\sum\limits_{j=1}^{m}(y_i-\mu_Y)}$$

_Proof:_

$$\begin{align*} 
\rho
	&= \frac{Cov(X,Y)}{\sqrt{\sigma_X^2\sigma_Y^2}} \\
	&= \frac{Cov(X,Y)}{\sqrt{\sigma_X^2\sigma_Y^2}} \\
	&= \frac{\sum\limits_{i=1}^{N}(x_i-\mu_X)(y_i-\mu_Y)\frac{1}{N}}
		{\sqrt{\frac{\sum\limits_{i=1}^{N}(x_i-\mu_X)^2}{N}\frac{\sum\limits_{i=1}^{N}(y_i-\mu_Y)^2}{N}}} \\
  &= \frac{\frac{1}{N}\sum\limits_{i=1}^{N}(x_i-\mu_X)(y_j-\mu_Y)}
		{\frac{1}{N}\sqrt{\sum\limits_{i=1}^{N}(x_i-\mu_X)^2\sum\limits_{i=1}^{N}(y_i-\mu_Y)^2}} \\
	&= \frac{\sum\limits_{i=1}^{N}(x_i-\mu_X)(y_i-\mu_Y)}
		{\sqrt{\sum\limits_{i=1}^{N}(x_i-\mu_X)\sum\limits_{i=1}^{N}(y_i-\mu_Y)}}
\end{align*}$$

<!--chapter:end:CorrelationPearson.Rmd-->

# Covariance

## Definition of Covariance

For any two random variables $X$ and $Y$, the covariance of $X$ and $Y$ is defined as

$$Cov(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]$$

## Theorems on Covariance

### Theorem

Let $X$ be a random variable.  Then
$$Cov(X,X) = V(X)$$

_Proof:_

$$\begin{align*}
Cov(X,X)
	&= E[(X-\mu)(X-\mu)] \\
	&= E[(X-\mu)^2] \\
	&= V(X)
\end{align*}$$


### Theorem {#covariance-theorem2}

Let $X$ and $Y$ be random variables.  Then

$$Cov(X,Y) = E(XY)-E(X)E(Y)$$

_Proof:_

$$\begin{align*}
Cov(X,Y)
  &= E[(X-\mu_x)(Y-\mu_Y)] \\
  &= E[XY - X\mu_y - Y\mu_X + \mu_X\mu_Y] \\
  &= E(XY) - E(X)\mu_Y - \mu_XE(Y) + \mu_X\mu_Y \\
  &= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\
  &= E(XY) - 2E(X)E(Y) + E(X)E(Y) \\
  &= E(XY) - E(X)E(Y)
\end{align*}$$



### Covariance

Let $X$ and $Y$ be random variables and let $a$ and $b$ be constants.  Then

$$Cov(aX,bY) = abCov(X,Y)$$

_Proof:_

$$\begin{align*}
Cov(aX,bY)
	&= E(aXbY) - E(aX)E(bY) \\
  &= abE(XY) - abE(X)E(Y) \\
	&= ab[E(XY) - E(X)E(Y)] \\
  &= abCov(X,Y)
\end{align*}$$


### Theorem

Let $X_1 , X_2 , \ldots , X_n$ be random variables with $E(X_i) = \mu_i$ for $i = 1,2,\ldots,n$ and let $Y_1,Y_2,\ldots,Y_m$ be random variables with $E(Y_j) = \phi_j$ for $j=1,2,\ldots,m$.  Also, let $a_1,a_2,\ldots,a_n$ and $b_1,b_2,\ldots,b_m$ be constants.\\

If $U_1 = \sum\limits_{i=1}^{n}a_iX_i$
 and $U_2 = \sum\limits_{i=1}^{m}b_iY_i$, then 
 
$$Cov(U_1,U_2) = \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_jCov(X_i,Y_j)$$

_Proof:_

$$\begin{align*}
Cov(U_1,U_2)
	&= E[(U_1-E(U_1))(U_2-E(U_2))] \\
  &= E\Big[\big(\sum\limits_{i=1}^{n}a_iX_i-\sum\limits_{i=1}^{n}a_i\mu_i\big)
		\big(\sum\limits_{j=1}^{m}b_jY_j-\sum\limits_{j=1}^{m}b_j\phi_j\big)\Big] \\
  &= E\Big[\big(\sum\limits_{i=1}^{n}a_i(X_i-\mu_i)\big)\big(\sum\limits_{j=1}^{m}b_j(Y_j-\phi_j)\big)\Big] \\
  &= E\Big[\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_j(X_i-\mu_i)(Y_j-\phi_j)\Big] \\
  &= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_jE[(X_i-\mu_i)(Y_j-\phi_j)] \\
  &= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_j\ Cov(X_i,Y_j)
\end{align*}$$


### Theorem

Let $X_1,X_2,\ldots,X_n$ be random variables with $E(X_i)=\mu_i$ for $i=1,2,\ldots,n$ and let $a_1,a_2,\ldots,a_n$ be constants.  

If $Y = \sum\limits_{i=1}^{n}a_iX_i$ then

$$V(Y) = 
  \sum\limits_{i=1}^{n}a_i^2V(X_i)+2\sum\limits_{\ \ i<}\sum\limits_{j\ \ }a_ia_jCov(X_i,X_j)$$

_Proof:_

$$\begin{align*}
V(Y)
	&= E[(Y-\mu_Y)^2] \\
	&= E[(Y-\mu_Y)(Y-\mu_Y)] \\
  &= E\Big[\big(\sum\limits_{i=1}^{n}a_iX_i-a_i\mu_i\big)
      \big(\sum\limits_{n=1}^{n}a_jX_j-a_j\mu_j\big)\Big] \\
  &= \sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}a_ia_jE[(X_i-\mu_i)(X_j-\mu_j)] \\
  &= \sum\limits_{i=1}^{n}a_i^2Cov(X_i,X_i)+
		\sum\limits_{\ \ i\neq}\sum\limits_{j\ \ \ \ }a_ia_jE[(X_i-\mu_i)(X_j-\mu_j)] \\
  &= \sum\limits_{i=1}^{n}a_i^2V(X_i)+
      2\sum\limits_{\ \ i<}\sum\limits_{j\ \ \ \ }a_ia_jCov(X_i,X_j)
\end{align*}$$


<!--chapter:end:Covariance.Rmd-->

# Experimental Designs


## Designs in Categorical Data Analysis

Studies in Categorical Data Analysis can be classified into several designs.  These designs fall into the following two categories:

1. _Retrospective Design_: looks at and analyzes measurements that have already been taken.
2. _Prospective Design_: specifies the measurements to be collected at a future time.  

### Case Control Study

In case control studies, the marginal distribution of the _response variable_ is fixed by the sampling design.  In other words, researchers select particular numbers of each category of the response variable in order to ensure that enough of each case are included in the sample.  The result is that the marginal distribution of the response is non-random.

Unfortunately, in order to calculate conditional probabilities, the marginal distribution of interest must be random.  The difference of proportions for the response and the relative risk are both based on the marginal distribution of the response, and are both invalid procedures in case-control studies.

In taking the measurements, researchers idenitfy people who are already classified into the response variable, making the design retrospective.

### Cross Sectional Study


### Cohort Study

In Cohort Studies, subjects make their own choice about which group in the explanatory variable to join and researchers monitor the subjects with respect to a response variable over a period of time.  Both the explanatory and response variables are random and only the total sample size is fixed by the researcher.  Thus, conditional probabilities may be computed for both the predictor and response variables; differences in proportions may be estimated; and the relative risk is defined for the response variable.

Since subjects select the group in which they will be and a measurement of their response is taken later, cohort studies are prospective.

### Randomized Study

In randomized Studies, the researcher randomly assigns subjects to the explanatory variable and then observes their response (making this a prospective study).  The marginal distribution of the explanatory variable is therefore fixed, and conditional probabilities may not be computed.

The response variable, on the other hand, is random and conditional probabilites may be computed, as well as the difference of proportions and relative risk.


### Summary of Designs

```{r, echo = FALSE, results = 'asis'}
data.frame(type = c("", "", "", "Case Control", 
                    "Cross-Sectional", "Cohort", "Randomized"),
           col2 = c("conditional", "probability", "explanatory",
                    "xxx", "xxx", "xxx", ""),
           col3 = c("conditional", "probability", "response",
                    "", "xxx", "xxx", "xxx"),
           col4 = c("", "difference of", "proportions", "",
                    "xxx", "xxx", "xxx"),
           col5 = c("", "Relative", "Risk", "",
                    "xxx", "xxx", "xxx"),
           col6 = c("", "Odds", "Ratio", 
                    "xxx", "xxx", "xxx", "xxx"),
           stringsAsFactors = FALSE) %>%
  dust(bookdown = TRUE) %>%
  sprinkle_colnames("", "", "", "", "", "") %>%
  sprinkle(pad = 3) %>%
  sprinkle(rows = 1, 
           border = "top") %>%
  sprinkle(rows = c(3:7), 
           border = "bottom") %>%
  print(asis = FALSE) %>% cat
```

<!--chapter:end:ExperimentalDesign.Rmd-->

# Exponential Distribution

## Probability Distribution Function

A random variable is said to have an Exponential Distribution with parameter $\beta$ if its probability distribution function is

\[f(x)=\left\{
	\begin{array}{ll}
		\frac{1}{\beta}e^{\frac{-x}{B}}, & 0<x,\ \ 0<\beta\\
		0 & otherwise 
	\end{array}\right.
\]

## Cumulative Distribution Function

$$\begin{align*}
F(x)
	&= \int\limits_{0}^{x}\frac{1}{\beta}\exp\Big\{{\frac{-t}{\beta}}\Big\}dt \\
	&= \exp\Big\{{\frac{-t}{\beta}}\Big\}\Big|_0^x \\
	&= \exp\Big\{{\frac{-x}{\beta}}\Big\}-\exp\Big\{{\frac{-0}{\beta}}\Big\} \\
  &= \exp\Big\{{\frac{0}{\beta}}\Big\}-\exp\Big\{{\frac{-x}{\beta}}\Big\} \\
	&= 1-\exp\Big\{{\frac{-x}{\beta}}\Big\}
\end{align*}$$

And so the cumulative distribution function is given by
\[F(x)=\left\{
	\begin{array}{ll}
		1-e^{\frac{-x}{\beta}}, & 0<x,\ 0<\beta\\
		0 & otherwise 
	\end{array} \right. 
\]

```{r Exponential_Distribution, echo = FALSE, fig.path = 'figures/', fig.cap = 'The figures on the top and bottom display the Exponential probability and cumulative distirubtion functions, respectively, for $\\beta=1,3$.'} 
Exponential <- 
  expand.grid(x = seq(0, 5, by = .01),
              beta = c(1, 3)) %>%
  mutate(dexp = dexp(x, rate = beta),
         pexp = pexp(x, rate = beta)) %>%
  gather(type, prob, -x, -beta) %>%
  mutate(beta = factor(beta),
         type = factor(type,
                       levels = c("dexp",
                                  "pexp"),
                       labels = c("Probability Distribution",
                                  "Cumulative Distribution")))

ggplot(data = Exponential,
       mapping = aes(x = x,
                     y = prob,
                     colour = beta)) + 
  geom_line() + 
  facet_grid(type ~ ., scales = "free_y") + 
  theme_bw() + 
  scale_colour_manual(values = palette[c(1, 9)]) + 
  xlab("X") + 
  ylab("Probability")
```


## Expected Values

$$\begin{align*}
E(X)
	      &= \int\limits_{0}^{\infty}xf(x)dx \\
	      &= \int\limits_{0}^{\infty}x\frac{1}{\beta}e^{\frac{-x}{\beta}}dx \\
	      &= \frac{1}{\beta}\int\limits_{0}^{\infty}xe^{\frac{-x}{\beta}}dx \\
	      &= \frac{1}{\beta}\int\limits_{0}^{\infty}x^{2-1}e^{\frac{-x}{\beta}}dx\\
^{[1]}  &= \frac{1}{\beta}(\beta^2\Gamma(2)) \\
        &=\frac{\beta^2\cdot 1!}{\beta} \\
        &=\beta
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx = \beta^\alpha\Gamma(\alpha)$

$$\begin{align*}
E(X^2)
      	&= \int\limits_{0}^{\infty}x^2f(x)dx \\
      	&= \int\limits_{0}^{\infty}x^2\frac{1}{\beta}e^{\frac{-x}{\beta}}dx \\
      	&= \frac{1}{\beta}\int\limits_{0}^{\infty}x^2e^{\frac{-x}{\beta}}dx \\
      	&= \frac{1}{\beta}\int\limits_{0}^{\infty}x^{3-1}e^{\frac{-x}{\beta}}dx \\
^{[1]}  &= \frac{1}{\beta}(\beta^3\Gamma(3)) \\
        &= \frac{\beta^3\cdot 2!}{\beta} \\
        &= 2\beta^2
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx = \beta^\alpha\Gamma(\alpha)$

$$\begin{align*}
\mu
	&= E(X) \\
	&= \beta \\
\\
\\
\sigma^2
	&= E(X^2)-E(X)^2 \\
	&= 2\beta^2-\beta^2 \\
	&= \beta^2
\end{align*}$$


## Moment Generating Function

$$\begin{align*} 
M_X(t)
	&= E(e^{tX}) \\
	&= \int\limits_{0}^{\infty}e^{tx}\frac{1}{\beta}e^{\frac{-x}{\beta}}dx \\
	&= \frac{1}{\beta}\int\limits_{0}^{\infty}e^{tx}e^{\frac{-x}{\beta}}dx \\
	&= \frac{1}{\beta}\int\limits_{0}^{\infty}e^{tx-\frac{x}{\beta}}dx \\
  &= \frac{1}{\beta}\int\limits_{0}^{\infty}e^{\frac{\beta tx}{\beta}-\frac{x}{\beta}}dx \\
	&= \frac{1}{\beta}\int\limits_{0}^{\infty}e^{\frac{\beta tx-x}{\beta}}dx \\
	&= \frac{1}{\beta}\int\limits_{0}^{\infty}e^{\frac{-x(\beta 1-\beta t}{\beta}}dx \\
  &= \frac{1}{\beta}(\frac{-\beta}{1-\beta t})e^{\frac{-x(1-\beta t}{\beta}}|_0^\infty \\
	&= \frac{-1}{1-\beta t}e^{\frac{-x(1-\beta t}{\beta}}|_0^\infty \\
	&= \frac{-1}{1-\beta t}\cdot 0-\frac{-1}{1-\beta t}e^0 \\
  &= \frac{1}{1-\beta t}=(1-\beta t)^{-1} \\
\\
\\
M_X^{(1)}(t)
	&= -1(1-\beta t)^{-2}(-\beta) \\
	&= \beta(1-\beta t)^{-2} \\
\\
\\
M_X^{(2)}(t)
	&= -2\beta(1-\beta t)^{-3}(-\beta) \\
	&= 2\beta^2(1-\beta t)^{-3} \\
\\
\\
E(X)
	&= M_X^{(1)}(0) \\
	&= \beta(1-\beta\cdot 0)^{-2} \\
	&= \beta(1-0)^{-2} \\
	&= \beta(1)^{-2} \\
	&= \beta \\
\\
\\
E(X^2)
	&= M_X^{(2)}(0) \\
	&= 2\beta^2(1-\beta\cdot 0)^{-3} \\
	&= 2\beta^2(1-0)^{-3} \\
	&= 2\beta^2(1)^{-3} \\
	&= 2\beta^2 \\
\\
\\
\mu
	&= E(X) \\
	&= \beta \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= 2\beta^2 - \beta^2 \\
	&= \beta^2
\end{align*}$$


## Maximum Likelihood Estimator

Let $x_1,x_2,\ldots,x_n$ be a random sample from an Exponential distribution with parameter $\beta$.

### Likelihood Function

$$\begin{align*}
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta) \\
	&= f(x_1|\theta)f(x_2|\theta)\cdots f(x_n|\theta)\\
  &= \frac{1}{\theta}\exp\bigg\{-\frac{x_1}{\theta}\bigg\}
		\cdot\frac{1}{\theta}\exp\bigg\{-\frac{x_n}{\theta}\bigg\}
		\cdots\frac{1}{\theta}\exp\bigg\{-\frac{x_n}{\theta}\bigg\} \\
	&= \frac{1}{\theta^n}\exp\bigg\{-\frac{1}{\theta}\sum\limits_{i=1}^{n}x_i\bigg\}
\end{align*}$$

### Log-likelihood Function
$$\begin{align*} 
\ell(\theta)
	&= \ln(L(\theta)) \\
	&= \ln(1)-n\ln(\theta)-\frac{1}{\theta}\sum\limits_{i=1}^{n}x_i \\
	&= 0-n\ln(\theta)-\theta^{-1}\sum\limits_{i=1}^{n}x_i \\
  &= -n\ln(\theta)-\theta^{-1}\sum\limits_{i=1}^{n}x_i
\end{align*}$$

### MLE for $\beta$

$$\begin{align*} 
\frac{d\ell(\beta)}{d\beta}
	&= -\frac{n}{\beta}+\beta^2\sum\limits_{i=1}^{n}x_i \\
\\
\\
0                                 &= -\frac{n}{\beta}+\beta^2\sum\limits_{i=1}^{n}x_i \\
\Rightarrow\frac{n}{\beta}        &= \beta^2\sum\limits_{i=1}^{n}x_i \\
\Rightarrow\frac{n\beta^2}{\beta} &= \sum\limits_{i=1}^{n}x_i \\
\Rightarrow n\beta                &= \sum\limits_{i=1}^{n}x_i \\
\Rightarrow \beta                 &= \frac{1}{n}\sum\limits_{i=1}^{n}x_i
\end{align*}$$

So $\hat\beta=\frac{1}{n}\sum\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $\beta$.


## Theorems for the Exponential Distribution

### Validity of the Distribution

$$ \int\limits_{0}^{\infty}\frac{1}{\beta}e^{\frac{-x}{B}}dx = 1 $$

_Proof:_

$$\begin{align*}
\int\limits_{0}^{\infty}\frac{1}{\beta}e^{\frac{-x}{\beta}}dx
	&= -e^{\frac{-x}{\beta}}\Big|_0^\infty \\
	&= -e^{\frac{-\infty}{\beta}}-(-e^{\frac{-0}{\beta}}) \\
	&= e^{\frac{0}{\beta}}-e^{\frac{-\infty}{\beta}} \\
	&= 1-0 \\
	&= 1
\end{align*}$$


### Sum of Exponential Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from an Exponential distribution with parameter $\beta$, i.e. $X_i\sim$Exponential$(\beta)$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.  Then $Y\sim$Gamma$(n,\beta)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{t(X_1+X_2+\cdots+X_n}) \\
	&= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
  &= (1-\beta t)^{-1}(1-\beta t)^{-1}\cdots(1-\beta t)^{-1} \\
	&= (1-\beta t)^{-n}
\end{align*}$$

Which is the mgf for a Gamma random variable with parameters $n$ and $\beta$.  Thus $Y\sim$Gamma$(n,\beta)$. 

<!--chapter:end:Exponential_Distribution.Rmd-->

# Functions

## Fundamental Concepts and Definitions

_Much of this chapter is taken from the lectures of Dr. John Brunette, University of Southern Maine _

A **function** is a collection of ordered pairs in which no two pairs have the same first element.

The set of all _first_ members of the pairs is called the **domain**.

The set of all _second_ members of the pairs is called the **range**.

Suppose now that for any function $f$ we have two items $x$ and $y$ such that $x\in dom(f)$ and $y\in ran(x)$  where $dom(f)$ and $ran(f)$ denote the domain and range of $f$, respectively.  It is said that $f$ maps $x$ onto $y$, written

\[f:\ x\mapsto y\]

It is common to write the $ran(f)$ as some expression of $x$.  For example, $f: x\mapsto x^2$ takes each element in the domain, and pairs it with it's square.  The common shorthand for this is $f(x)=x^2$, meaning that whatever appears between the parentheses following the $f$ is to be squared.

### Function Operations

The three basic operations that can be performed on functions are addition, multipilication, and composition.  For any two functions $f$ and $g$ these operations are defined as:

|                |      |
|----------------|------|
| Addition       | $\lbrack f+g\rbrack(x)=:\big\{\big(x,f(x)+g(x)\big)\mid x\in dom(f)\cap dom(g)\big\}$      |
| Multiplication |  $\lbrack f\cdot g\rbrack(x):=\big\{\big(x,f(x)\cdot g(x)\big) \mid x\in dom(f)\cap dom(g)\big\}$     |
| Composition    | $\lbrack f\circ g\rbrack(x)=\big\{\big(x,f\big(g(x)\big) \mid x\in dom(g)$ and  $g(x)\in dom(f)\big\}$      |


Notice that the composition $\lbrack f\circ g\rbrack(x)=f \circ g: g(x)\mapsto f(x)$.  In other words, the result of $g$ is then applied to $f$ to produce the result of the composition.


## Identities and Inverses

Recall that addition and multiplication have identity properties.  Specifically, for any real number $x$, applying one of these identities returns the value $x$, i.e. $x+0=x$ and $x\cdot 1$=x.  Functions also have an identity, denoted $id(x)$, that is defined as

$$id:\ x\mapsto x$$

Furthermore, the composition of $id$ with $f$ behaves in this way:

$$id\circ f=f\circ id=f$$

Functions may also exhibit the property of inverses that are exhibited by addition and multiplication.  In the latter two, combining any real number $x$ and its inverse returns the identity of that operation, i.e. $x+-x=0$ and $x\cdot x^{-1}=1,\ x\neq 0$.  Likewise, some functions have an inverse function.  If a function $f$ has an inverse $f^{-1}$, then

$$f\circ f^{-1}=f^{-1}\circ f=id$$

On closer observation, we see 

$$f^{-1}\circ f\big(dom(x)\big)=f^{-1}\Big(f\big(dom(x)\big)\Big)=f^{-1}\big(ran(x)\big)=dom(x)$$

So $f^{-1}$ must be the set of all ordered pairs $(y,x)$ where $x\in dom(x)$ and $y\in ran(x)$, i.e. $f^{-1}(x)=\{(y,x) \mid x\in dom(x)$ and $y\in ran(x)\}$.  By the definition of functions, no two first elements in $f^{-1}$ can be the same.  But the first elements in $f^{-1}$ are the second elements in $f$.  So $f^{-1}$ only exists if no two second elements in $f$ are the same.  We thus make the following definition:

A function $f$ is called a **one-to-one** function if it has no two ordered pairs with the same second element.

For any one-to-one function $f$, no two of the first elements are the same, and no two of the second elements are the same.  Thus, $f^{-1}$ is a function, because no two of its first elements are the same, and because the range of $f^{-1}$ is the domain of $f$, no two second elements in $f^{-1}$ are the same, and $f^{-1}$ is a one-to-one function.  Thus, every one-to-one function has an inverse.

If a function $f$ is not one-to-one, however, then there exist two pairs in $f$ that have the same second element.  The inverse $f^{-1}$ therefore has two pairs where the first element is the same.  When such is the case, the definition of a function is violated, and $f^{-1}$ cannot be a function.  Thus, if a function is invertible, it must be one-to-one.


## Odd and Even Functions

A function is said to be _even_ if for any real number $x,\ f(-x)=f(x)$.

A function is said to be _odd_ if for any real number $x,\ f(-x)=-f(x)$.

If neither of these criteria are met, the function is simply said to be neither odd nor even.


## Theorems


### Operations on Even Functions

Let $f$ and $g$ both be even functions.  Then:

i.   $[f+g](x)$ is an even function
ii.  $[f\cdot g](x)$ is an even function
iii. $[f\circ g](x)$ is an even function.

_Proof:_

i. $$\begin{align*}
     [f+g](-x)
        &= f(-x)+g(-x) \\
	      &= f(x)+g(x) \\
	      &= [f+g](x)
	 \end{align*}$$

so $[f+g](x)$ is an even function.


ii. $$\begin{align*}
      [f\cdot g](-x)
        &= f(-x)\cdot g(-x) \\
	      &= f(x)\cdot g(x) \\
	      &= [f\cdot g](x)
	  \end{align*}$$
	  
so $[f\cdot g](x)$ is an even function.

iii. $$\begin{align*}
      [f\circ g](-x)
        &= f\big(g(-x)\big) \\
	      &= f\big(g(x)\big) \\
	      &= [f\circ g](x)
	   \end{align*}$$
	   
so $[f\circ g](x)$ is an even function.



### Operations on Odd Functions

Let $f$ and $g$ both be odd functions.  Then:

i.   $[f+g](x)$ is an odd function
ii.  $[f\cdot g](x)$ is an even function
iii. $[f\circ g](x)$ is an odd function.

_Proof:_

i. $$\begin{align*} 
    [f+g](-x)
      &= f(-x) + g(-x) \\
	    &= -f(x) - g(x) \\
	    &= -[f+g](x)
	 \end{align*}$$
	 
so $[f+g](x)$ is an odd function.

ii. $$\begin{align*}
      [f\cdot g](-x)
        &= f(-x)\cdot g(-x) \\
	      &= -f(x)\cdot -g(x) \\
	      &= f(x)\cdot g(x) \\
	      &= [f\cdot g](x)
	   \end{align*}$$
	   
so $[f\cdot g](x)$ is an even function.

iii. $$\begin{align*}
      [f\circ g](-x)
        &= f\big(g(-x)\big) \\
	      &= f\big(-g(x)\big) \\
	      &= -f\big(g(x)\big) \\
	      &= -[f\circ g](x)
	   \end{align*}$$
	   
so $[f\circ g](x)$ is an odd function.



### Operations on an Odd and Even Function

Let $f$ be an even function and let $g$ both be an odd function.  Then:

i.   $[f+g](x)$ is neither an odd nor an even function
ii.  $[f\cdot g](x)$ is an odd function
iii. $[f\circ g](x)$ is an even function
iv.  $[g\circ f](x)$ is an even function.

_Proof:_

i. $$\begin{align*}
    [f+g](-x)
      &= f(-x) + g(-x) \\
	    &= -f(x) - g(x)
	 \end{align*}$$
 
so $[f+g](x)$ is neither an odd nor an even function.

ii. $$\begin{align*}
      [f\cdot g](-x)
        &= f(-x)\cdot g(-x) \\
	      &= f(x)\cdot -g(x) \\
	      &= -\big(f(x)\cdot g(x)\big) \\
	      &= -[f\cdot g](x)
	   \end{align*}$$
	   
so $[f\cdot g](x)$ is an odd function.

iii. $$\begin{align*}
      [f\circ g](-x)
        &= f\big(g(-x)\big) \\
	      &= f\big(-g(x)\big) \\
	      &= f\big(g(x)\big) \\
	      &= [f\circ g](x)
	   \end{align*}$$

so $\lbrack f\circ g\rbrack(x)$ is an even function.

iv. $$\begin{align*}
      [g\circ f](-x)
        &= g\big(f(-x)\big) \\
	      &= g\big(f(x)\big) \\
	      &= [g\circ f](x)
	   \end{align*}$$

so $\lbrack f\circ g\rbrack(x)$ is an even function.
\end{itemize}



### Derivatives and Anti-derivatives of Odd Functions

Let $f$ be an odd function and let $f^\prime$ and $F$ denote the derivative and anti-derivative of $f$, respectively.  Then $f^\prime$ and $F$ are both even functions.

_Proof:_

$$\begin{align*}
  f(-x) &= -f(x)\\
\Rightarrow \frac{d}{dx}\big\lbrack f(-x)\big\rbrack
		&= \frac{d}{dx}\big\lbrack-f(x)\big\rbrack \\
\Rightarrow f^\prime(-x)\cdot -1 
    &= -f^\prime(x) \\
\Rightarrow -f^\prime(-x) 
    &= -f^\prime(x) \\
\Rightarrow f^\prime(-x) 
    &= f^\prime(x)
\end{align*}$$

So $f^\prime$ is an even function.

$$\begin{align*}
f(-x)                    &= -f(x) \\
\Rightarrow \int f(-x)   &= \int-f(x)\\
\Rightarrow F(-x)\cdot-1 &= -F(x)\\
\Rightarrow -F(-x)       &= -F(x)\\
\Rightarrow F(-x)        &= F(x)
\end{align*}$$

So $F$ is also an even function.\ \ \rule{.05in}{.05in}




### Derivatives and Anti-derivatives of Even Functions

Let $g$ be an even function, and let $g^\prime$ and $G$ denote the derivative and anti-derivative of $g$, respectively.  Then $g^\prime$ and $G$ are both odd functions.

_Proof:_

$$\begin{align*}
g(-x) &= g(x) \\
\Rightarrow \frac{d}{dx}\big\lbrack g(-x)\big\rbrack
		&= \frac{d}{dx}\big\lbrack g(x)\big\rbrack \\
\Rightarrow g^\prime(-x)\cdot -1 
    &= g^\prime(x) \\
\Rightarrow -g^\prime(-x) 
    &= g^\prime(x) \\
\Rightarrow g^\prime(-x) 
    &= -g^\prime(x)
\end{align*}$$

So $g^\prime$ is an odd function.

$$\begin{align*}
g(-x)                    &= g(x)\\
\Rightarrow \int g(-x)   &= \int g(x)\\
\Rightarrow G(-x)\cdot-1 &= G(x)\\
\Rightarrow -G(-x)       &= G(x)\\
\Rightarrow G(-x)        &= -G(x)
\end{align*}$$

So $G$ is also an odd function.


<!--chapter:end:Functions.Rmd-->

# Gamma Distribution

## Probability Distribution Function

A random variable $X$ is said to have a Gamma Distribution with parameters $\alpha$ and $\beta$ if its probability distribution function is
\[f(x)=\left\{
	\begin{array}{ll}
		\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}{\Gamma(\alpha)\beta^\alpha},
			& 0<x,\ 0<\alpha,\ 0<\beta\\
		0 & otherwise 
	\end{array} \right.
\]
Where $\alpha$ is a scale parameter and\\
\indent $\beta$ is a shape parameter.

## Cumulative Distribution Function

The cumulative distribution function for the Gamma Distribution cannot be expressed in closed form.  It's interval form is expressed here.

\[F(x) = \left\{
	\begin{array}{ll}
		\int\limits_{0}^{x}\frac{t^{\alpha-1}e^{-\frac{t}{\beta}}}{\Gamma(\alpha)\beta^\alpha},
			& 0<t,\ 0<\alpha,\ 0<\beta\\
		\\
		0 & otherwise 
	\end{array} \right.
\]

```{r Gamma_Distribution, echo = FALSE, fig.path = 'figures/', fig.cap = 'The figures on the left and right display the Gamma probability and cumulative distirubtion functions, respectively, for the combinations of $\\alpha=2,3$ and $\\beta=1,3$.'}
Gamma <- 
  expand.grid(x = seq(0, 10, by = .01),
              alpha = c(2, 3),
              beta = c(1, 3)) %>%
  mutate(dgamma = dgamma(x, shape = alpha, rate = beta),
         pgamma = pgamma(x, shape = alpha, rate = beta)) %>%
  gather(type, prob, -x, -alpha, -beta) %>%
  mutate(alpha = factor(alpha),
         beta = factor(beta),
         type = factor(type,
                       levels = c("dgamma",
                                  "pgamma"),
                       labels = c("Probability Distribution",
                                  "Cumulative Distribution")))

ggplot(data = Gamma,
       mapping = aes(x = x,
                     y = prob,
                     colour = alpha,
                     linetype = beta)) + 
  geom_line() + 
  facet_grid(type ~ ., scales = "free_y") + 
  theme_bw() + 
  scale_colour_manual(values = palette[c(1, 9)]) + 
  xlab("X") + 
  ylab("Probability")
```


## Expected Values

$$\begin{align*} 
E(X)
      	&= \int\limits_{0}^{\infty}x\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}
      		{\Gamma(\alpha)\beta^\alpha}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x\cdot x^{\alpha-1}e^{-\frac{x}{\beta}}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha}e^{-\frac{x}{\beta}}dx \\
^{[1]}  &= \frac{1}{\Gamma(\alpha)\beta^\alpha}
		[\Gamma(\alpha+1)\beta^{\alpha+1}] \\
      	&= \frac{\Gamma(\alpha+1)\beta^{\alpha+1}}{\Gamma(\alpha)\beta^\alpha} \\
      	&= \frac{\alpha\Gamma(\alpha)\beta^{\alpha+1}}{\Gamma(\alpha)\beta^\alpha} \\
      	&= \alpha\beta
\end{align*}$$
	
> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$


$$\begin{align*} 
E(X^2)
      	&= \int\limits_{0}^{\infty}x^2\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}
      		{\Gamma(\alpha)\beta^\alpha}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^2\cdot 
      		x^{\alpha-1}e^{-\frac{x}{\beta}}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha+1}e^{-\frac{x}{\beta}}dx \\
^{[1]}  &= \frac{1}{\Gamma(\alpha)\beta^{\alpha}}
      		[\Gamma(\alpha+2)\beta^{\alpha+2}] \\
      	&= \frac{\Gamma(\alpha+2)\beta^{\alpha+2}}{\Gamma(\alpha)\beta^\alpha} \\
      	&= \frac{(\alpha+1)\Gamma(\alpha+1)\beta^{\alpha+2}}
      		{\Gamma(\alpha)\beta^\alpha} \\
        &= \frac{(\alpha+1)\alpha\Gamma(\alpha)\beta^{\alpha+2}}
      		{\Gamma(\alpha)\beta^\alpha} \\
      	&= \alpha(\alpha+1)\beta^2 \\
      	&= (\alpha^2+\alpha)\beta^2 \\
      	&= \alpha^2\beta^2+\alpha\beta^2 
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$
		
$$\begin{align*}
\mu
	&= E(X) \\
	&= \alpha\beta \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \alpha^2\beta^2 + \alpha\beta^2 - \alpha^2\beta^2 \\
	&= \alpha\beta^2
\end{align*}$$



## Moment Generating Function

$$\begin{align*} 
M_X(t) 
      	&= E(e^{tX}) \\
      	&= \int\limits_{0}^{\infty}e^{tx}
      		\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}
      		{\Gamma(\alpha)\beta^\alpha}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}e^{tx}
      		x^{\alpha-1}e^{-\frac{x}{\beta}}dx \\
        &= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{tx}e^{-\frac{x}{\beta}}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{tx-\frac{x}{\beta}}dx \\
        &= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{\frac{\beta tx}{\beta}-\frac{x}{\beta}}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{\frac{\beta tx-x}{\beta}}dx \\
        &= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{-x\frac{-\beta t+1}{\beta}}dx \\
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}
      		e^{-x\frac{1-\beta t}{\beta}}dx \\
^{[1]}  &= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\Big[\Gamma(\alpha)\Big(\frac{\beta}{1-\beta t}\Big)\alpha)\Big] \\
      	&= \frac{\Gamma(\alpha)\beta^\alpha}
      		{\Gamma(\alpha)\beta^\alpha(1-\beta t)^\alpha} \\
        &= \frac{1}{(1-\beta t)^\alpha}=(1-\beta t)^{-\alpha}
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
	      =\beta^\alpha\Gamma(\alpha)$

$$\begin{align*}
M_X^{(1)}(t)
	&= -\alpha(1-\beta t)^{-\alpha-1}(-\beta) \\
	&= \alpha\beta(1-\beta t)^{-\alpha-1} \\
M_X^{(2)}(t) 
	&= (-\alpha-1)\alpha\beta(1-\beta t)^{-\alpha-2}(-\beta) \\
	&= (\alpha+1)\alpha\beta^2(1-\beta t)^{-\alpha-2} \\
  &= (\alpha^2\beta^2+\alpha\beta^2)(1-\beta t)^{-\alpha-2} \\
\\
\\
E(X)
	&= M_X^{(1)}(0)=\alpha\beta(1-\beta\cdot 0)^{-\alpha-1} \\
	&= \alpha\beta(1-0)^{\alpha-1}=\alpha\beta(1)^{-\alpha-1} \\
	&= \alpha\beta \\
\\
\\
E(X^2)
	&= M_X^{(2)}(0)=(\alpha^2\beta^2+\alpha\beta^2)(1-\beta 0)^{-\alpha-2} \\
	&= (\alpha^2\beta^2+\alpha\beta^2)(1-0)^{-\alpha-2} \\
  &= (\alpha^2\beta^2+\alpha\beta^2)(1)^{-\alpha-2} \\
	&= \alpha^2\beta^2+\alpha\beta^2 \\
\\
\\
\mu
	&= E(X) \\
	&= \alpha\beta\\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \alpha^2\beta^2 + \alpha\beta^2 - \alpha^2\beta^2 \\
	&= \alpha\beta^2
\end{align*}$$



## Maximum Likelihood Estimators

Let $x_1,x_2,\ldots,x_n$ denote a random sample from a Gamma Distribution with parameters $\alpha$ and $\beta$.


### Likelihood Function

$$\begin{align*}
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta) \\
	&= f(x_1|\theta) f(x_2|\theta) \cdots f(x_n|\theta) \\
  &= \frac{x_1^{\alpha-1}e^{-x_1/\beta}}{\Gamma(\alpha)\beta^\alpha}
		\frac{x_2^{\alpha-1}e^{-x_2/\beta}}{\Gamma(\alpha)\beta^\alpha}
		\cdots \frac{x_n^{\alpha-1}e^{-x_n/\beta}}{\Gamma(\alpha)\beta^\alpha} \\
  &= \prod\limits_{i=1}^{n}\frac{x_i^{\alpha-1}e^{-x_i/\beta}}{\Gamma(\alpha)\beta^\alpha} \\
	&= \bigg(\frac{1}{\Gamma(\alpha)\beta^\alpha}\bigg)^n  
	      \prod\limits_{i=1}^{n}x_i^{\alpha-1}e^{-x_i/\beta} \\
  &= \big( \Gamma(\alpha)\beta^\alpha \big)^{-n} 
        \prod\limits_{i=1}^{n}x_i^{\alpha-1}e^{-x_i/\beta} \\
  &= \big( \Gamma(\alpha)\beta^\alpha \big)^{-n} 
		\exp\bigg\{\sum\limits_{i=1}^{n}-\frac{x_i}{\beta} \bigg\}
		\prod\limits_{i=1}^{n}x_i^{\alpha-1} \\
  &= \big( \Gamma(\alpha)\beta^\alpha \big)^{-n} 
		\exp\bigg\{-\frac{1}{\beta}\sum\limits_{i=1}^{n}x_i \bigg\}
		\prod\limits_{i=1}^{n}x_i^{\alpha-1}
\end{align*}$$


### Log-likelihood Function

$$\begin{align*}
\ell(\theta)
	&= \ln\bigg[ \big( \Gamma(\alpha)\beta^\alpha \big)^{-n} 
		\exp\bigg\{-\frac{1}{\beta}\sum\limits_{i=1}^{n}x_i \bigg\}
		\prod\limits_{i=1}^{n}x_i^{\alpha-1} \bigg]  \\
  &= \ln\big( \Gamma(\alpha) \beta^\alpha \big)^{-n}
		+ \ln\bigg( \exp \bigg\{ -\frac{1}{\beta}\sum\limits_{i=1}^{n}x_i \bigg\} \bigg)
		+ \ln\bigg( \prod\limits_{i=1}^{n}x_i^{\alpha-1} \bigg) \\
  &= -n\ln\big( \Gamma(\alpha) \beta^\alpha \big)
		- \frac{1}{\beta}\sum\limits_{i=1}^{n}x_i
		+ \ln\bigg( \prod\limits_{i=1}^{n}x_i^{\alpha-1} \bigg) \\
  &= -n\big[ \ln\big( \Gamma(\alpha)\beta^\alpha\big) \big]
		- \frac{1}{\beta}\sum\limits_{i=1}^{n}x_i
		+ \sum\limits_{i=1}^{n}(\alpha-1)\ln x_i \\
  &= -n\ln\Gamma(\alpha) - n\alpha\ln\beta
		- \frac{1}{\beta}\sum\limits_{i=1}^{n}x_i
		+ (\alpha-1)\sum\limits_{i=1}^{n}\ln x_i
\end{align*}$$



### MLE for $\alpha$

$$\begin{align*}
\frac{d\ell}{d\alpha}
	&= -n\frac{1}{\Gamma(\alpha)}\Gamma^\prime(\alpha) - n\ln\beta - 0
		+ \sum\limits_{i=1}^{n}\ln x_i \\
  &= -n\frac{\Gamma^\prime(\alpha)}{\Gamma(\alpha)} - n\ln\beta
		+ \sum\limits_{i=1}^{n}\ln x_i\\
\\
\\
0 &= -n\frac{\Gamma^\prime(\alpha)}{\Gamma(\alpha)} - n\ln\beta
		+ \sum\limits_{i=1}^{n}\ln x_i \\
\Rightarrow n\frac{\Gamma^\prime(\alpha)}{\Gamma(\alpha)}
		&= \sum\limits_{i=1}^{n}\ln x_i - n\ln\beta \\
\Rightarrow \frac{\Gamma^\prime(\alpha)}{\Gamma(\alpha)}
		&= \frac{1}{n}\bigg( \sum\limits_{i=1}^{n}\ln x_i - n\ln\beta \bigg)
\end{align*}$$

However, this must be solved numerically.  Notice also that the MLE for $\alpha$ depends on $\beta$.



### MLE for $\beta$

$$\begin{align*}
\frac{d\ell}{d\beta}
	&= 0 - n\alpha\frac{1}{\beta} + \frac{1}{\beta^2}\sum\limits_{i=1}^{n}x_i + 0 \\
	&= -\frac{n\alpha}{\beta} + \frac{1}{\beta^2}\sum\limits_{i=1}^{n}x_i \\
\\
\\
0 &= -\frac{n\alpha}{\beta} + \frac{1}{\beta^2}\sum\limits_{i=1}^{n}x_i \\
\Rightarrow \frac{n\alpha}{\beta} &= \frac{1}{\beta^2}\sum\limits_{i=1}^{n}x_i \\
\Rightarrow n\alpha\beta &= \sum\limits_{i=1}^{n}x_i \\
\Rightarrow \beta &= \frac{1}{n\alpha} \sum\limits_{i=1}^{n}x_i
\end{align*}$$

This estimate, however, depends on $\alpha$.  Since each estimator depends on the value of the other parameter, we must maximize the likelihood functions simulatneously.  That is, we must simultaneously solve the system
\[ \left\{
	\begin{array}{rl}
		-n\frac{\Gamma^\prime(\alpha)}{\Gamma(\alpha)} - n\ln\beta
		+ \sum\limits_{i=1}^{n}\ln x_i 
			& = 0\\
		-\frac{n\alpha}{\beta} + \frac{1}{\beta^2}\sum\limits_{i=1}^{n}x_i
			& = 0\\
	\end{array} \right.
\]
Solving this system will require numerical methods.



### Approximation of $\hat\alpha$ and $\hat\beta$

Approximations of $\hat\alpha$ and $\hat\beta$ can be obtained by noticing that\\
$$\begin{align*}
\frac{d\ell}{d\beta}    &= 0 \\
\Rightarrow \beta       &= \frac{1}{n\alpha}\sum\limits_{i=1}^{n}x_i\\
\Rightarrow \alpha\beta &= \frac{1}{n}\sum\limits_{i=1}^{n}x_i
\end{align*}$$

So $\widehat{\alpha\beta} = \frac{1}{n}\sum\limits_{i=1}^{n}x_i$.  Recall that $\alpha\beta$ and $\alpha\beta^2$ are the mean and variance of the Gamma Distribution, respectively.  We utilize
\[ \frac{\alpha\beta^2}{\alpha\beta} = \beta \]

If we assume that $\widehat{\alpha\beta^2} = \frac{1}{n-1}\sum\limits_{i=1}^{n}(x_i-\bar x)^2$, then

\[ \frac{\widehat{\alpha\beta^2}}{\widehat{\alpha\beta}} = \beta^* \approx \hat\beta \]
Where $\beta^*$ denotes an approximation to $\hat\beta$

We now substitute $\beta^*$ into

$$\begin{align*}
\widehat{\alpha\beta}       &= \frac{1}{n}\sum\limits_{i=1}^{n}x_i\\
\Rightarrow \alpha^*\beta^* &= \frac{1}{n}\sum\limits_{i=1}^{n}x_i\\
\Rightarrow \alpha^*        &= \frac{1}{n\beta^*}\sum\limits_{i=1}^{n}x_i \approx \hat\alpha
\end{align*}$$

Where $\alpha^*$ denotes an approximation to $\hat\alpha$.

This method of estimation is prone to error because $\beta^*$ is found through two levels of estimation and $\alpha^*$ is found through three levels of estimation.  Surely, this process inflates the error of estimation.  At this point, however, I have no information to indicate how badly the error of estimation is inflated, nor have I performed any investigation into this problem.



## Theorems for the Gamma Distribution


### Validity of the Distribution

$$ \int\limits_{0}^{\infty}x\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}
	{\Gamma(\alpha)\beta^\alpha}dx = 1$$

_Proof:_

$$\begin{align*}
 \int\limits_{0}^{\infty}\frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}
		{\Gamma(\alpha)\beta^\alpha}dx 
      	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}
      		\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx \\
^{[1]}	&= \frac{1}{\Gamma(\alpha)\beta^\alpha}[\Gamma(\alpha)\beta^\alpha] \\
        &= \frac{\Gamma(\alpha)\beta^\alpha}{\Gamma(\alpha)\beta^\alpha} \\
        &= 1
\end{align*}$$

> 1. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		= \beta^\alpha\Gamma(\alpha)$


### Sum of Gamma Random Variables

Let $X_1,X_2,\ldots,X_n$ be Gamma distributed random variables with parameters $\alpha_i$ and $\beta$, that is $X_i\sim$Gamma$(\alpha_i,\beta)$.  Let $Y = \sum\limits_{i=1}^{n}X_i$.\\
Then $Y\sim$Gamma$(\sum\limits_{i=1}^{n}\alpha_i,\beta)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY})=E(e^{t(X_1+X_2+\cdots+X_n)} \\
	&= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
	&= E(e^{tX_1})E(e^{tX_2})\cdots E(e^{tX_n}) \\
  &= (1-\beta t)^{-\alpha_1}(1-\beta t)^{-\alpha_2}\cdots
		(1-\beta t)^{-\alpha_n}=(1-\beta t)^{-\sum\limits_{i=1}^{n}\alpha_i}
\end{align*}$$

Which is the moment generating function of a Gamma random variable with parameters $\sum\limits_{i=1}^{n}\alpha_i$ and \nolinebreak$\beta$.  Thus $Y\sim$Gamma$(\sum\limits_{i=1}^{n}\alpha_i,\beta)$. 



### Sum of Exponential Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from an Exponential distribution with parameter $\beta$, i.e. $X_i\sim$Exponential$(\beta)$.  Let $Y = \sum\limits_{i=1}^{n}X_i$.  Then $Y\sim$Gamma$(n,\beta)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{t(X_1+X_2+\cdots+X_n}) \\
	&= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
  &= (1-\beta t)^{-1}(1-\beta t)^{-1}\cdots(1-\beta t)^{-1} \\
	&= (1-\beta t)^{-n}
\end{align*}$$

Which is the moment generating function for a Gamma random variable with parameters $n$ and $\beta$.  Thus $Y\sim$Gamma$(n,\beta)$. 



<!--chapter:end:Gamma_Distribution.Rmd-->

# Geometric Distribution

## Probability Mass Function

A random variable $X$ is said to have a Geometric Distribution with parameter $p$ if its probability mass function is

\[p(x) = \left\{
	\begin{array}{ll}
		p(1-p)^{x-1}, & x=1,2,3,\ldots\\
		0 & otherwise 
	\end{array} \right. 
\]

where $p$ is the probability of a success on any given trial and $x$ is the number of trials until the first success.


## Cumulative Distribution Function

The cumulative probability of $x$ is computed as the $x^{th}$ partial sum of the Geometric Series
	See \@ref(geometric-series-partial-and-infinite-summations).
	
$$\begin{align*} 
P(x) 
	&= \sum\limits_{i=1}^{x}p(1-p)^{x-1} \\
	&= \frac{p-p(1-p)^{x-1}}{1-(1-p)} \\
	&= \frac{p[1-(1-p)^{x-1}]}{p} \\
	&= 1-(1-p)^{x-1}
\end{align*}$$

So

\[P(x) = \left\{
	\begin{array}{ll}
		1-(1-p)^{x-1},& x=1,2,3,\ldots\\
		0 & otherwise 
	\end{array} \right. 
\]

A recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:

$$\begin{align*}
P(X=x+1)
	&= p(1-p)^x \\
	&= (1-p)p(1-p)^{x-1} \\
	&= (1-p)P(X=x)
\end{align*}$$

```{r, echo = FALSE, fig.path = 'figures/', fig.cap = 'The figures on the left and right display the Geometric probability and cumulative distirubtion functions, respectively, for the combinations of $p=.3$.'}
Geometric <- 
  data.frame(x = 0:5) %>%
  mutate(pmf = dgeom(x, p = 0.3),
         cmf = pgeom(x, p = 0.3)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             c("pmf", "cmf"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Geometric,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob), 
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:1) + 
  ylab("P(x)") + 
  theme_bw()
```



## Expected Values

$$\begin{align*} 
E(X)
      	&= \sum\limits_{x=1}^{\infty}x\cdot p(1-p)^{x-1} \\
      	&= p\sum\limits_{x=1}^{\infty}x\cdot (1-p)^{x-1} \\
^{[1]}	&= p\sum\limits_{x=1}^{\infty}x\cdot q^{x-1} \\
^{[2]}  &= p\frac{d}{dq}\Big(\sum\limits_{x=1}^{\infty}q^x\Big) \\
      	&= p\frac{d}{dq}\Big(\sum\limits_{x=1}^{\infty}q\cdot q^{x-1}\Big) \\
^{[3]}	&= p\frac{d}{dq}\Big(\frac{q}{1-q}\Big) \\
^{[4]}  &= p\Big(\frac{(1-q)\cdot 1-q(-1)}{(1-q)^2}\Big) \\
	      &= p\Big(\frac{1-q+q}{(1-q)^2}\Big) \\
	      &= p\Big(\frac{1}{(1-q)^2}\Big) \\
^{[5]}  &= p\cdot\frac{1}{p^2}
        &= \frac{p}{p^2}
        &= \frac{1}{p}
\end{align*}$$

> 1. Let $1-p = 1$
> 2. $a\cdot x^{a-1}=\frac{d}{dx}(x^a)$
> 3. $\sum\limits_{k=1}^{\infty}ar^{k-1}=\frac{a}{1-r}$ (Geometric Series \@ref(geometric-series-partial-and-infinite-summations)).
> 4. Product Rule for Differentiation:  
		$\frac{d}{dx}(\frac{f(x)}{g(x)})
		= \frac{g\prime(x)\cdot f(x)-f\prime(x)\cdot g(x)}{g^2(x)}$
> 5. $1-p=q \ \ \Rightarrow p=1-q$


$$\begin{align*} 
E[X(X-1)]
^{[1]}	&= \sum\limits_{x=2}^{\infty}x(x-1)p(1-p)^{x-1} \\
	      &= p(1-p)\sum\limits_{x=2}^{\infty}x(x-1)(1-p)^{x-2} \\
^{[2]}  &= pq\sum\limits_{x=2}^{\infty}x(x-1)q^{x-2} \\
^{[3]}	&= pq\frac{d^2}{dq^2}\Big(\sum\limits_{x=2}^{\infty}q^x\Big) \\
	      &= pq\frac{d^2}{dq^2}\Big(\sum\limits_{x=2}^{\infty}q\cdot q^{x-1}\Big) \\
^{[4]}  &= pq\frac{d^2}{dq^2}\Big(\frac{2q-1}{1-q}\Big) \\
^{[5]}	&= pq\frac{d}{dq}(-(1-q)^{-2}) \\
^{[6]}	&= pq\frac{2}{(1-q)^3} \\
        &= \frac{2pq}{(1-q)^3} \\
^{[7]}  &= \frac{2p(1-p)}{p^3} \\
        & =\frac{2(1-p)}{p^2}
\end{align*}$$

> 1. We start the summand at $x=2$ because the term for $x=1$ is 0.
> 2. Let $1-p = 1$
> 3. $a(a-1)x^{a-2}=\frac{d^2}{dx^2}x^a$
> 4. $\sum\limits_{k=1}^{\infty}ar^{k-1}
		= \frac{a}{1-r}=1+a+ar+ar^2+\cdots.$  The current series leaves out the first term,\\ 
		\ \ $\sum\limits_{k=2}^{\infty}ar^{k-1}=(\sum\limits_{k=1}^{\infty}ar^{k-1})-1
		= \frac{a}{1-r}-1=\frac{1}{1-r}-\frac{1-r}{1-r}=\frac{a-(1-r)}{1-r}=\frac{a+r-1}{r-1}$
> 5. $\frac{d}{dx}(\frac{2x-1}{1-x})=\frac{-(2x-1)-2(1-x)}{(1-x)^2}
		= \frac{2x+1-2+2x}{(1-x)^2}=\frac{-1}{(1-x)^2}=-(1-x)^{-2}$
> 6. $\frac{d}{dx}-(1-x)^{-2}=2(1-x)^{-3}=\frac{2}{(1-x)^3}$
> 7. See note number 5.
		
$$\begin{align*}
\mu
	&= E(X) \\
	&= \frac{1}{p} \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= E(X^2) - E(X) + E(X) - E(X)^2 \\
  &= [E(X^2)-E(X)]+E(X)-E(X)^2 \\
	&= E(X^2-X)+E(X)-E(X)^2 \\
  &= E[(X(X-1)]+E(X)-E(X)^2 \\
  &= \frac{2(1-p)}{p^2} + \frac{1}{p} - \frac{1}{p^2} \\
	&= \frac{2(1-p)}{p^2} + \frac{p}{p^2} - \frac{1}{p^2} \\
	&= \frac{2(1-p)+p-1}{p^2} \\
  &= \frac{2-2p+p-1}{p^2} \\
	&= \frac{2-1+p-2p}{p^2} \\
	&= \frac{1-p}{p^2}
\end{align*}$$



## Moment Generating Function

$$\begin{align*}
M_X(t)
      	&= E(e^{tX}) \\
      	&= \sum\limits_{x=1}^{\infty}e^{tx}p(1-p)^{x-1} \\
      	&= p\sum\limits_{x=1}^{\infty}e^{tx}(1-p)^{x-1} \\
        &= p\sum\limits_{x=1}^{\infty}e^{t^x}(1-p)^{x-1} \\
      	&= pe^t\sum\limits_{x=1}^{\infty}e^{t^{(x-1)}} \\
      	&= pe^t\sum\limits_{x=1}^{\infty}[(1-p)e^t]^{x-1} \\
^{[1]}  &= pe^t\frac{1}{1-(1-p)e^t} \\
        &= \frac{pe^t}{1-(1-p)e^t}
\end{align*}$$

> 1. $\sum\limits_{k=1}^{\infty}ar^{k-1}=\frac{a}{1-r}$, (Geometric Series \@ref(geometric-series-partial-and-infinite-summations))


$$\begin{align*} 
M_X^{(1)}(t)
	&= \frac{[1-(1-p)e^t]pe^t-pe^t[-(1-p)e^t]}{\big(1-(1-p)e^t\big)^2} \\
	&= \frac{pe^t[1-(1-p)e^t+(1-pe^t)]}{\big(1-(1-p)e^t\big)^2} \\
  &= \frac{pe^t(1)}{\big(1-(1-p)e^t\big)^2} \\
  &= \frac{pe^t}{\big(1-(1-p)e^t\big)^2} \\
\\
\\
M_X^{(2)}(t)
	&= \frac{(1-(1-p)e^t)^2pe^t-pe^t[-2(1-(1-p)e^t)(1-p)e^t}{\big(1-(1-p)e^t\big)^4} \\
  &= \frac{pe^t[(1-(1-p)e^t)^2+2(1-(1-p)e^t(1-p)e^t]}{\big(1-(1-p)e^t\big)^4} \\
\\
\\
E(X)
	&= M_X^{(1)}(0) \\
	&= \frac{pe^0}{\big(1-(1-p)e^0\big)^2} \\
	&= \frac{p}{\big(1-(1-p)\big)^2} \\
  &= \frac{p}{(1-1+p)^2} \\
	&= \frac{p}{p^2} \\
	&= \frac{1}{p}\\
\\
\\
E(X^2)
	&= M_X^{(2)}(0) \\
	&= \frac{pe^0[(1-(1-p)e^0)^2+2(1-(1-p)e^0)(1-p)e^0]}{(1-(1-p)e^0)^4} \\
  &= \frac{p[(1-(1-p))^2+2(1-(1-p))(1-p)]}{(1-(1-p))^4} \\
  &= \frac{p[(1-1+p)^2+2(1-1+p)(1-p)]}{(1-1+p)^4} \\
	&= \frac{p[p^2=2p(1-p)]}{p^4} \\
  &= \frac{p(p^2+2p-2p^2)}{p^4} \\
	&= \frac{p(2p-p^2)}{p^4} \\
	&= \frac{p^2(2-p)}{p^4} \\
	&= \frac{2-p}{p^2} \\
\\
\\ 
\mu 
	&= E(X) \\
	&= \frac{1}{p}\\
\\
\\ 
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \frac{2-p}{p^2} - \frac{1}{p^2} \\
	&= \frac{1-p}{p^2}
\end{align*}$$



## Maximum Likelihood Estimator

Let $x_1, x_2 , \ldots , x_n$ be a random sample from a Geometric distribution with parameter $p$.

### Likelihood Function

$$\begin{align*} 
L(\theta)
	&= P(x_1,x_2,\ldots,x_n|\theta) \\
	&= p(1-p)^{x-1}
\end{align*}$$

### Log-likelihood Function

$$\begin{align*}
\ell(\theta)
	&= \ln p(1-p)^{x-1} \\
	&= \ln p+\ln(1-p)^{x-1} \\
	&= \ln p+(x-1)\ln(1-p)
\end{align*}$$


### MLE for $p$
$$\begin{align*} 
\frac{d\ell}{dp}
	&= \frac{1}{p} + \frac{-(x-1)}{1-p} \\
	&= \frac{1}{p} + \frac{1-x}{1-p}\\
\\
\\
0                               &= \frac{1}{p}+\frac{1-x}{1-p}\\
\Rightarrow \frac{1-x}{1-p}     &= -\frac{1}{p}\\
\Rightarrow 1-x                 &= \frac{-1+p}{p}\\
\Rightarrow -x                  &= \frac{-1+p}{p}-1\\
\Rightarrow x=1-\frac{-1+p}{p}  &= \frac{p}{p}-\frac{-1+p}{p} \\
                                &= \frac{p+1-p}{p} \\
                                &= \frac{1}{p}\\
\Rightarrow p                   &= \frac{1}{x}
\end{align*}$$

So $$\hat p=\frac{1}{x}$$ is the Maximum Likelihood Estimator for $p$.


## Theorems for the Geometric Distribution

### Validity of the Distribution

$$\sum\limits_{i=1}^{\infty}p(1-p)^{x-1} = 1$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{\infty}p(1-p)^{x-1}
  	&= \frac{p}{1-(1-p)} \\
    &= \frac{p}{p} \\
    &= 1
\end{align*}$$

> 1. $S=\lim\limits_{k\rightarrow\infty}S_k
		=\lim\limits_{k\rightarrow\infty}\frac{a-ar^k}{1-r}$ (Geometric Series \@ref(geometric-series-partial-and-infinite-summations))


### Sum of Geometric Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from a Geometric Distribution with parameter $p$, that is, $X_i\sim$Geometric$(p)$, $i=1,2,3,\ldots$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.  Then $Y\sim$Negative Binomial$(n,p)$.

_Proof:_

$$\begin{align*} 
M_Y(t)
  &= E(e^{tY}) \\
  &= E(e^{t(X_1+X_2+\cdots+X_n)}) \\
  &= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
  &= E(e^{tX_1})E(e^{tX_2})\cdots E(e^{tX_n}) \\
	&=\frac{pe^t}{1-(1-p)e^t}\cdot\frac{pe^t}{1-(1-p)e^t}\cdot\ \cdots
		\ \cdot\frac{pe^t}{1-(1-p)e^t} \\
  &= \Big(\frac{pe^t}{1-(1-p)e^t}\Big)^n
\end{align*}$$

Which is the mgf of a Negative Binomial random variable with parameters $n$ and $p$.  Thus $Y\sim$Negative Binomial$(n,p)$. 


### Lemma {#geometric-distribution-memoryless-lemma}

Let $X$ be a Geometric random variable with parameter $p$.
Then $P(X>a)=(1-p)^a$.

_Proof:_

$$\begin{align*}
P(X>a)
  &= 1-P(X\leq a)=1-\sum\limits_{i=1}^{a}p(1-p)^i-1 \\
^{[1]}  &= 1-\frac{p-p(1-p)^a}{1-(1-p)} \\
        &= 1-\frac{p\big(1-(1-p)^a\big)}{1-1+p} \\
        &= 1-\frac{p\big(1-(1-p)^a\big)}{p} \\
        &= 1-\big(1-(1-p)^a\big) \\
        &= 1-1+(1-p)^a \\
        &= (1-p)^a
\end{align*}$$

> 1. $S_k=\lim\limits_{k\rightarrow\infty}\frac{a-ar^k}{1-r}$ (Geometric Series \@ref(geometric-series-partial-and-infinite-summations))


### Memoryless Property

$$P(X\geq k+j|X\geq k)=P(X\geq k)$$

_Proof:_

$$\begin{align*}
P(X > a+b)
^{[1]}  &= (1-p)^{a+b}=(1-p)^a(1-p)^b \\
\\
\\
P(X > k+j|X > k)
        &=\frac{P(X > k+j\cap X > k)}{P(X > k)} \\
^{[2]}	&= \frac{P(X > k+j)}{P(X > k)} \\
        &= \frac{(1-p)^k(1-p)^j}{(1-p)^k} \\
        &= (1-p)^j \\
        &= P(X > j)
\end{align*}$$

> 1. Geometric Distribution \@ref(geometric-distribution-memoryless-lemma)
> 2. Since $j$ must be a positive integer in the Geometric Distribution,
		it is certain that \\$(k+j)\cap k=k+j$.

<!--chapter:end:Geometric_Distribution.Rmd-->

# Geometric Series

A Geometric Series is a series of the form $\sum\limits_{k=1}^{\infty}ar^{k-1}$ 
where $a\neq 0,\ r\neq 0,1$.  Expanding the series gives $\sum\limits_{k=1}^{\infty}ar^{k-1}=a+ar+ar^2+ar^3+\cdots$.


## Partial and Infinite Summations {#geometric-series-partial-and-infinite-summations}

Let $S_k$ denote the sum of a series over $k$ terms (or the $k^{th}$ partial sum).  For the Geometric Series\\

$$\begin{align*}
S_k
	&= \sum\limits_{k=1}^{k}ar^{k-1} \\
	&= a + ar + ar^2 + ar^3 + \cdots + ar^{k-1} \\
  &= ar^0 + ar^2 + ar^2 + \cdots + ar^{k-1} \\
	&= a + ar + ar^2 + \cdots + ar^{k-1}
\end{align*}$$

Notice that $rS_n = ar + ar^2+ a r^3 + \cdots + ar^k$.  So

$$\begin{align*}
S_k - rS_n
	&= (a + ar + \cdots + ar^{k-1}) - (ar + ar^2 + \cdots + ar^k) \\
  &= a + ar - ar + ar^2 - ar^2 + \cdots + ar^{k-2} - ar^{k-2} + ar^{k-1} - ar^{k-1} - ar^k \\
  &= a - ar^k
\end{align*}$$

Also 

$$\begin{align*} 
S_k - rS_k 
	              &= S_k(1-r) \\
	              &= a - ar^k \\
\Rightarrow S_k &= \frac{a-ar^k}{1-r}
\end{align*}$$



## Proofs of Convergence

$\sum\limits_{k=1}^{\infty}ar^{k-1}$ converges when $|r|<1$ and diverges when $|r|>1$.

_Proof:_

Recall that the $k^{th}$ partial sum of the Geometric Series is 

$$
S_k 
	= \frac{a-ar^k}{1-r}
$$

And let $S$ denote the sum of the infinite series, i.e. the sum as $k\rightarrow\infty$.

_Case 1:_ $|r|<1$

$$\begin{align*}
S
	&= \lim\limits_{k\rightarrow\infty}S_k \\
	&=  \lim\limits_{k\rightarrow\infty} \frac{a-ar^k}{1-r} \\
  &= \frac{a-\lim\limits_{k\rightarrow\infty}ar^k}{1-r} \\
	&= \frac{a}{1-r}
\end{align*}$$

So $\sum\limits_{k=1}^{\infty}ar^{k-1}$ converges when $|r|<1$ and $S=\frac{a}{1-r}$


_Case 2:_ \rm$|r|>1$
$$\begin{align*}
S
	&= \lim\limits_{k\rightarrow\infty}S_k \\
	&= \lim\limits_{k\rightarrow\infty} \frac{a-ar^k}{1-r} \\
  &= \frac{a-\lim\limits_{k\rightarrow\infty}ar^k}{1-r} \\
	&= \frac{a-\infty}{1-r}
\end{align*}$$

So $\sum\limits_{k=1}^{\infty}ar^{k-1}$ diverges whern $|r|>1$.



<!--chapter:end:Geometric_Series.Rmd-->

# Hypergeometric Distribution

## Probability Mass Function 

A random variable $X$ is said to follow a Hypergeometric Distribution if its probability mass function is

\[p(x) = \left\{
	\begin{array}{ll}
		\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}}, & x=0,1,2,\ldots\\
		0 & otherwise 
	\end{array} \right. 
\]

where 

* $N$ is the number of objects available to choose from
* $n$ is the number of objects chosen from $N$
* $r$ is the number of objects in $N$ that posses a desired characteristic (successes)
* $x$ is the number of objects in $n$ that possess the desired characterstic

## Cumulative Mass Function

\[P(x) = \left\{
	\begin{array}{ll}
		\sum\limits_{i=0}^{x}\frac{{r\choose i}{N-r\choose n-i}}{{N\choose n}}, & x=0,1,2,\ldots\\
		0 & otherwise 
	\end{array} \right. 
\]



## Expected Values

$$\begin{align*} 
E(X)
      	&= \sum\limits_{x=0}^{n}x\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}} \\
      	&= \sum\limits_{x=0}^{n}x{r\choose x}\frac{{N-r\choose n-x}}{{N\choose n}} \\
^{[1]}	&= \sum\limits_{x=0}^{n}x\frac{r}{x}{r-1\choose x-1}\frac{{N-r\choose n-x}}
      		{\frac{N}{n}{N-1\choose n-1}} \\
        &= \frac{rn}{N}\sum\limits_{x=0}^{n}
          \frac{{r-1\choose x-1}{N-r\choose n-x}}{{N-1\choose n-1}} \\
^{[2]}	&= \frac{rn}{N}\sum\limits_{y=0}^{n-1}
           \frac{{r-1\choose y}{N-r\choose n-y-1}}{{N-1\choose n-1}}
		       \frac{rn}{N}\sum\limits_{y=0}^{n-1}
		       \frac{{r-1\choose y}{N-r\choose n-y-1}}{{N-1\choose n-1}} \\
        &= \frac{\frac{rn}{N}\cdot n}{N}r\sum\limits_{y=0}^{n-1}
            \frac{{r-1\choose y}{N-r\choose n-y-1}}{{N-1\choose n-1}} \\
^{[3]}	&= \frac{rn}{N}\cdot 1 \\
        &= \frac{rn}{N}
\end{align*}$$

> 1. For any integer $a$ such that $0\leq a\leq k,\ 
		{n\choose k}=\frac{n(n-1)\cdots(n-a+1)}{k(k-1)\cdots(k-a+1)}{n-a\choose k-a}$ (Theorem \@ref(combinations-theorem-1)).
> 2. Let $y=x-1$\ \ $\Rightarrow x=y+1$.
> 3. $\frac{\sum\limits_{i=1}^{n}{N_1\choose i}{N_2\choose n-i}}{{N_1+N_2\choose n}}=1$\\
		\indent with $N_1=r,\ N_2=N-r,\ i=x$.  (Theorem \@ref(binomialtheorem-other-theorem-2)
	
	
$$\begin{align*} 
E[X(X-1)]
	      &= \sum\limits_{x=0}^{n}x(x-1)\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}} \\
^{[1]}	&= \sum\limits_{x=0}^{n}\frac{x(x-1)r(r-1)}{x(x-1)}\frac{{r-2\choose x-2}
	      	{N-r\choose n-x}}{\frac{N(N-1)}{n(n-1)}{N-2\choose n-2}} \\
        &= \frac{r(r-1)n(n-1)}{N(N-1)}\sum\limits_{x=0}^{n}
          \frac{{r-2\choose x-2}{N-r\choose n-x}}{{N-2\choose n-2}} \\
^{[2]}	&= \frac{r(r-1)n(n-1)}{N(N-1)}\sum\limits_{y=0}^{n-2}
          \frac{{r-2\choose y}{N-r\choose n-y-2}}{{N-2\choose n-2}} \\
^{[3]}  &= \frac{r(r-1)n(n-1)}{N(N-1)}\cdot 1 \\
        &=\frac{r(r-1)n(n-1)}{N(N-1)}
\end{align*}$$

> 1. For any integer $a$ such that $0\leq a\leq k,\ 
		{n\choose k}=\frac{n(n-1)\cdots(n-a+1)}{k(k-1)\cdots(k-a+1)}{n-a\choose k-a}$ (Theorem \@ref(combinations-theorem-1)).
> 2. Let $y=x-1$\ \ $\Rightarrow x=y+1$.
> 3. $\frac{\sum\limits_{i=1}^{n}{N_1\choose i}{N_2\choose n-i}}{{N_1+N_2\choose n}}=1$\\
		\indent with $N_1=r,\ N_2=N-r,\ i=x$.  (Theorem \@ref(binomialtheorem-other-theorem-2)
		
$$\begin{align*}
\mu
	&= E(X) \\
	&= \frac{rn}{N} \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= E(X^2) - E(X) + E(X) - E(X)^2 \\
  &= (E(X^2) - E(X) + E(X) - E(X)^2 \\
	&= E(X^2-X) + E(X) - E(X)^2 \\
  &= E[X(X-1)] + E(X) - E(X)^2\\
	&= \frac{r(r-1)n(n-1)}{N(N-1)} + \frac{rn}{N} - \frac{r^2n^2}{N^2} \\
  &= \frac{r(r-1)n(n-1)N}{N^2(N-1)} + \frac{rnN(N-1)}{N^2(N-1)}
		- \frac{r^2n^2(N-1)}{N^2(N-1)} \\
  &= \frac{(r^2-r)(n^2-n)N rn(N^2-N)-r^2n^2(N-1)}{N^2(N-1)} \\
  &= \frac{(r^2n^2N-r^2n^2N-rn^2N+rnN+rnN^2-rnN-r^2n^2N+r^2n^2}{N^2(N-1)} \\
  &= \frac{-r^2nN-rn^2N+rnN^2+r^2n^2}{N^2(N-1)} \\
	&= \frac{nr(-rN-nN+N^2+rn}{N^2(N-1)} \\
  &= \frac{nr(N^2-nN-rN+rn}{N^2(N-1)} \\
	&= \frac{nr(N-r)(N-n)}{N^2(N-1)} \\
	&= \frac{nr(N-r)(N-n)}{N\cdot N(N-1)} \\
  &= \frac{nr}{N}\cdot\frac{N-r}{N}\cdot\frac{N-n}{N-1}
\end{align*}$$


## Moment Generating Function

$$\begin{align*}
M_X(t)
	&= E(e^{tX}) \\
	&= \sum\limits_{x=0}^{n}e^{tx}\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}} \\
	&= \frac{1}{{N\choose n}}\sum\limits_{x=0}^{n}e^{tx}{r\choose x}{N-r\choose n-x} \\
  &= \frac{1}{{N\choose n}}[e^{0t}{r\choose 0}{N-r\choose n-0} + 
      e^{1t}{r\choose 1}{N-r\choose n-1} + 
      e^{2t}{r\choose 2}{N-r\choose n-2} + \cdots + 
      e^{nt}{r\choose n}{N-r\choose n-n}] \\
  &= \frac{1}{{N\choose n}}[{N-r\choose n-0}+e^{t}{r\choose 1}{N-r\choose n-1}
		+ e^{2t}{r\choose 2}{N-r\choose n-2}+\cdots+e^{nt}{r\choose n}{N-r\choose n-n}]
\end{align*}$$

This mgf does not reduce to any form which can be differentiated, and we cannot use it to generate moments for the distribution.


## Theorems for the Hypergeometric Distribution

### Validity of the Distribution

$$ 
\sum\limits_{x=0}^{n}\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}} = 1
$$

_Proof:_

Theorem \@ref(binomialtheorem-other-theorem-1) states

$$ 
{N_1\choose 0}{N_2\choose n}+{N_1\choose 2}{N_2\choose n-1}+\cdots
		+{N_1\choose n-1}{N_2\choose 1}+{N_1\choose n}{N_2\choose 0} \\
  = \sum\limits_{x=0}^{n}{N_1\choose x}{N_2\choose n-x} \\
	= {N_1+N_2\choose n}
$$

Using $N_1 = r$ and $N_2 = N-r$ we have
$$\begin{align*}
\sum\limits_{x=0}^{n}\frac{{r\choose x}{N-r\choose n-x}}{{N\choose n}}
	&= \frac{{r+N-r\choose n}}{{N\choose n}} \\
	&= \frac{{N\choose n}}{{N\choose n}} \\
	&= 1
\end{align*}$$

<!--chapter:end:HyperGeometric_Distribution.Rmd-->

# Integration: Techniques and Theorems

## Elementary Theorems

### Integration of Even Functions about Zero {#integration-even-function-about-zero}

Suppose $f$ is an integratable function, and let $F(x)=\int\limits_{0}^{x_0}f(x)dx$.

Then $\int\limits_{-x_0}^{0}f(x)dx=\int\limits_{0}^{x_0}f(x)dx$ 
if and only if $f$ is an even function.\\

_Proof:_

First, let $f$ be an even function.  Then, by Theorem \@ref(derivatives-and-anti-derivatives-of-even-functions), the anti-derivative $F$ is an odd function.

$$\begin{align*}
\int\limits_{-x_0}^{0}f(x)dx 
	      &= F(0) - F(-x_0) \\
	      &= F(0) + F(x_0) \\
^{[1]}	&= F(x_0)  \\
\\
\\
\int\limits_{0}^{x_0}f(x)dx 
	      &= F(x_0) - F(0) \\
	      &= F(x_0)
\end{align*}$$

> 1. $F(0)=\int\limits_{0}^{0}f(x)dx=0$.

So 
$$\begin{align*} 
\int\limits_{-x_0}^{0}f(x)dx
	&= F(x_0) \\
	&= \int\limits_{0}^{x_0}f(x)dx
\end{align*}$$

Now suppose 

$$
\int\limits_{-x_0}^{0}f(x)dx
	= \int\limits_{0}^{x_0}f(x)dx
$$ 

Then

$$\begin{align*}
\int\limits_{-x_0}^{0}f(x)dx
	&= F(0) - F(-x_0) \\
	&= -F(-x_0)
\end{align*}$$

and

$$\begin{align*} 
\int\limits_{0}^{x_0}f(x)dx
	= F(x_0) - F(0) \\
	= F(x_0)$
\end{align*}$$

So 
$$\begin{align*}
-F(-x_0)          	&= F(x_0) \\
\Rightarrow F(-x_0) &= -F(x_0)
\end{align*}$$

This satisfies the definition of an odd function.  So by Theorem \@ref(derivatives-and-anti-derivatives-of-odd-functions), $f$ must be an even function.


### Corollary

If $f$ is a continuous and even function and $t\in\Re$, then 

$$ \int\limits_{-t}^{t}f(x)dx = 2\int\limits_{0}^{t}f(x)dx $$

Furthermore, 

$$\int\limits_{-\infty}^{\infty}f(x)dx
	= 2\int\limits_{0}^{\infty}f(x)dx$$.

_Proof:_
Since $f(x)$ is even and by Theorem \@ref(integration-even-function-about-zero)

$$\begin{align*}
\int\limits_{-t}^{0}f(-x)dx
	&= \int\limits_{-t}^{0}f(x)dx \\
	&= \int\limits_{0}^{t}f(x)dx
\end{align*}$$

It follows that

$$\begin{align*}
\int\limits_{-t}^{t}f(x)dx
	&= \int\limits_{-t}^{0}f(-x)dx + \int\limits_{0}^{t}f(x)dx \\
	&= \int\limits_{0}^{t}f(x)dx + \int\limits_{0}^{t}f(x)dx \\
	&= 2\int\limits_{0}^{t}f(x)dx
\end{align*}$$

The second statement is proven by taking the limits as $t\rightarrow\infty$. 

### Integrals of Horizontal Translations

Let $x$ be any real number and $a,b,$ and $c$ be constants.  Also, let $f(x)$ be continuous on the interval $(a,b)$.  Then 

$$\int\limits_{a}^{b}f(x)dx = \int\limits_{a+c}^{b+c} f(x+c)dx$$

_Proof:_

The proof of this theorem is completed by applying a change of variable to 

$$\int\limits_{a}^{b}f(x)dx$$

We let 

$$\begin{align*}
            y &= x+c \\
\Rightarrow x &= y-c
\end{align*}$$

So $dx=dy$.

$$\begin{align*}
x &= a   & \Rightarrow \ \ \ \   y &= a+c\\
x &= b   & \Rightarrow \ \ \ \   y &= b+c
\end{align*}$$. 

Thus

$$\begin{align*}
\int\limits_{a}^{b}f(x)dx
	&= \int\limits_{a+c}^{b+c}f(y)dy \\
	&= \int\limits_{a+c}^{b+c}f(x+c)dx
\end{align*}$$


<!--chapter:end:Integration.Rmd-->

# Logistic Regression

## The Logit Transformation
Logistic regression intends to model the probability that a given response will occur based on the characteristics in the predictor variables.  The response, therefore, is a probability; a value between zero and one.  This is in contrast to typical linear regression where the response lies range of real numbers.  This is further complicated by the fact that in the observed data, a subject does not have a probabilistic response, but the response is the dichotomous occurrence of an event.  The nature of the response variable in logistic regression, therefore, necessitates that a transformation be applied.

### Obtaining the Logit Transformation
Let us call the response for our logistic model the probability $p$ that an event will occur.  Since $p$ is a probability, by definition, it's domain is from 0 to 1.  Ideally, we would like to have a response whose domain is $\Re$.  First, let us consider the transformation $\frac{p}{1-p}$ (also called the odds of $p$) and it's limits as $p$ approaches 0 and 1.

$$\begin{align*}
\lim\limits_{p\rightarrow 0} \frac{p}{1-p} 
    = \frac{0}{1-0} 
    = 0
\end{align*}$$

and

$$\begin{align*}
\lim\limits_{p\rightarrow 1} \frac{p}{1-p} 
    = \infty
\end{align*}$$
  
So the domain of $\frac{p}{1-p}$ is $(0,\infty)$.  This is handy, as we do know that the $\ln$ function takes a variable on the domain $(0,\infty)$ and maps it onto the range $(-\infty,\infty)$.  Thus, the equation for our logistic model the transformation (called the logit, or log-odds):

$$\ln\bigg(\frac{p}{1-p}\bigg) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n$$


## Retrieving the Modelled Probability
While the logit transformation allows us to perform the logistic regression, the resulting measure tells us about the risk of an event associated with a predictor, but does not tell us directly about the probability of the event occuring.  If we need to know the probability of the event ocurring, we must back transform the results of our regression equation.  Essentially, we extract $\hat p$ from the modelled log-odds.  This is done as by:

$$\begin{align*}
\ln\bigg(\frac{\hat{p}}{1-\hat{p}}\bigg)  
		    &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n \\
^{[1]}  &= B \\
\Rightarrow \frac{\hat{p}}{1-\hat{p}} &= \exp(B) \\
\Rightarrow \hat{p} &= (1-\hat{p})\exp(B) \\
\Rightarrow \hat{p} &= \exp(B) - \hat{p} \cdot \exp(B) \\
\Rightarrow \hat{p} + \hat{p} \cdot \exp(B) &= \exp(B) \\
\Rightarrow \hat{p}(1+\exp(B)) &= \exp(B) \\
\Rightarrow \hat{p} &= \frac{\exp(B)}{1 + \exp(B)} \\
\Rightarrow \hat{p} &= \frac{\exp(\beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n)}
                     {1 + \exp(\beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n)}
\end{align*}$$

> 1. Let $B = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_nx_n$

<!--chapter:end:LogisticRegression.Rmd-->

# Mantel-Haenszel Test of Linear Trend

The Mantel-Haenszel Test is a method for testing independence of categorical variables on an ordinal scale.  See @Agres1996a for more discussion.

Let $X$ be a categorical variable of ordinal type with $R$ levels.

Let $Y$ be a categorical variable of ordinal type with $C$ levels.

Suppose we take a sample of size $n$ and take a measurement on each item in the sample with respect to $X$ and $Y$.  The presence of a progresive between $X$ and $Y$ can be tested using the correlation coefficient $\rho$ [@Mantel1963a].  We may begin by taking the estimate of $\rho$

$$\begin{align*} 
r
	&= \frac{\widehat{Cov}(X,Y)}{\sqrt{s_X^2 s_Y^2}} \\
	&= \frac{\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}(x_i-\bar x)(y_j-\bar y)p(x_i,y_j)}
		{\sqrt{\sum\limits_{i=1}^{n}(x_i-\bar x)^2p(x_i)
		 \sum\limits_{j=1}^{n}(y_j-\bar y)^2p(y_j)}}
\end{align*}$$

But since $X$ and $Y$ are categorical, we cannot sensibly perform any of the operations.  Instead, we define the variables $U$ and $V$ to be the ordinal scoring of $X$ and $Y$ respecitively.  In other words, $U_i$ is the score for the category of $X_i$ and $V_i$ is the score for the category of $Y_i$.  Using this replacement we get

$$ 
r
	= \frac{\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}(u_i-\bar u)(v_j-\bar v)p(u_i,v_j)}
		{\sqrt{\sum\limits_{i=1}^{n}(u_i-\bar u)^2p(u_i)
		 \sum\limits_{j=1}^{n}(v_j-\bar v)^2p(v_j)}}
$$

To obtain the values of $\bar u$ and $\bar v$, we consider the following table.  Recall that there are $R$ levels of the variable $X$ and $C$ levels of the variable $Y$.

*u~1~*

|                 |          |         |     |         |         |        |
|-----------------|----------|---------|-----|---------|---------|--------|
|                 | Category |      of | _V_ |         |         |        |
| Category of _U_ |        1 |       2 | ... | _C_     | Total   | _U_    |
|               1 |  _n~11~_ | _n~12~_ | ... | _n~1c~_ | _n~1+~_ | _U~1~_ |
|               2 |  _n~21~_ | _n~22~_ | ... | _n~2c~_ | _n~2+~_ | _U~2~_ |
|             ... |      ... |     ... | ... |     ... |     ... |    ... |
|             _R_ |  _n~r1~_ | _n~r2~_ | ... | _n~rc~_ | _n~r+~_ | _U~r~_ |
|           Total |  _n~+1~_ | _n~+2~_ | ... | _n~+c~_ | _n~++~_ |        |
|             _V_ |   _V~1~_ |  _V~2~_ | ... |  _V~c~_ |         |        | 


In the table, $n_{rc},\ r=1,2,\ldots,R,\ c=1,2,\ldots,C$ is the number of observations in the sample with scores $r$ and $c$.  From the table we can understand the marginal distributions of $U$ and $V$, and we see that for $r=1,2,\ldots,R,\ c=1,2,\ldots,C$

$$\begin{align*}
p(u_r) &= \frac{n_{r+}}{n} \\
\\
p(v_c) &= \frac{n_{+ c}}{n} \\
\\
p(u_r,v_c) &= \frac{n_{rc}}{n} \\
\\
\bar u &= \sum\limits_{r=1}^{R}u_i\frac{n_{r+}}{n} \\
\\
\bar v &= \sum\limits_{c=1}^{C}v_i\frac{n_{+ c}}{n}
\end{align*}$$

With these observations, we can derive the value of $r$ as

$$\begin{align*} 
r
	&= \frac{\widehat{Cov}(U,V)}{\sqrt{s_U^2s_V^2}} \\
	&= \frac{\frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}
			(u_r-\bar u)(v_c-\bar v)n_{rc}}{n-1}}
		{\sqrt{\frac{\sum\limits_{r=1}^{R}(u_r-\bar u)^2}{n-1}
		 \frac{\sum\limits_{c=1}^{C}(v_c-\bar v)^2}{n-1}}} \\
	&= \frac{\frac{1}{n-1}\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}
			(u_r-\bar u)(v_c-\bar v)n_{rc}}
		{\frac{1}{n-1}\sqrt{\sum\limits_{r=1}^{R}(u_r-\bar u)^2
		 \sum\limits_{c=1}^{C}(v_c-\bar v)^2}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}(u_r-\bar u)(v_c-\bar v)n_{rc}}
		{\sqrt{\sum\limits_{r=1}^{R}(u_r-\bar u)^2
		 \sum\limits_{c=1}^{C}(v_c-\bar v)^2}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}
			(u_rv_c-u_r\bar v-\bar uv_c+\bar u\bar v)n_{rc}}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}
			(u_rv_cn_{rc}-u_r\bar vn_{rc}-\bar uv_cn_{rc}+\bar u\bar vn_{rc})}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_r\bar vn_{rc}
			- \sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}\bar uv_cn_{rc}
			+ \sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}\bar u\bar vn_{rc}}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \bar v\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rn_{rc}
			- \bar u\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}v_cn_{rc}
			+ \bar u\bar v\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}n_{rc}}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \bar v\sum\limits_{r=1}^{R}u_rn_{r+}
			- \bar u\sum\limits_{c=1}^{C}v_cn_{+ c}
			+ \bar u\bar vn}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \frac{\sum\limits_{c=1}^{C}v_cn_{+ c}\sum\limits_{r=1}^{R}
				u_rn_{r+}}{n}
			- \frac{\sum\limits_{r=1}^{R}u_rn_{r+}\sum\limits_{c=1}^{C}
				v_cn_{+ c}}{n}
			+ n\frac{\sum\limits_{r=1}^{R}u_rn_{r+}\sum\limits_{c=1}^{C}
				v_cn_{+ c}}{n^2}}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \frac{2\sum\limits_{r=1}^{R}u_rn_{r+}\sum\limits_{c=1}^{C}
				v_cn_{+ c}}{n}
			+ \frac{\sum\limits_{r=1}^{R}u_rn_{r+}\sum\limits_{c=1}^{C}
				v_cn_{+ c}}{n}}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}} \\
  &= \frac{\sum\limits_{r=1}^{R}\sum\limits_{c=1}^{C}u_rv_cn_{rc}
			- \frac{1}{n}(\sum\limits_{r=1}^{R}u_rn_{r+})
			(\sum\limits_{c=1}^{C}v_cn_{+ c})}
		{\sqrt{\bigg(\sum\limits_{r=1}^{R}u_r^2n_{r+}
			- \frac{1}{n}\Big(\sum\limits_{r=1}^{R}u_rn_{r+}\Big)^2\bigg)
		 \bigg(\sum\limits_{c=1}^{C}v_c^2n_{+ c}
			- \frac{1}{n}\Big(\sum\limits_{c=1}^{C}v_cn_{+ c}\Big)^2\bigg)}}
\end{align*}$$

<!--chapter:end:MantelHaenszel.Rmd-->

# McNemar Test
This chapter only represents work that needed to be done for a specific application.  Some of the formulas and equations provided are not necessarily coherent with the articles originally published on the topic.  The majority of the work was derived from Connor's 1987 paper.  This chapter could benefit from a great deal of improvement and additional explanation.

The McNemar Test compares proportions of related samples in which the outcome for each sample is a binary response.  Th response is the same in each sample.  Related samples may mean subjects from one sample are matched with subjects with similiar qualities (subjects are related, but outcomes are not); or it may mean that subjects are paired with themselves, as in a pre-post design (outcomes are related because they are taken on the same subject).\\

|         |        |       |         |            |            | 
|---------|--------|-------|---------|------------|------------|
|         |        | Trial |     2   |            |            |
|         |        |       |     1   |     0      |            |
| Trial 1 |        | 1     | _p~11~_ | _p~10~_    | _p~1~_     |
|         |        | 0     | _p~01~_ | _p~00~_    | 1 - _p~1~_ |
|         |        |       | _p~2~_  | 1 - _p~2~_ |            |      



The table demonstrates the possible outcomes of such a experiment.  Suppose $Y_i$ deontes the outcome of Trial 1 and $Y_j$ denotes the outcome of Trial 2.  $Y_{ij}$ denotes the outcome of the first and second trials, that is $Y_{ij} = Y_i \cap Y_j$.  Then:

$$\begin{align*}
 p_{11} &= P(Y_i = 1 \cap Y_j = 1) \\
 p_{10} &= P(Y_i = 1 \cap Y_j = 0) \\
 p_{01} &= P(Y_i = 0 \cap Y_j = 1) \\
 p_{00} &= P(Y_i = 0 \cap Y_j = 0) \\
\end{align*}$$

Furthermore

$$\begin{align*}
 p_1 &= p_{11} + p_{10} &= P(Y_i = 1) \\
 p_2 &= p_{11} + p_{01} &= P(Y_j = 1) 
\end{align*}$$

In upcoming sections, the values of the difference and sum of $p_1$ and $p_2$ will be important, so we define

$$\begin{align*}
\delta &= p_1 - p_2 
       &= (p_{11} + p_{10}) - (p_{11} + p_{01}) 
       &= p_{10} - p_{01} \\
\\
\\
\psi &= p_1 + p_2 
     &= (p_{11} + p_{10}) + (p_{11} + p_{01}) 
     &= 2p_{11} + p_{10} + p_{01}
\end{align*}$$

## Sample Size Calculations for Paired Design
Three methods of calculating power for McNemar's Test have been presented.  Miettenen proposed a method of estimating the power in 1968.  Duffy provided the exact power in 1984.  Connor provided an additional method of estimating the power in 1987.

### Miettenen's Sample Size Calculation
Miettenen was the first to provide a popular power calculation for McNemar's test with a paired-design.  Duffy would later show that this calculation tends to under-estimate the power.  Subsequently, sample sizes derived from this calculation are generally lower than is needed to obtain the designed power.

Let $\alpha$ be the probability of Type I Error, and let $\beta$ be the probability of Type II Error.  Furthermore, let $Z_\alpha = \Phi(1-\alpha)$ and $Z_\beta = \Phi(1-\beta)$.  Now suppose we wish to determine the sample size $n_m$ (for Miettenen method) required to find a change in proportion from $p_1$ to $p_2$ with significance $\alpha$ and power $1-\beta$.  The required sample size is calculated by:

$$ 
n_m = 
		\frac{\Big( Z_\alpha \psi^{1/2} + Z_\beta \big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} \Big)^2}{\delta^2}
$$


### Connor's Sample Size Calculation
Connor proposed a method for sample size calculation in addition to Miettenen's.  Connor's method tends to over-estimate the power.  Subsequently, sample sizes derived from this calculation are generally higher than is actually needed to obtain the designed power.

Let $\alpha$ be the probability of Type I Error, and let $\beta$ be the probability of Type II Error.  Furthermore, let $Z_\alpha = \Phi(1-\alpha)$ and $Z_\beta = \Phi(1-\beta)$.  Now suppose we wish to determine the sample size $n_c$ (for Miettenen method) required to find a change in proportion from $p_1$ to $p_2$ with significance $\alpha$ and power $1-\beta$.  The required sample size is calculated by:

$$
n_c =
		\frac{\big( Z_\alpha \psi^{1/2} + Z_\beta (\psi - \delta^2)^{1/2} \big)^2}{\delta^2}
$$
		
## Power Calculation for Paired Design
The following power calculations are derived in a backward fashion.  In the application I had at the time, I needed to calculate sample sizes, and also wanted to allow functionality in my R function to obtain power with a supplied sample size.  Since I had the sample size equations, I solved for the power.  Normally this would be done the other way around, ie, take the power function and solve for $n$.  In the future, this should be revised appropriately.

### Power Calculation for Miettenen Method
Let $\alpha$ be the probability of Type I Error, and let $\beta$ be the probability of Type II Error.  Furthermore, let $Z_\alpha = \Phi(1-\alpha)$ and $Z_\beta = \Phi(1-\beta)$.  The power function can be found from the sample size equation by:

$$\begin{align*} 
n_m 
	&= \frac{\Big( Z_\alpha \psi^{1/2} + Z_\beta \big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} \Big)^2}
						{\delta^2} \\
\Rightarrow n_m\delta^2 
				&= \Big( Z_\alpha \psi^{1/2} + Z_\beta \big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} \Big)^2 \\
\Rightarrow \sqrt{n_m}\delta  
				&= Z_\alpha \psi^{1/2} + Z_\beta \big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} \\
\Rightarrow \sqrt{n_m}\delta - Z_\alpha \psi^{1/2} 
				&= Z_\beta \big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} \\
\Rightarrow \frac{\sqrt{n_m}\delta - Z_\alpha \psi^{1/2}}{\big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2}} 
				&= Z_\beta \\
\Rightarrow \Phi^{-1}\bigg(\frac{\sqrt{n_m}\delta - Z_\alpha \psi^{1/2}}
                                {\big(\psi - \delta^2 (3+\psi) / (4\psi) \big)^{1/2} }\bigg) 
				&= 1 - \beta
\end{align*}$$
				
### Power Calculation for Connor Method

Let $\alpha$ be the probability of Type I Error, and let $\beta$ be the probability of Type II Error.  Furthermore, let $Z_\alpha = \Phi(1-\alpha)$ and $Z_\beta = \Phi(1-\beta)$.  The power function can be found from the sample size equation by:

$$\begin{align*}
n_c 
				&= \frac{\big( Z_\alpha \psi^{1/2} + Z_\beta (\psi - \delta^2)^{1/2} \big)^2}{\delta^2} \\
\Rightarrow n_c\delta^2 
				&= \big( Z_\alpha \psi^{1/2} + Z_\beta (\psi - \delta^2)^{1/2} \big)^2 \\
\Rightarrow \sqrt{n_c\delta}
    		&= Z_\alpha \psi^{1/2} + Z_\beta (\psi - \delta^2)^{1/2} \\
\Rightarrow \sqrt{n_c\delta} - Z_\alpha \psi^{1/2}
				&= Z_\beta (\psi - \delta^2)^{1/2} \\
\Rightarrow \frac{\sqrt{n_c\delta} - Z_\alpha \psi^{1/2}}{(\psi - \delta^2)^{1/2}}
				&= Z_\beta \\
\Rightarrow \Phi^{-1}\bigg(\frac{\sqrt{n_c\delta} - Z_\alpha \psi^{1/2}}{(\psi - \delta^2)^{1/2}}\bigg)
				&= 1 - \beta
\end{align*}$$
				

<!--chapter:end:McNemar.Rmd-->

# Moments and Moment Generating Functions

## Definitions of Moments

### Definition: General Definition of Moments

The $k^{th}$ moment of a random variable $X$ about some point $c$ is defined to be $E[(X-c)^k]$.

There are two moments that are of particular use in statistics.  First, the moment of $X$ about the origin; second, the moment of $X$ about the mean.

### Definition: Ordinary Moments

The $k^{th}$ moment of a random variable $X$ about the origin is defined to be $E[(X-0)^k] = E(X^k)$.


### Definition: Central Moments

The $k^{th}$ moment of a random variable $X$ about the mean $\mu$ is defined to be $E[(X-\mu)^k]$.

Using these definitions we can derive the first three central moments as follows:

$$\begin{align*}
E[(X-\mu)^1]
	&= E(X - \mu) \\
	&= E(X) - \mu \\
	&= E(X) - E(X)
\\
\\
E[(X-\mu)^2]
	&= E[(X-\mu)(X-\mu)] \\
	&= E(X^2-\mu X-\mu X+\mu^2) \\
  &= E(X^2-2\mu X+\mu^2) \\
	&= E(X^2) - E(2\mu X) + E(\mu^2) \\
	&= E(X^2) - 2\mu E(X) + \mu^2 \\
  &= E(X^2) - 2\mu\cdot\mu + \mu^2 \\
	&= E(X^2) - 2\mu^2 + \mu^2 \\
  &= E(X^2) - \mu^2 \\
	&= E(X^2) - E(X)^2 \\
\\
\\
E[(X-\mu)^3]
	&= E[(X-\mu)(X-\mu)(X-\mu)] \\
	&= E[(X^2-2\mu X+\mu^2)(X-\mu)] \\
  &= E(X^3-\mu X^2-2\mu X^2+2\mu^2X+\mu^2X+\mu^3) \\
  &= E(X^3-3\mu X^2+3\mu^2X-\mu^3) \\
	&= E(X^3) - E(3\mu X^2) + E(3\mu^2X) - E(\mu^3) \\
  &= E(X^3) - 3\mu E(X^2) + 3\mu^2E(X) - \mu^3 \\
	&= E(X^3) - 3\mu E(X^2) + 3\mu^3 - \mu^3 \\
  &= E(X^3) - 3\mu E(X^2) + 2\mu^3
\end{align*}$$

It should be noticed that with all of these results, the moment about the mean can be evaluated by finding the ordinary moments.  Thus, if we can find a consistent way to generate ordinary moments , we may use these results to find various parameters of a distribution.

## Moment Generating Functions

### Definition: Moment Generating Function

The moment generating function of a random variable, denoted $M_X(t)$, is defined to be:

\[ M_X(t) = E(e^{tX}) \]

The moment generating function of $X$ is said to exist if for any positive constant $c,\ M_X(t)$ is finite for $|t|<c$.  The definition can be expanded to

$$\begin{align*}
M_X(t)
	&= E(e^{tX} \\
	&= \sum\limits_{i=1}^{\infty}e^{tx_i}p(x_i)  \\
^{[1]}  &= \sum\limits_{i=1}^{\infty}[\frac{(tx_i)^0}{0!}+\frac{(tx_i)^1}{1!}
		+ \frac{(tx_i)^2}{2!}+\frac{(tx_i)^3}{3!}+\cdots]p(x_i) \\
  &= \sum\limits_{i=1}^{\infty}[1+tx_i+\frac{(tx_i)^2}{2!}
		+ \frac{(tx_i)^3}{3!}+\cdots]p(x_i) \\
  &= \sum\limits_{i=1}^{\infty}[p(x_i)+tx_ip(x_i)
		+ \frac{(tx_i)^2}{2!}p(x_i)+\frac{(tx_i)^3}{3!}p(x_i) + \cdots] \\
  &= \sum\limits_{i=1}^{\infty}p(x_i)+\sum\limits_{i=1}^{\infty}tx_ip(x_i)
		+ \sum\limits_{i=1}^{\infty}\frac{(tx_i)^2}{2!}p(x_i)
		+ \sum\limits_{i=1}^{\infty}\frac{(tx_i)^3}{3!}p(x_i) + \cdots \\
  &= \sum\limits_{i=1}^{\infty}p(x_i)+t\sum\limits_{i=1}^{\infty}x_ip(x_i)
		+ \frac{t^2}{2!}\sum\limits_{i=1}^{\infty}x_i^2p(x_i)
		+ \frac{t^3}{3!}\sum\limits_{i=1}^{\infty}x_i^3p(x_i)+\cdots \\
  &= 1 + tE(X) \\
	&= \frac{t^2}{2!}E(X^2) + \frac{t^3}{3!} + \cdots
\end{align*}$$
	
> 1. Taylor Series Expansion: 
		$e^x=\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}\cdots
		= 1+x+\frac{x^2}{2!}+\cdots$


### Theorem: Extraction of Moments from Moment Generating Functions

Let $M_X^{(k)}(t)$ denote the $k^{th}$ derivative of $M_X(t)$ with respect to $t$. Then $M_X^{(k)}(0)=E(X^k)$.

_Proof:_

$$\begin{align*} 
M_X(t)
	&= 1 + tE(X) \\
	&= \frac{t^2}{2!}E(X^2) + \frac{t^3}{3!} + \cdots \\
\\
\\
M_X^{(1)}(t)
	&= 0 + E(X) + \frac{2t}{2!}E(X^2) + \frac{3t^2}{3!}E(X^3) + \cdots \\
  &= E(X) + tE(X^2) + \frac{t^2}{2!}E(X^3) + \cdots \\
\\
\\
M_X^{(2)}(t)
	&= 0 + E(X^2) + \frac{2t}{2!}E(X^3) + \frac{3t^2}{3!}E(X^4) + \cdots \\
  &= E(X^2) + tE(X^3) + \frac{t^2}{2!}E(X^4) + \cdots \\
\vdots \\
\\
\\
M_X^{(k)}(t)
	&= 0 + E(X^k) + \frac{2t}{2!}E(X^{k+1}) + \frac{3t^2}{3!}E(X^{k+2}) + \cdots \\
  &= E(X^k) + tE(X^{k+1}) + \frac{t^2}{2!}E(X^{k+2}) + \cdots \\
\\
\\
M_X^{(1)}(0)
	&= 0 + E(X) + \frac{2\cdot 0}{2!}E(X^2) + \frac{3\cdot 0t^2}{3!}E(X^3) + \cdots \\
	&= E(X)\\
\\
\\
M_X^{(2)}(0)
	&= 0 + E(X^2) + \frac{2\cdot 0}{2!}E(X^3) + \frac{3\cdot 0^2}{3!}E(X^4) + \cdots \\
	&= E(X^2) \\
\\
\\
\vdots \\
\\
\\
M_X^{(0)}(t)
	&= 0 + E(X^k) + \frac{2\cdot 0}{2!}E(X^{k+1}) + \frac{3\cdot 0^2}{3!}E(X^{k+2}) + \cdots \\
	&= E(X^k)
\end{align*}$$

<!--chapter:end:Moments.Rmd-->

# Multinomial Distribution

Let $E_1,E_2,\ldots,E_k$ be mutually exclusive and exhaustive events and define a multinomial experiment to have the following characteristics:

i. The experiment consists of $N$ indepedendent trials.
ii. The outcome of each trial belongs to exactly one $E_j,\ j=1,2,\ldots,k$.
iii. The probability that an outcome belongs to event $E_j$ is $p_j$.

Let $X_{ij}=\left\{
	\begin{array}{ll}
		1& \rm if\ the\ outcome\ of\ the\ \it i^{th}\ \rm trial\ belongs\ to\ \it E_j.\\
		0& otherwise 
	\end{array} \right.$
	
and let $n_j=\sum\limits_{i=1}^{N}X_{ij}$.  Under these conditions, $N=\sum\limits_{j=1}^{k}n_j$.

By Lemma \@ref(combinations-lemma) the number of ways to partition $N$ into the $k$ events, without respect to order, is $\frac{N!}{n_1!n_2!\cdots n_k!}$.  So the probabilitiy of any particular outcome of the experiment is

\[p(n_1,n_2,\ldots,n_{k-1})
	= \frac{N!}{n_1!n_2!\cdots n_{k-1}!n^\prime!}
		p_1^{n_1}p_2^{n_2}\cdots p_{(k-1)}p^{\prime n^\prime}
\] 

where $n^\prime = N - n_1 - n_2 - \cdots - n_{k-1}$ and 

$p^\prime = 1 - p_1 - p_2 - \cdots - p_{k-1}$.  

In other words, the entire distribution is defined by the first $k-1$ terms.

## Cumulative Distribution Function

\[P(n_1,n_2,\ldots,n_{k-1}) 
	= \sum\limits_{n_1=0}^{N} \sum\limits_{n_2=0}^{N-n_1} \cdots \sum\limits_{n_{k-1}=0}^{N^\prime}
		\frac{N!}{n_1!n_2!\cdots n_{k-1}!n^\prime!} p_1^{n_1}p_2^{n_2}\cdots p_{k-1}^{n_{k-1}}p^{\prime n^\prime}\]

where $N^\prime=N-n_1-n_2-\cdots n_{k-1}$.

## Expected Values

Since this is a multivariate distribution, we discuss finding the expected values for each variate $n_j$ as opposed to an overall mean.\\

$n_j$ is a random variable from a multinomial distribution that specifies how many of the $N$ observations were of type $j$.  Each of the $N$ observations willf all into exactly one type, so we can conclude that an observation is either of type $j$ or it isn't.  Also, it is of type $j$ with probability $p_j$, and each trial is independent.  Thus, we may consider $n_j$ a binomial random variable and $E(n_j)=Np_j$ and $V(n_j)=Np_j(1-p_j)$.  Now we must derive the Covariance of $n_j$.

We begin by defnining the random variables for $j\neq m$:

\[X_i=\left\{
	\begin{array}{ll}
		1 & \rm if\ trial \it\ i\ \rm results\ in\ type\ \it j.\\
		0 & otherwise 
	\end{array} \right.
\]

\[Y_i=\left\{
	\begin{array}{ll}
		1 & \rm if\ trial \it\ i\ \rm results\ in\ type\ \it m.\\
		0 & otherwise 
	\end{array} \right.
\]

and let $n_j=\sum\limits_{i=1}^{n}X_i$ and $n_m=\sum\limits_{i=1}^{n}Y_i$.  Since $X_i$ and $Y_i$ cannot simultaneously equal 1, $X_i\cdot Y_i=0$ for all $i$.  We thus have the following results so far:

$$E(X_i\cdot Y_i) = 0$$

$$E(X_i) = p_j$$

$$E(Y_i) = p_m$$

$Cov(X_i,Y_i) = 0$ if $i\neq j$ because the trials are independent

$Cov(X_i,Y_i) = E(X_i\cdot Y_i) - E(X_i)E(Y_i) = 0 - p_j p_m = -p_j p_m$. ($Cov(X,Y) = E(XY) - E(X)E(Y)$ (Theorem \@ref(covariance-theorem2))

Using these results we find the Covariance of $n_j$ and $n_m$.

$$\begin{align*}
Cov(n_j,n_m) 
  &= \sum\limits_{j=1}^{N}\sum\limits_{m=1}^{N}Cov(X_i,Y_i) \\
  &= \sum\limits_{i=1}^{N}Cov(X_iY_i) + \sum\sum\limits_{i\neq j}Cov(X_i,Y_i) \\
  &= \sum\limits_{i=1}^{n}-p_jp_m + \sum\sum\limits_{i\neq j}0=-np_jp_m
\end{align*}$$

The Expected Values of the $p_j$'s can be found by

$$\begin{align*} 
E(\hat p_j)
	&= E(\frac{n_j}{N}) \\
	&= \frac{1}{N}E(n_j) \\
	&= \frac{1}{N}Np_j \\
	&= p_j \\
\\
\\
V(\hat p_j)
	&= V(\frac{n_j}{N}) \\
	&= \frac{1}{N^2}V(n_j) \\
	&= \frac{1}{N^2}Np_j(1-p_j) \\
	&= \frac{p_j(1-p_j)}{N} \\
\\
\\
Cov(\hat p_j,\hat p_m)
	&= Cov(\frac{n_j}{N},\frac{n_m}{N}) \\
	&= \frac{1}{N^2}Cov(n_j,n_m) \\
	&= \frac{1}{N^2}(-Np_jp_m) \\
	&= \frac{-p_jp_m}{N}
\end{align*}$$

<!--chapter:end:Multinomial.Rmd-->

# Normal Distribution

## Probability Distribution Function

A random variable $X$ is said to have a Normal Distribution with parameters $\mu$ and $\sigma^2$ if its probability density function is

\[f(x)
	= \frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2},\ \  x\in\Re,\ \mu\in\Re,\ 0<\sigma^2 \]


## Cumulative Distribution Function

The cdf of the Normal Distribution cannot be written in closed form.  

It is not uncommon in statistical practice to denote the cdf of the Standard Normal Distribution, that is, the Normal distribution with $\mu=0$ and $\sigma^2=1$ to be denoted by $\Phi(\cdot)$.  Although it is rare, the $\Phi(\cdot)$ notation is sometimes used to denote any cumulative density function.  Because such citations are rare they are usually accompanied by a statement about the distribution $\Phi(\cdot)$ represents.  If no such statement is given, it is reasonably safe to assume that the function refers to the Standard Normal Distribution.

The importance of the Standard Normal Distribution is made evident by the fact that the area under any Normal curve is proportional to the distribution's variance.  A proof of this is provided in Theorem \ref{Normal4.6}

```{r, echo = FALSE, warning = FALSE, fig.path = 'figures/', fig.cap = 'The graphs on the left show probability distribution functions, and the graphs on the right show cumulative distribution functions.  The effect of changing the variance, a vertical rescaling of the distribution, is evident'}
Normal <- 
  expand.grid(x = seq(-6, 6, by = 0.01),
              mu = c(0),
              sigma = 1:3) %>%
  mutate(dnorm = dnorm(x, mu, sigma),
         pnorm = pnorm(x, mu, sigma)) %>%
  gather(cumulative, prob, -x, -mu, -sigma) %>%
  mutate(cumulative = factor(cumulative,
                             c("dnorm", "pnorm"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Normal,
       mapping = aes(x = x,
                     y = prob,
                     colour = factor(sigma))) + 
  geom_line() + 
  facet_grid(~ cumulative) + 
  theme_bw() + 
  scale_colour_manual(values = palette[c(1, 3, 7)])
```

```{r, echo = FALSE, message = FALSE, fig.path = 'figures/', fig.cap = 'The graphs on the left show probability distribution functions, and the graphs on the right show cumulative distribution functions. The effect of changing the mean, a horizontal shift in location, is illustrated'}
Normal <- 
  expand.grid(x = seq(-3, 7, by = 0.01),
              mu = c(0, 2, 4),
              sigma = 1) %>%
  mutate(dnorm = dnorm(x, mu, sigma),
         pnorm = pnorm(x, mu, sigma)) %>%
  gather(cumulative, prob, -x, -mu, -sigma) %>%
  mutate(cumulative = factor(cumulative,
                             c("dnorm", "pnorm"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Normal,
       mapping = aes(x = x,
                     y = prob,
                     colour = factor(mu))) + 
  geom_line() + 
  facet_grid(~ cumulative) + 
  theme_bw() + 
  scale_colour_manual(values = palette[c(1, 3, 7)])
```


## Expected Values

$$\begin{align*}
E(X)
	&= \int\limits_{-\infty}^{\infty}x\frac{1}{\sqrt{2\pi}\sigma}\exp\Big\{-\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\Big\} \\
^{[1]}	&= \frac{1}{\sqrt{2\pi}\sigma}\int\limits_{-\infty}^{\infty}x\exp\Big\{-\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\Big\} \\
  &= \frac{1}{\sqrt{2\pi}\sigma}\int\limits_{-\infty}^{\infty}(z\sigma+\mu)\exp\Big\{-\frac{1}{2}z^2\Big\}\sigma dz \\
  &= \frac{1}{\sqrt{2\pi}\sigma}\int\limits_{-\infty}^{\infty}z\sigma^2\exp\Big\{-\frac{1}{2}z^2\Big\}
		+\mu\sigma\exp\Big\{-\frac{1}{2}z^2\Big\}dz \\
  &= \frac{1}{\sqrt{2\pi}\sigma}\Bigg\lbrack\int\limits_{-\infty}^{\infty}z\sigma^2\exp\Big\{-\frac{1}{2}z^2\Big\}dz 
		+\int\limits_{-\infty}^{\infty}\mu\sigma\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack\\
&= \frac{1}{\sqrt{2\pi}\sigma}\Bigg\lbrack-\sigma^2\exp\Big\{-\frac{1}{2}z^2\Big\}\Bigg|_{-\infty}^{\infty}
		+\int\limits_{-\infty}^{\infty}\mu\sigma\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack\\
  &= \frac{1}{\sqrt{2\pi}\sigma}\Bigg\lbrack-0+0+\int\limits_{-\infty}^{\infty}\mu\sigma\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack\\
  &= \frac{1}{\sqrt{2\pi}\sigma}\Bigg\lbrack\mu\sigma\int\limits_{-\infty}^{\infty}\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack \\
^{[2]}	&= \frac{1}{\sqrt{2\pi}\sigma}\Bigg\lbrack2\mu\sigma\int\limits_{0}^{\infty}\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack \\
  &= \frac{2\mu\sigma}{\sqrt{2\pi}\sigma}\Bigg\lbrack\int\limits_{0}^{\infty}\exp\Big\{-\frac{1}{2}z^2\Big\}dz\Bigg\rbrack \\
^{[3]}	&= \frac{2\mu}{\sqrt{2\pi}}\int\limits_{0}^{\infty}\frac{1}{2}u^{-\frac{1}{2}}e^{-\frac{u}{2}}du\\
  &= \frac{2\mu}{2\sqrt{2\pi}}\int\limits_{0}^{\infty}u^{-\frac{1}{2}}e^{-\frac{u}{2}}du \\
	&= \frac{\mu}{\sqrt{2\pi}}\int\limits_{0}^{\infty}u^{\frac{1}{2}-1}e^{-\frac{u}{2}}du\\
^{[4]}  &= \frac{\mu}{\sqrt{2\pi}}2^{\frac{1}{2}}\Gamma\Big(\frac{1}{2}\Big) \\
^{[5]}	&= \frac{\mu\sqrt{2\pi}}{\sqrt{2\pi}} \\
  &= \mu
\end{align*}$$

> 1. Let $z=\frac{x-\mu}{\sigma}\\ \Rightarrow x=z\sigma+\mu \Rightarrow dx=\sigma dz$
> 2. This change can be made because the function being integrated is an even function.  See Theorem \ref{Integration1.2}
> 3. Let $u=z^2 \Rightarrow z=u^{\frac{1}{2}} \Rightarrow dz=\frac{1}{2}u^{-\frac{1}{2}}$
> 4. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$
> 5. $\Gamma(\frac{1}{2})=\sqrt{\pi}$


$$\begin{align*} 
E(X^2)
	&= \int\limits_{-\infty}^{\infty}x^2\frac{1}{\sqrt{2\pi}\sigma}
		\exp\Big\{\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\Big\}dx \\
^{[1]}	&= \int\limits_{-\infty}^{\infty}\frac{(z\sigma+\mu)^2}{\sqrt{2\pi}\sigma}
		\exp\Big\{\frac{z^2}{2}\Big\}\sigma dz \\
  &= \int\limits_{-\infty}^{\infty}\frac{(z\sigma+\mu)^2}{\sqrt{2\pi}}
		\exp\Big\{\frac{z^2}{2}\Big\}dz \\
	&= \int\limits_{-\infty}^{\infty}\frac{z^2\sigma^2+2z\sigma\mu+\mu^2}{\sqrt{2\pi}}
		\exp\Big\{\frac{z^2}{2}\Big\}dz \\
  &= \int\limits_{-\infty}^{\infty}\Bigg\lbrack
		\frac{z^2\sigma^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}
		+\frac{2z\sigma\mu}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}
		+\frac{\mu^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\} \Bigg\rbrack dz \\
  &= \int\limits_{-\infty}^{\infty}\frac{z^2\sigma^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+\int\limits_{-\infty}^{\infty}\frac{2z\sigma\mu}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+\int\limits_{-\infty}^{\infty}\frac{\mu^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz\\
  &= \sigma^2\int\limits_{-\infty}^{\infty}\frac{z^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+2\sigma\mu\int\limits_{-\infty}^{\infty}\frac{z}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+\mu^2\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz \\
^{[2]}  &= \sigma^2\int\limits_{-\infty}^{\infty}\frac{z^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+2\sigma\mu\int\limits_{-\infty}^{\infty}\frac{z}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+\mu^2\cdot 1 \\
^{[3]} &= \sigma^2\int\limits_{-\infty}^{\infty}\frac{z^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+2\sigma\mu E(Z)+\mu^2 \\
 &= \sigma^2\int\limits_{-\infty}^{\infty}\frac{z^2}{\sqrt{2\pi}}\exp\Big\{\frac{z^2}{2}\Big\}dz
		+2\sigma\mu\cdot0+\mu^2 \\
^{[4]} &= \sigma^2\Bigg(\frac{z}{\sqrt{2\pi}\cdot-\exp\Big\{-\frac{z^2}{2}\Big\}}
		-\int\limits_{-\infty}^{\infty}-\exp\Big\{-\frac{z^2}{2}\frac{1}{\sqrt{2\pi}}dz\Bigg)+\mu^2 \\
 &= \sigma^2\Bigg(-0+0+\int\limits_{-\infty}^{\infty}
		\frac{1}{\sqrt{2\pi}}\exp\Big\{-\frac{z^2}{2}\Big\}dz\Bigg)+\mu^2 \\
^{[5]} &= \sigma^2(1)+\mu^2 \\
  &=\sigma^2+\mu^2
\end{align*}$$

> 1. $z=\frac{x-\mu}{\sigma} \\
		\Rightarrow x=z\sigma+\mu\Rightarrow dx=\sigma dz$
> 2. Theorems \ref{Normal4.1} and \ref{Normal4.2} with $\mu=0$ and $\sigma^2=1$
> 3. Using the previous result with $\mu=0$ and $\sigma^2=1$, $E(Z)=0$.
> 4. Integration by parts: $\int\limits_{a}^{b}u\ dv
		=\lbrack u\cdot v\rbrack_a^b-\int\limits_{a}^{b}v\ du$ with 
		$u=\frac{z}{\sqrt{2\pi}}\Rightarrow du=\frac{1}{\sqrt{2\pi}}dz$\\
		 and $dv=z\exp\{-\frac{z^2}{2}\}\Rightarrow v=-\exp\{-\frac{z^2}{2}\}$
> 5. See footnote 2.

$$\begin{align*}
\mu
	&= E(X) \\
	&= \mu \\
\\
\\
\sigma^2
	&= E(X^2)-E(X)^2 \\
	&= \sigma^2+\mu^2-\mu^2 \\
	&= \sigma^2
\end{align*}$$



## Moment Generating Function

$$\begin{align*} 
E(e^{tX})
	&= \int\limits_{-\infty}^{\infty}\exp\{tx\}\frac{1}{\sqrt{2\pi}\sigma}
		\exp\Big\{\frac{1}{2\sigma^2}(x-\mu)^2\Big\}dx \\
	&= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}
		\exp\Big\{tx-\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\Big\}dx \\
^{[1]} &= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}
		\exp\Big\{t(z\cdot\sigma+\mu-\frac{1}{2}z^2\Big\}\sigma dz \\
 &= \int\limits_{-\infty}^{\infty}\frac{\sigma}{\sqrt{2\pi}\sigma}
		\exp\Big\{t(z\cdot\sigma+\mu)-\frac{1}{2}z^2\Big\}dz \\
 &= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}
		\exp\{\mu t\}\exp\Big\{z\sigma t-\frac{1}{2}z^2\Big\}dz \\
 &= \exp\{\mu t\}\int\limits_{-\infty}^{\infty}\frac{\sigma}{\sqrt{2\pi}\sigma}
		\exp\Big\{z\sigma t-\frac{1}{2}z^2\Big\}dz \\
 &= \exp\{\mu t\}\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}
		\exp\Big\{z\sigma t-\frac{1}{2}z^2-\frac{1}{2}t^2\sigma^2+\frac{1}{2}t^2\sigma^2\Big\}dz \\
 &= \exp\{\mu t\}\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}
		\exp\Big\{-\frac{1}{2}z^2+z\sigma t-\frac{1}{2}t^2\sigma^2\Big\}
		\exp\Big\{\frac{1}{2}t^2\sigma^2\Big\}dz \\
 &= \exp\{\mu t\}\exp\Big\{\frac{1}{2}t^2\sigma^2\Big\}\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}
		\exp\Big\{z^2-2z\sigma t+t^2\sigma^2\Big\}dz \\
 &= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}
		\exp\Big\{\frac{1}{2}(z-\sigma t)^2\Big\}dz \\
^{[2]} &= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}
		\exp\Big\{\frac{1}{2}(u)^2\Big\}du \\
 &= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\cdot 1 \\
 &= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}
\end{align*}$$

> 1. $z=\frac{x-\mu}{\sigma}\\ 
		\Rightarrow x=z\cdot\sigma+\mu\Rightarrow dx=\sigma dz$
> 2. $u=z-\sigma t\\ \Rightarrow z=u+\sigma t \Rightarrow dz=du$
> 3. See Theorems \ref{Normal4.1} and \ref{Normal4.2}

$$\begin{align*}
M_X^{(1)}(t)
	&= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\cdot(\mu+t\sigma^2) \\
\\
\\
M_X^{(2)}(t)
	&= \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\cdot\sigma^2
		+(\mu+t\sigma^2)\exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}(\mu+t\sigma^2) \\
  &= M_X^{(2)}(t)=\exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}\cdot\sigma^2
		+(\mu+t\sigma^2)^2\exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\} \\
\\
\\
E(X)
	&= M_X^{(1)}(0)=\exp\Big\{\mu\cdot0+\frac{0^2\sigma^2}{2}\Big\}\cdot(\mu+0\cdot\sigma) \\
	&= e^0\cdot(\mu+0) \\
	&= 1\cdot\mu \\
	&= \mu \\
\\
\\
E(X^2)
	&= M_X^{(2)}(0) \\
	&= \exp\Big\{\mu\cdot0+\frac{0^2\sigma^2}{2}\Big\}\cdot\sigma^2 
		+(\mu+0\cdot \sigma^2)^2\exp\Big\{\mu\cdot0+\frac{0^2\sigma^2}{2}\Big\} \\
  &= e^0\sigma^2+(\mu+0)^2e^0 \\
	&= 1\cdot\sigma^2+\mu^2\cdot1 \\
	&= \sigma^2+\mu^2 \\
\\
\\
\mu
	&= E(X) \\
	&= \mu \\
\\
\\
\sigma^2
	&= E(X^2)-E(X) \\
	&= \sigma^2+\mu^2-\mu^2 \\
	&= \sigma^2
\end{align*}$$


## Maximum Likelihood Estimators

Let $x_1,x_2,\ldots,x_n$ be a random sample from a Normal distribution with mean $\mu$ and variance $\sigma^2$.

### Likelihood Function
$$\begin{align*}
L(x_1,x_2,\ldots,x_n|\theta)
	&= f(x_1|\theta)f(x_2|\theta)\cdots f(x_n|\theta) \\
  &= \frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{\frac{-(x_1-\mu)^2}{2\sigma^2}\bigg\}
		\frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{\frac{-(x_2-\mu)^2}{2\sigma^2}\bigg\}
		\cdots \frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{\frac{-(x_n-\mu)^2}{2\sigma^2}\bigg\} \\
  &= \prod\limits_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp\bigg\{\frac{-(x_i-\mu)^2}{2\sigma^2}\bigg\} \\
	&= \bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n  \prod\limits_{i=1}^{n}\exp\bigg\{\frac{-(x_i-\mu)^2}{2\sigma^2} \bigg\} \\
  &= \bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n \exp\bigg\{ \sum\limits_{i=1}^{n}\frac{-(x_i-\mu)^2}{2\sigma^2} \bigg\} \\
  &= \bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n \exp\bigg\{ \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \bigg\}
\end{align*}$$


### Log-likelihood Function
$$\begin{align*} 
\ell(\theta)
	&= \ln\big(L(\theta)\big) \\
	&= \ln\Bigg(\bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n 
		\exp\bigg\{ \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \bigg\}\Bigg) \\
  &= \ln\bigg(\frac{1}{\sqrt{2\pi}\sigma}\bigg)^n 
		+ \ln\bigg(\exp\bigg\{ \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \bigg\}\bigg) \\
  &= n\ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
  &= n\ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}(x_i^2 - 2\mu x_i + \mu^2) \\
  &= n\ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2} 
		\bigg\lbrack \sum\limits_{i=1}^{n}x_i^2 - 2\mu\sum\limits_{i=1}^{n}x_i + \sum\limits_{i=1}^{n}\mu^2\bigg\rbrack \\
  &= n\ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2}\sum\limits_{i=1}^{n}x_i^2
		+ \frac{2\mu}{2\sigma^2}\sum\limits_{i=1}^{n}x_i - \frac{1}{2\sigma^2}n\mu^2 \\
  &= n\ln\bigg(\frac{1}{\sqrt{2\pi}}\sigma^{-1}\bigg) - \frac{\sigma^{-2}}{2}\sum\limits_{i=1}^{n}x_i^2
		+ \mu\sigma^{-2}\sum\limits_{i=1}^{n}x_i - \frac{n\mu^2\sigma^{-2}}{2}
\end{align*}$$


### MLE for $\mu$

$$\begin{align*}
\frac{d\ell}{d\mu}
	&= 0 - 0 + \sigma^{-2}\sum\limits_{i=1}^{n} - \frac{2n\mu\sigma^{-2}}{2} \\
	&= \sigma^{-2}\sum\limits_{i=1}^{n} - n\mu\sigma^{-2}\\
\\
\\
0 &= \sigma^{-2}\sum\limits_{i=1}^{n} - n\mu\sigma^{-2}\\
 \Rightarrow n\mu\sigma^{-2} &= \sigma^{-2}\sum\limits_{i=1}^{n}\\
 \Rightarrow n\mu\ &= \sum\limits_{i=1}^{n}\\
 \Rightarrow \mu &= \frac{1}{n}\sum\limits_{i=1}^{n}
\end{align*}$$

So $\hat\mu = \frac{1}{n}\sum\limits_{i=1}^{n}$ is the maximum likelihood estimator for $\mu$.


### MLE for $\sigma^2$

The work in deriving the MLE for $\sigma^2$ is greatly reduced if we use

$$\begin{align*}
\ell(\theta)
	&= n\ln\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2} \sum\limits_{i=1}^{n}-(x_i-\mu)^2\\
	&= n\ln\bigg(\frac{1}{\sqrt{2\pi}}\sigma^{-1}\bigg) - \frac{1}{2\sigma^2}  \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
\\
\\
\frac{d\ell}{d\sigma}
	&= n\bigg(\frac{1}{\frac{1}{\sqrt{2\pi}}\sigma^{-1}}\bigg) \bigg(\frac{-1}{\sqrt{2\pi}\sigma^{-2}}\bigg)
		- \frac{-2}{2}\sigma^{-3} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
 &= \frac{-n\sqrt{2\pi}\sigma}{\sqrt{2\pi}\sigma^2} + \frac{1}{\sigma^3} \sum\limits_{i=1}^{n}-(x_i-\mu)^2\\
 &= \frac{-n}{\sigma} + \frac{1}{\sigma} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
\\
\\
0 &= \frac{-n}{\sigma} + \frac{1}{\sigma} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
 \Rightarrow \frac{n}{\sigma} &= \frac{1}{\sigma} \sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
 \Rightarrow \frac{\sigma^3}{\sigma} &= \frac{1}{n}\sum\limits_{i=1}^{n}-(x_i-\mu)^2 \\
 \Rightarrow \sigma^2 &= \frac{1}{n}\sum\limits_{i=1}^{n}-(x_i-\mu)^2
\end{align*}$$

So $\hat\sigma^2 = \frac{1}{n}\sum\limits_{i=1}^{n}-(x_i-\mu)^2$ is the maximum likelihood estimator for $\sigma^2$.  Notice however that this $MLE$ is a biased estimator\footnote{See \ref{Variance2}}.


## Theorems for the Normal Distribution

### Validity of the Distribution (Polar Coordinates)

$$\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2}dx=1$$

_Proof:_

Let 
$A = \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2}dx$.  

By using the identity transformation $y=x$, we may also write 

$$\begin{align*}
A &= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(y-\mu)^2}{2\sigma^2}dy \\
\Rightarrow A^2 &= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}
			e^\frac{-(x-\mu)^2}{2\sigma^2}dx\cdot 
		\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}
			e^\frac{-(y-\mu)^2}{2\sigma^2}dy \\
  &= \frac{1}{2\pi\sigma^2}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}
		e^\frac{-(x-\mu)^2}{2\sigma^2}e^\frac{-(y-\mu)^2}{2\sigma^2}dx\ dy \\
  &= \frac{1}{2\pi\sigma^2}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}
		e^{\frac{-(x-\mu)^2}{2\sigma^2}-\frac{(y-\mu)^2}{2\sigma^2}} \\
  &= \frac{1}{2\pi\sigma^2}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}
		e^{-\frac{1}{2\sigma^2}[(x-\mu)^2+(y-\mu)^2]}dx\ dy \\
^{[1]}  &= \frac{1}{2\pi\sigma^2}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}
		e^{-\frac{1}{2\sigma^2}t^2+u^2}dt\ du \\
^{[2]}	&= \frac{1}{2\pi\sigma^2}\int\limits_{0}^{2\pi}\int\limits_{0}^{\infty} 
		e^{-\frac{r^2}{2\sigma^2}}r\ dr\ d\theta \\
  &=\frac{1}{2\pi\sigma^2}\int\limits_{0}^{2\pi}
		-\sigma^2e^{-\frac{r^2}{2\sigma^2}}\Big|_0^\infty d\theta \\
	&=\frac{1}{2\pi\sigma^2}\int\limits_{0}^{2\pi}0+\sigma^2\ d\theta \\
  &=\frac{1}{2\pi\sigma^2}\int\limits_{0}^{2\pi}\sigma^2\ d\theta \\
	&=\frac{1}{2\pi\sigma^2}\cdot \Big(\sigma^2\theta\Big|_0^{2\pi}\Big) \\
	&=\frac{1}{2\pi\sigma^2}[2\pi\sigma^2-0] \\
	&=\frac{2\pi\sigma^2}{2\pi\sigma^2} \\
	&=1\\
\Rightarrow A^2 &= 1 \\
\Rightarrow A &= 1
\end{align*}$$

> 1. Substitute $t=x-\mu$ and $u=y-\mu$
> 2. $t=r\cos\theta$ and $u=r\sin\theta$.\\ Thus 
		$t^2+u^2=r^2\cos^2\theta+r^2\sin^2\theta=r^2(\cos^2\theta+\sin^2\theta)=r^2$

Thus  $\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2}dx=1$


### Validity of the Distribution (Gamma Function)

$$\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2}dx=1$$

_Proof:_

$$\begin{align*}
\int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^\frac{-(x-\mu)^2}{2\sigma^2}dx
	      &=\frac{1}{\sqrt{2\pi}\sigma}\int\limits_{-\infty}^{\infty}e^\frac{-(x-\mu)^2}{2\sigma^2}dx
^{[1]}	&= \frac{1}{\sqrt{2\pi}\sigma}
              \int\limits_{-\infty}^{\infty}e^\frac{-(y)^2}{2\sigma^2}dy\\
        &=\frac{1}{\sqrt{2\pi}\sigma}
              \int\limits_{-\infty}^{\infty}e^{-\frac{1}{2}\frac{y^2}{\sigma^2}}dy
^{[2]}	&= \frac{1}{\sqrt{2\pi}\sigma}
              \int\limits_{-\infty}^{\infty}
                    e^{-\frac{u}{2}}\frac{1}{2}\sigma u^{-\frac{1}{2}}du\\
        &=\frac{\sigma}{2\sqrt{2\pi}\sigma}
              \int\limits_{-\infty}^{\infty}u^{-\frac{1}{2}}e^{-\frac{u}{2}}du
^{[3]}	&= \frac{2\sigma}{2\sqrt{2\pi}\sigma}
              \int\limits_{0}^{\infty}u^{\frac{1}{2}-1}e^{-\frac{u}{2}}du\\
^{[4]}  &= \frac{1}{\sqrt{2\pi}}2^\frac{1}{2}\Gamma\big(\frac{1}{2}\big) \\
^{[5]}	&= \frac{\sqrt{2\pi}}{\sqrt{2\pi}} \\
        &= 1
\end{align*}$$

> 1. $y=x-\mu\ \Rightarrow x=y+\mu\ \Rightarrow dx=dy$
> 2. $u=(\frac{y}{\sigma})^2\ \Rightarrow y=\sigma u^\frac{1}{2}\ 
		\Rightarrow dy=\frac{1}{2}\sigma u^{-\frac{1}{2}}$
> 3. If $f(x)$ is an even function then $\int\limits_{-\infty}^{\infty}f(x)dx
		=2\int\limits_{0}^{\infty}f(x)dx$ (Theorem \ref{Integration1.2}).
> 4. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$
> 5. $\Gamma(\frac{1}{2})=\sqrt{\pi}$
		


### Multiple of a Normal Random Variable

Let $X$ be a Normal random variable with parameters $\mu$ and $\sigma^2$, and let $c$ be a constant.  If $Y=cX$, then $Y\sim Normal(c\mu,\ c^2\sigma^2)$.

_Proof:_

$$\begin{align*}
M_Y(t)
  &= E(e^{tY}) \\
  &= E(e^{tcX}) \\
  &= \exp\Big\{\mu tc+\frac{t^2c^2\sigma^2}{2}\Big\} \\
  &= \exp\Big\{c\mu t+\frac{t^2c^2\sigma^2}{2}\Big\}
\end{align*}$$

Which is the Moment Generating Function of a Normal random variable with mean $c\mu$ and variance $c^2\sigma^2$.  Thus $Y\sim Normal(c\mu,\ c^2\sigma^2)$. 


### Weighted Sum of Normal Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from Normal distributions with parameters $\mu_i$ and $\sigma_i^2$, i.e. $X_i\sim Normal(\mu_i,\sigma_i^2),\ i=1,2,\ldots,n$, and let $a_1,a_2,\ldots,a_n$ be constants.

If $Y=\sum\limits_{i=1}^{n}a_iX_i$, then $Y\sim Normal\big(\sum\limits_{i=1}^{n}a_i\mu_i,\ \sum\limits_{i=1}^{n}a_i^2\sigma_i^2\big)$.

_Proof:_

$$\begin{align*} 
M_Y(t)
  &=E(e^{tY}) \\
  &=E(e^{t(a_1X_1+a_2X_2+\cdots+a_nX_n}) \\
  &=E(e^{ta_1X_1}e^{ta_2X_2}\cdots e^{ta_nX_n}) \\
  &=E(e^{ta_1X_1})E(e^{ta_2X_2})\cdots E(e^{ta_nX_n}) \\
  &=\exp\Big\{ta_1\mu_1+\frac{ta_1^2\sigma_1^2}{2}\Big\}\cdot 
		\exp\Big\{ta_2\mu_2+\frac{ta_2^2\sigma_2^2}{2}\Big\} \cdot
		\cdots \cdot \exp\Big\{ta_n\mu_n+\frac{ta_n^2\sigma_n^2}{2}\Big\} \\
  &=\exp\Big\{ta_1\mu_1+\frac{ta_1^2\sigma_1^2}{2}+ta_2\mu_2+\frac{ta_2^2\sigma_2^2}{2}
		+\cdots+ta_n\mu_n+\frac{ta_n^2\sigma_n^2}{2}\Big\} \\
  &=\exp\Big\{t\sum\limits_{i=1}^{n}a_i\mu_i +
      \frac{t\sum\limits_{i=1}^{n}a_i^2\sigma_i^2}{2}\Big\}
\end{align*}$$

Which is the mgf of a Normal random variable with parameters $\sum\limits_{i=1}^{n}a_i\mu_i$ and $\sum\limits_{i=1}^{n}a_i^2\sigma_i^2$.  Thus $Y\sim Normal\big(\sum\limits_{i=1}^{n}a_i\mu_i,\ \sum\limits_{i=1}^{n}a_i^2\sigma_i^2\big)$.


### Sum of Normal Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from Normal distributions with parameters $\mu_i$ and $\sigma_i^2$, i.e. $X_i\sim Normal(\mu_i, \sigma_i^2),\ i=1,2,\ldots,n$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.  Then $Y\sim Normal\big(\sum\limits_{i=1}^{n}\mu_i,\ \sum\limits_{i=1}^{n}\sigma_i^2\big)$.

_Proof:_

$Y=\sum\limits_{i=1}^{n}X_i$ is a special case of the Weighted Sum of Normal Random Variables where each of the weights is equal to 1.  It follows then that $Y\sim Normal\big(\sum\limits_{i=1}^{n}1\mu_i,\ \sum\limits_{i=1}^{n}1^2\sigma_i^2\big)$ which is equivalent to saying $Y\sim Normal\big(\sum\limits_{i=1}^{n}\mu_i,\ \sum\limits_{i=1}^{n}\sigma_i^2\big)$.



### Standard Normal Transformation

Let $X$ be a Normal random variable with mean $\mu$ and variance $\sigma^2$.  That is, $X\sim$Normal($\mu,\sigma^2$).  If $Z=\frac{X-\mu}{\sigma}$, then $Z\sim$Normal(0,1), and we say $Z$ has a _Standard Normal_ distribution.

Furthermore, if $Z\sim$Normal(0,1), and $X=Z\cdot\sigma+\mu$, then $X\sim Normal(\mu,\sigma^2$).

_Proof:_

First, since $X\sim Normal(\mu,\sigma^2$) and $Z=\frac{X-\mu}{\sigma}$

$$\begin{align*}
E(e^{tZ})
  &=E\Big(\exp\Big\{t\frac{x-\mu}{\sigma}\Big\}\Big) \\
  &=E\Big(\exp\Big\{\frac{tx-t\mu}{\sigma}\Big\}\Big) \\
	&=E\Big(\exp\Big\{\frac{tx}{\sigma}-\frac{t\mu}{\sigma}\Big\}\Big) \\
  &=E\Big(\exp\Big\{\frac{tx}{\sigma}\Big\}\exp\Big\{-\frac{t\mu}{\sigma}\Big\}\Big) \\
	&=\exp\Big\{\frac{-t\mu}{\sigma}\Big\}E\Big(\exp\Big\{\frac{t}{\sigma}x\Big\} \\
  &=\exp\Big\{\frac{-t\mu}{\sigma}\Big\}\exp\Big\{\frac{t\mu}{\sigma}+\frac{t^2\sigma^2}{2\sigma^2}\Big\} \\
	&=\exp\Big\{\frac{-t\mu}{\sigma}-\frac{t\mu}{\sigma}+\frac{t^2\sigma^2}{2\sigma^2}\Big\} \\
	&=\exp\Big\{\frac{t^2\sigma^2}{2\sigma^2}\Big\}
\end{align*}$$

Which is the Moment Generating Function of a Normal random variable with $\mu=0$ and $\sigma^2=1$.  Thus $Z\sim Normal(0,1)$.

To complete the proof, we start with $Z\sim$Normal(0,1) and let $X=Z\cdot\sigma+\mu$.

$$\begin{align*}
E(e^{tX})
  = E(e^{t(z\sigma+\mu)}) \\
  = E(e^{tz\sigma}e^{t\mu}) \\
  = e^{t\mu}E(e^{t\sigma z}) \\
  = \exp\{t\mu\}\exp\Big\{\frac{t^2\sigma^2}{2}\Big\} \\
  = \exp\Big\{\mu t+\frac{t^2\sigma^2}{2}\Big\}
\end{align*}$$

Which is the Moment Generating Funciton of a Normal random variable with mean $\mu$ and variance $\sigma^2$.  Thus $X\sim Normal(\mu,\sigma^2)$.



### Distribution of $\bar X$

Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Normal distribution with parameters $\mu$ and $\sigma^2$, i.e. $X_i\sim Normal(\mu,\ \sigma^2)$.  Let $\bar X=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$.  Then $\bar X\sim Normal\big(\mu,\ \frac{\sigma^2}{n}\big)$.

_Proof:_

$\bar X=\frac{1}{n}\sum\limits_{i=1}^{n}X_i$ is a special case of the Weighted Sum of Normal Random Variables where $a_i=\frac{1}{n},\ i=1,2,\ldots,n$.  It follows then that

$$\begin{align*}
M_{\bar X}(t)
  &=exp\Big(t\sum\limits_{i=1}^{n}\frac{1}{n}\mu
		+t\frac{\sum\limits_{i=1}^{n}\frac{1}{n}\sigma^2}{2}\Big) \\
	&=exp\big(t\frac{n\mu}{n}+\frac{t\frac{n\sigma^2}{n}}{2}\big) \\
	&=exp\big(t\mu+\frac{t\frac{\sigma^2}{n}}{2}\big) 
\end{align*}$$

Which is the mgf of a Normal random variable with parameters $\mu$ and $\frac{\sigma^2}{n}$.  Thus $\bar X\sim Normal\big(\mu,\ \frac{\sigma^2}{n}\big)$. 


### Square of a Standard Normal Random Variable

If $Z\sim N(0,1)$, then $Z^2\sim\chi^2(1)$. 

_Proof:_
$$\begin{align*}
M_{Z^2}(t)
      	&= E(e^{tZ^2}) \\
      	&= \int\limits_{-\infty}^{\infty}e^{tz^2}\frac{1}{\sqrt{2\pi}}
      		e^{-\frac{z^2}{2}}dz \\
      	&= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}e^{tz^2}
      		e^{-\frac{z^2}{2}}dz \\
        &= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}
      		e^{-\frac{z^2}{2}(-2t+1)}dz \\
      	&= \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}
      		e^{-\frac{z^2}{2}(1-2t)}dz \\
^{[1]}  &= \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}
      		e^{-\frac{z^2}{2}(1-2t)}dz \\
^{[2]}	&= \frac{2}{\sqrt{2\pi}}\int\limits_{0}^{\infty}e^{-u}
      		\frac{\sqrt{2}u^{-\frac{1}{2}}}{2(1-2t)^{\frac{1}{2}}}du \\
        &= \frac{2\sqrt{2}}{2\sqrt{2\pi}(1-2t)^{\frac{1}{2}}}
      		\int\limits_{0}^{\infty}e^{-u}u^{-\frac{1}{2}}du \\
      	&= \frac{2\sqrt{2}}{2\sqrt{2\pi}(1-2t)^{\frac{1}{2}}}
      		\int\limits_{0}^{\infty}u^{\frac{1}{2}-1}e^{-u}du \\
^{[3]}  &= \frac{1}{\sqrt{\pi}(1-2t)^{\frac{1}{2}}}\Gamma(\frac{1}{2}) \\
      	&= \frac{\sqrt{\pi}}{\sqrt{\pi}(1-2t)^{\frac{1}{2}}} \\
      	&= \frac{1}{(1-2t)^{\frac{1}{2}}}=(1-2t)^{-\frac{1}{2}} \\
\end{align*}$$

> 1. $\int\limits_{-\infty}^{\infty}f(x)dx
		= 2\int\limits_{0}^{\infty}f(x)dx$ when f(x) is an even function (\ref{Integration1.2})
> 2. Let $u=\frac{z^2}{2}(1-2t)  
>    \ \ \ \ \Rightarrow z=\frac{\sqrt{2}u^{\frac{1}{2}}}{(1-2t)^{\frac{1}{2}}}$  
>    \ \ \ \ So $dz=\frac{\sqrt{2}u^{-\frac{1}{2}}} {2(1-2t)^{\frac{1}{2}}}$
> 3. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-\frac{x}{\beta}}dx
		=\beta^\alpha\Gamma(\alpha)$	
		
		
Which is the mgf of a Chi-Square random variable with 1 degree of freedom.  Thus $Z^2\sim\chi^2(1)$. 

<!--chapter:end:NormalDistribution.Rmd-->

# Poisson Distribution

## Probability Mass Function

A random variable is said to have a Poisson distribution with parameter $\lambda$ if its probability mass function is:

\[p(x)=\left\{
	\begin{array}{ll}
		\frac{\lambda^xe^{-\lambda}}{x!}, & x=0,1,2,...\\
		0 & otherwise 
	\end{array}\right. 
\]

## Cumulative Mass Function

\[P(x)=\left\{
	\begin{array}{lll}
		e^{-\lambda}\sum\limits_{i=0}^{x}\frac{\lambda^i}{i!}, & x=1,2,3,...\\
		0 & otherwise 
	\end{array}\right.
\]

A recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:

$$\begin{align*} 
P(X=x+1)
	&= \frac{e^{-\lambda}\lambda^{x+1}}{(x+1)!} \\
	&= \frac{\lambda}{x+1}\frac{e^{-\lambda}\lambda^x}{x!} \\
	&= \frac{\lambda}{x+1}P(X=x)
\end{align*}$$

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.path = 'figures/', fig.cap = 'The graphs on the left and right show a Poisson probability cumulative distribution function, respectively, for $\\lambda=3$.'}
Poisson <- 
  data.frame(x = 0:10) %>%
  mutate(pmf = dpois(x, 3),
         cmf = ppois(x, 3)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             c("pmf", "cmf"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Poisson,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob), 
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:1) + 
  ylab("P(x)") + 
  theme_bw()
```


## Expected Values

$$\begin{align*}
E(X)
	&= \sum\limits_{x=0}^{\infty}x\frac{\lambda^xe^{-\lambda}}{x!} \\
	&= e^{-\lambda}\sum\limits_{x=0}^{\infty}x\frac{\lambda^x}{x!} \\
	&= e^{-\lambda}\Big(0\frac{\lambda^0}{0!}+1\frac{\lambda^1}{1!}
		+2\frac{\lambda^2}{2!}+3\frac{\lambda^3}{3!}+\cdots\Big) \\
  &= e^{-\lambda}\Big(0+\lambda^1+\frac{\lambda^2}{1!}+\frac{\lambda^3}{2!}+\cdots\Big) \\
	&= \lambda e^{-\lambda}\Big(\lambda^0+\frac{\lambda^1}{1!}+\frac{\lambda^2}{2!}+\cdots\Big) \\
  &= \lambda e^{-\lambda}\Big(\frac{\lambda^0}{0!}+\frac{\lambda^1}{1!}
		+\frac{\lambda^2}{2!}+\cdots\Big) \\
	&= \lambda e^{-\lambda}e^\lambda \\
	&= \lambda e^(-\lambda+\lambda) \\
	&= \lambda
\end{align*}$$

> 1. Taylor Series Expansion: $e^x=\frac{x^0}{0!}+\frac{x^1}{1!}
			+\frac{x^2}{2!}+\cdots
		=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\cdots$
		
$$\begin{align*}
E(X^2)
	&= \sum\limits_{x=0}^{\infty}x\frac{\lambda^xe^{-\lambda}}{x!} \\
	&= e^{-\lambda}\sum\limits_{x=0}^{\infty}x\frac{\lambda^x}{x!} \\
	&= e^{-\lambda}\Big(0^2\frac{\lambda^0}{0!}+1^2\frac{\lambda^1}{1!}
		+2^2\frac{\lambda^2}{2!}+3^2\frac{\lambda^3}{3!}+\cdots\Big) \\
  &= \lambda e^{-\lambda}\Big(\frac{\lambda^0}{1}
		+2\frac{\lambda^1}{1!}+3\frac{\lambda^2}{2!}+\cdots\Big) \\
	&= \lambda e^{-\lambda}\sum\limits_{x=0}^{\infty}(x+1)\frac{\lambda^x}{x!} \\
	&= \lambda e^{-\lambda}\sum\limits_{x=0}^{\infty} 
		\Big(x\frac{\lambda^x}{x!}+\frac{\lambda^x}{x!}\Big) \\
  &= \lambda e^{-\lambda}\Big(\sum\limits_{x=0}^{\infty}x\frac{\lambda^x}{x!}
		+\sum\limits_{x=0}^{\infty}\frac{\lambda^x}{x!}\Big) \\
	&= \lambda\sum\limits_{x=0}^{\infty}x\frac{\lambda^xe^{-\lambda}}{x!}
		+\lambda\sum\limits_{x=0}^{\infty}\frac{\lambda^xe^{-\lambda}}{x!} \\
  &= \lambda E(X)+\lambda \\
  &= \lambda^2+\lambda\\
\\
\\
\mu 
	&= E(X) \\
	&= \lambda \\
\\
\\
\sigma^2 
	&= E(X^2) - E(X) \\
	&= \lambda^2 + \lambda - \lambda^2 \\
	&= \lambda
\end{align*}$$



## Moment Generating Function

$$\begin{align*} 
M_X(t)
	&= E(e^{tX}) \\
	&= \sum\limits_{x=0}^{\infty}e^{tx}\frac{\lambda^xe^{-\lambda}}{x!} \\
	&= \sum\limits_{x=0}^{\infty}\frac{(\lambda e^{tx})^xe^{-\lambda}}{x!} \\
  &= e^{-\lambda}\sum\limits_{x=0}^{\infty}\frac{(\lambda e^{tx})^x}{x!} \\
	&= e^{-\lambda}\Big[\frac{(\lambda e^t)^0}{0!}+\frac{(\lambda e^t)^1}{1!}+
		\frac{(\lambda e^t)^2}{2!}+\cdots\Big] \\
  &= e^{-\lambda}e^{\lambda e^t} \\
  &= e^{(\lambda e^t-\lambda)} \\
  &= e^{\lambda(e^t-1)}
\end{align*}$$

> 1. Taylor Series Expansion: $e^x=\frac{x^0}{0!}+\frac{x^1}{1!}
			+\frac{x^2}{2!}+\cdots
		=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\cdots$

$$\begin{align*}
M_X^{(1)}(t)
	&= e^{\lambda(e^t-1)}\lambda e^t=\lambda e^te^{\lambda(e^t-1)} \\
\\
\\
M_X^{(2)}(t)
	&= (e^{\lambda(e^t-1)}(\lambda e^t)+(e^{\lambda(e^t-1)}\lambda e^t)(\lambda e^t) \\
	&= \lambda e^t[e^{\lambda e^t-1)}+e^{\lambda(e^t-1)}\lambda e^t] \\
  &= \lambda e^t[e^{\lambda(e^t-1)}(1+\lambda e^t)] \\
\\
\\
E(X)
	&= M_X^{(1)}(0) \\
	&= \lambda e^0e^{\lambda(e^0-1)}\lambda e^t=\lambda \\
\\
\\
E(X^2)
	&= M_X^{(2)}(0) \\
	&= \lambda e^0[e^{\lambda(e^0-1)}(1+\lambda e^0)] \\
	&= \lambda e^0[e^{\lambda(e^0-1)}(1+\lambda e^0] \\
  &= \lambda(1+\lambda) \\
	&= \lambda+\lambda^2 \\
\\
\\
\mu
	&= E(X) \\
	&= \lambda \\
\sigma^2
	&= E(X^2)-E(X) \\
	&= \lambda+\lambda^2-\lambda^2 \\
	&= \lambda
\end{align*}$$


## Maximum Likelihood Estimator

Let $x_1,x_2,\ldots,x_n$ be a random sample drawn from a Poisson Distribution with parameter $\lambda$.

### Likelihood Function

$$\begin{align*}
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta) \\
	&= p(x_1|\theta)p(x_2|\theta)\cdots p(x_n|\theta) \\
  &= \frac{e^{-\lambda}\lambda^{x_1}}{x_1!}\cdot\frac{e^{-\lambda}\lambda^{x_2}}{x_2!}
		\cdot \ \cdots\ \cdot \frac{e^{-\lambda}\lambda^{x_n}}{x_n!} \\
	&= \frac{e^{-n\lambda}\lambda^{\sum\limits_{i=1}^{n}x_i}}{\prod\limits_{i=1}^{n}x_1!}
\end{align*}$$


### Log-likelihood

$$\begin{align*} 
\ell(\lambda)
	&= \ln(L(\lambda)) \\
	&= \ln\Big[\frac{e^{-n\lambda}\lambda^{\sum\limits_{i=1}^{n}x_i}}{\prod\limits_{i=1}^{n}x_1!}\Big] \\
	&= \ln(e^{-n\lambda})+\ln\Big(\lambda^{\sum\limits_{i=1}^{n}x_i}\Big)
		- \ln\Big(\prod\limits_{i=1}^{n}x_1!\Big) \\
  &= -n\lambda + \sum\limits_{i=1}^{n}x_i\ln\Big(\lambda)-\ln(\prod\limits_{i=1}^{n}x_1!\Big)
\end{align*}$$

### MLE for $\lambda$

$$\begin{align*} 
\frac{d\ell}{d\lambda}
	&= -n - \frac{\sum\limits_{i=1}^{n}x_i}{\lambda} - 0 \\
	&= \frac{\sum\limits_{i=1}^{n}x_i}{\lambda} - n \\
\\
\\
0 &= \frac{\sum\limits_{i=1}^{n}x_i}{\lambda}-n\\
\Rightarrow \frac{\sum\limits_{i=1}^{n}x_i}{\lambda} &= n\\
\Rightarrow \sum\limits_{i=1}^{n}x_i &= n\lambda\\
\Rightarrow \frac{\sum\limits_{i=1}^{n}x_i}{n} &= \lambda
\end{align*}$$

so $\displaystyle \hat\lambda=\frac{\sum\limits_{i=1}^{n}x_i}{n}$ is the Maximum Likelihood Estimator for $\lambda$.


## Theorems for the Poisson Distribution

### Derivation of the Poisson Distribution

Suppose $X$ is a Binomial random variable in all respects but has an infinite (non-fixed) number of trials, each with probability of success $p$.  

Then the pdf of $X$ is $P(x)=\frac{e^{-\lambda}\lambda^x}{x!}$

_Proof:_

For an infinite number of trials we take 
$\lim\limits_{n\rightarrow\infty}{n\choose x}p^x(1-p)^{n-x}$.  

By rewriting $p$ in terms of $\mu\ (\mu=np\ \Rightarrow p=\frac{\mu}{n})$ we get

$$\begin{align*} 
\lim\limits_{n\rightarrow\infty}{n\choose x}\frac{\mu}{n}^x(1-\frac{\mu}{n})^{n-x}
	&= \lim\limits_{n\rightarrow\infty}{n\choose x}
		\frac{\mu}{n}^x(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
  &= \lim\limits_{n\rightarrow\infty}\bigg(\frac{n(n-1)\cdots(n-x+1)(n-x)!}{x!(n-x)!}\bigg)
		\mu^x\frac{1}{n^x}(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
  &= \lim\limits_{n\rightarrow\infty}\bigg(\frac{n(n-1)\cdots(n-x+1)}{x!}\bigg)
		\mu^x\frac{1}{n^x}(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
  &= \frac{\mu^x}{x!}\lim\limits_{n\rightarrow\infty}\bigg(\frac{n(n-1)\cdots(n-x+1)}{n^x}\bigg)
		\frac{1}{n^x}(1-\frac{\mu}{n})^n(1-\frac{\mu}{n})^{-x} \\
  &= \frac{\mu^x}{x!}\lim\limits_{n\rightarrow\infty}\bigg(\frac{n(n-1)\cdots(n-x+1)}{n^x}\bigg)
		\lim\limits_{n\rightarrow\infty}\frac{1}{n^x}(1-\frac{\mu}{n})^n
		\lim\limits_{n\rightarrow\infty}(1-\frac{\mu}{n})^{-x} \\
  &= \frac{\mu^x}{x!}\cdot 1\cdot e^{-\mu}\cdot 1 \\
  &= \frac{e^{-\mu}\mu^x}{x!}
\end{align*}$$

> 1. $\lim\limits_{n\rightarrow\infty}(1-\frac{x}{n})^n=e^-x$

Traditionally, we use $\lambda$ in place of $\mu$ for the Poisson distribution, giving us the desired result. 


### Validity of the Distribution

$$\sum\limits_{x=0}^{\infty}\frac{e^{-\lambda}\lambda^x}{x!} = 1$$

_Proof:_

$$\begin{align*} 
\sum\limits_{x=0}^{\infty}\frac{e^{-\lambda}\lambda^x}{x!}
	&= e^{-\lambda}\sum\limits_{x=0}^{\infty}\frac{\lambda^x}{x!} \\
	&= e^{-\lambda}\bigg(\frac{\lambda^0}{0!}+
	    \frac{\lambda^1}{1!}+\frac{\lambda^2}{2!}+\cdots\bigg) \\
  &= e^{-\lambda}\cdot e^\lambda \\
  &= e^0 \\
  &= 1
\end{align*}$$
	
> 1. Taylor Series Expansion: $e^x=\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}+\cdots$


### Sum of Poisson Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent random variables from a Poisson distribution with parameter $\lambda_i,\ i=1,2,\ldots,n$; that is, $X_i\sim$Poisson$(\lambda_i)$.  

Let $Y=\sum\limits_{i=1}^{n}X_i$.  

Then $Y\sim$Poisson$(\sum\limits_{i=1}^{n}\lambda_i)$.\\

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{t(X_1+X_2+\cdots+X_n)} \\
	&= E(e^{tX_1}e^{tX_2}\cdots e^{tX_n}) \\
  &= E(e^{tX_1})E(e^{tX_2})\cdots E(e^{tX_n}) \\
  &= e^{\lambda_1(e^t-1)}e^{\lambda_2(e^t-1)}\cdots e^{\lambda_n(e^t-1)} \\
  &= e^{(\lambda_1+\lambda_2+\cdots+\lambda_n)(e^t-1)} \\
	&= e^{(e^{t-1})\sum\limits_{i=1}^{n}\lambda_i}
\end{align*}$$

Which is the mgf of a Poisson random variable with parameter $\sum\limits_{i=1}^{n}\lambda_i$.  Thus $Y\sim$Poisson$(\sum\limits_{i=1}^{n}\lambda_i)$.

<!--chapter:end:Poisson_Distribution.Rmd-->

# Probability

## Elementary Probability Concepts

### Definition of Probability

Let $S$ be a sample space associated with an experiment.  For every event $A \in S$ (ie, $A$ is a subset of $S$), we assign a number, $P(A)$ - called the \emph{probability} of $A$ - such that the following three axioms hold:

* Axiom 1: $P(A) \geq 0$.
* Axiom 2: $P(S) = 1$.
* Axiom 3: If $A_1, A_2, A_3, ...$ form a sequence of pairwise mutually exclusive events in $S$ 
       (that is, $A_i \cap A_j = \emptyset$ if $i \neq j$), then  
       $P(A_1 \cup A_2 \cup A_3 \cup ...) = \sum\limits_{i=1}^\infty P(A_i)$.



### Definition: Conditional Probability

The conditional probability of an event $A$, given that an event $B$ has occured and $P(B) > 0$ is equal to 

$$ P(A|B) = \frac{P(A\cap B)}{P(B)} $$


### Definition: Independence

Events $A$ and $B$ are said to be independent if any of the following holds

$$P(A|B) = P(A)$$
$$P(B|A) = P(B)$$
$$P(A\cap B) = P(A)\cdot P(B)$$

### Theorem: Multiplicative Law of Probability

The probability of the intersection of two events $A$ and $B$ is 

$$P(A\cap B) = P(A)\cdot P(B|A) = P(B)\cdot P(A|B)$$

_Proof:_

By the definition of conditional probability

$$\begin{align*}
P(A|B) 
  &= \frac{P(A\cap B)}{P(B)} \\
\Rightarrow P(A|B) \cdot P(B) &= P(A \cap B)
\end{align*}$$

Likewise

$$\begin{align*}
P(B|A) 
  &= \frac{P(B\cap A)}{P(A)} \\
\Rightarrow P(B|A) \cdot P(A) &= P(B \cap A)
\end{align*}$$

Since $P(A \cap B) = P(B \cap A)$

$$\begin{align*} 
P(A|B) \cdot P(B) 
  &= P(A \cap B) \\
  &= P(B \cap A) \\
  &= P(B|A) \cdot P(A) \\
\Rightarrow P(A \cap B) 
  &= P(A|B) \cdot P(B) \\ 
  &= P(B|A) \cdot P(A)
\end{align*}$$


### Corollary

If $A$ and $B$ are independent, then

$$P(A \cap B) = P(A) \cdot P(B)$$

_Proof:_

When $A$ and $B$ are independent, by the definition of independence\footnote{Definition \ref{Probability1.3}}, 

$$\begin{align*}
P(A \cap B) 
  & = P(A|B) \cdot P(B) 
  = P(B|A) \cdot P(A) \\
\Rightarrow &= P(A) \cdot P(B) = P(B) \cdot P(A)
\end{align*}
$$



### Additive Law of Probability
  
The probability of the union of two events is $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.

_Proof:_

$A \cup B = A \cup (A^c \cap B)$ where $A$ and $(A^c \cap B)$ are mutually exclusive.
$\Rightarrow P(A \cup B) = P(A) + P(A^c \cap B)$

$B = (A^c \cap B) \cup (A \cap B)$ where $(A^c \cap B)$ and $(A \cap B)$ are mutually exclusive.
$\Rightarrow P(B) = P(A^c \cap B) + P(A \cap B)\\
 \Rightarrow P(A^c \cap B) = P(B) - P(A \cap B)$

$P(A \cup B) = P(A) + P(A^c \cap B)\\
 \Rightarrow P(A \cup B) = P(A) + P(B) - P(A \cap B)$
 

### Corollary

If $A$ and $B$ are mutually exclusive events, then $P(A \cup B) = P(A) + P(B)$.

_Proof:_

When $A$ and $B$ are mutually exclusive, $(A \cap B) = \emptyset$ and $P(A \cap B) = 0$.  By Theorem \ref{Probability1.5},

$$\begin{align*}
P(A \cup B) &= P(A) + P(B) - P(A \cap B) \\
 \Rightarrow P(A \cup B) &= P(A) + P(B) - 0 \\
 \Rightarrow P(A \cup B) &= P(A) + P(B)
\end{align*}$$
 

### Theorem: Law of Complements

If $A$ is an event, then $P(A) = 1 - P(A^c)$.

_Proof:_

Let $S$ be the sample space.

$$\begin{align*}
S &= A \cup A^c \\
 \Rightarrow P(S) &= P(A \cup A^c) \\
 \Rightarrow P(S) &= P(A) + P(A^c) - P(A \cap A^c) \\
 \Rightarrow P(S) &= P(A) + P(A^c) - 0 \\
 \Rightarrow P(S) &= P(A) + P(A^c) \\
 \Rightarrow 1 &= P(A) + P(A^c) \\
 \Rightarrow 1 - P(A^c) &= P(A) \\
 \Rightarrow P(A) &= 1 - P(A^c)
\end{align*}$$
 

### Definition: Partition of a Sample Space

For some positive integer $k$, let the sets $B_1, B_2, \ldots, B_k$ be such that

* $S = B_1 \cup B_2 \cup \ldots \cup B_k$.
* $B_i \cap B_j = \emptyset$ for $i \neq j$.

Then the collection of sets ${B_1, B_2, \ldots, B_k}$ is said to be a _partition_ of $S$.


### Definition: Decomposition

If $A$ is any subset of $S$ and ${B_1, B_2, \ldots, B_k}$ is a partition of $S$, $A$ can be _decomposed_ as follows:

$A = (A \cap B_1) \cup (A \cap B_2) \cup \cdots \cup (A \cap B_k)$


### Theorem: Total Law of Probability

If ${B_1, B_2, \ldots, B_k}$ is a partition of $S$ such that $P(B_i) > 0$, for $i = 1, 2, \ldots, k$, then for any event $A$

$$P(A) = \sum\limits_{i=1}^k P(A|B_i)P(B_i)$$

_Proof:_

Any subset $A$ of $S$ can be written as

$$A = A \cap S = A \cap (B_1 \cup B_2 \cup \cdots \cup B_k) = (A \cap B_1) \cup (A \cap B_2) \cup \cdots \cup (A \cap B_k)$$

Since ${B_1, B_2, \ldots, B_k}$ is a partition of $S$, if $i \neq j$,

$(A \cap B_i) \cup (A \cap B_j) = A \cap (B_i \cap B_j) = A \cap \emptyset = \emptyset$.  That is, $(A \cap B_i)$ and $(A \cap B_j)$ are mutually exclusive events.  Thus,

$$\begin{align*}
P(A) &= P(A \cap B_1) + P(A \cap B_2) + \cdots + P(A \cap B_k) \\
^{[1]} \Rightarrow &= P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \cdots + P(A|B_k)P(B_k) \\
\Rightarrow &= \sum\limits_{i=1}^k P(A|B_i)P(B_i)
\end{align*}$$
 
> 1. Theorem \ref{Probability1.4}: Multiplicative Law of Probability
 

### Theorem: Bayes' Rule

If ${B_1, B_2, \ldots, B_k}$ is a partition of $S$ such that $P(B_i) > 0$, for $i = 1, 2, \ldots, k$, then

$$ P(B_j|A) = \frac{P(A|B_j)P(B_j)}{\sum\limits_{i=1}^k P(A|B_i)P(B_i)} $$

_Proof:_

$$\begin{align*}
 \ ^{[1]} P(B_j|A)  &= \frac{P(A \cap B)}{P(A)} \\
 ^{[2]} \Rightarrow &= \frac{P(A|B_j)P(B_j)}{P(A)} \\
 ^{[3]} \Rightarrow &= \frac{P(A|B_j)P(B_j)}{\sum\limits_{i=1}^k P(A|B_i)P(B_i)}
\end{align*}$$

> 1. Definition \ref{Probability1.2}, Conditional Probability
> 2. Definition \ref{Probability1.2}, Conditional Probability
> 3. Theorem \ref{Probability1.11}, Law of Total Probability

<!--chapter:end:Probability.Rmd-->

# Real Number System

This chapter was prepared by Steve MacDonald [@McDonaa].

## Historical Note

The first axiom system known in the history of mathematics was Euclid's aximoatic development of plane geometry.  Euclid's treatment began with some _primitive_ or _undefined_ terms and some assumed statements, which he called _axioms_ and _postulates_.  For Euclid an _axiom_ was a general "self-evident" truth, such as "The whole is greater than any of its parts" or "equals added to equals are equal," whereas a _postulate_ was an assumed statement about the relationships among the primitives of his system, such as "Two points determine exactly one line."  From these undefined terms and basic assumptions a whole body of other statements, called _theorems_ was deduced.  For centuries Euclidean Geometry, which was assumed to be the true description of physical reality, remained the only mathematical systems with such an axiomatic foundation.

Then in the nineteenth century, spurred by Lobatchevsky and others who discovered that by modifying the postulates another logically consistent geometry could be constructed, matematicians began to apply this deductive approach to other branches of mathematics.  Not only did this work do much to organize and clarify such familiar disciplines as number theory, analysis, and algebra, but it helped develop new areas of mathematics such as topology.  Note that today we do not make Euclid's distinction between _axiom_ and _postulate_, using the terms synonymously.

As an example of this deductive approach, we now want to give an axiomatic description of the real numbers system and thus place a logical foundation under many of the "rules" you learned in high school algebra.


## The Field of Real Numbers

### Definition: The Field of Real Numbers

The Field of Real Numbers is a set $\Re$ of objects called _numbers_ together with two well-defined binary operations, called _addition_, denoted by `+`, and _multiplication_, denoted by $\cdot$ or juxtaposition, satisfying the Field Axioms. (By well-defined, we mean that if $s=s^\prime$ and $t=t^\prime$, then $s+t=s^\prime + t^\prime$.)

### Field Axioms

1. (Closure for addition) For each pair $x,y,\in\Re$, there exists a unique object in $\Re$, called the _sum_ of $x$ and $y$ denoted by $x+y$.
2. (Associative law for addition) For all $x,y,z\in\Re,\ (x+y)=z=x+(y+z)$.
3. (Additive identity) There exists and object $0\in\Re$ such that for all $x\in\Re,\ x+0=x=0+x$.
4. (Additive inverse) For each $x\in\Re$, there exists some $y\in\Re$ such that $x+y=0=y+x$.  We will usually denote the additive inverse of $x$ by $-x$.
5. (Commutative law of addition) For all $x,y\in\Re,\ x+y=y+x$.
6. (Closure for multiplication) For each pair $,y,\in\Re$, there exists a unique object in $\Re$, called the _product_ of $x$ and $y$ and denoted by $x\cdot y$ or $xy$.
7. (Associative law of multiplication) For all $x,y,z\in\Re,\ x\cdot(y\cdot z)=(x\cdot y)\cdot z$.
8. (Multiplicative identity) There exists an object $1\in\Re$ such that for all $x\in\Re,\ 1\cdot x=x=x\cdot 1$.
9. (Multiplicative inverse) For each $x\in\Re$ such that $x\neq 0$, there exists an object $y\in\Re$ such that $x\cdot y=1=y\cdot x$.  We will usually denote the multiplicative inverse of $x$ by $x^{-1}$.
10. (Commutative law of multiplication) For all $x,y\in\Re,\ x\cdot y=y\cdot x$.
11. (Distributive law of multiplication over addition) For all $x,y,z\in\Re, x\cdot(y+z)=x\cdot y+x\cdot z$.
12. (Positive Elements) There exists a nonempty subset $\Re^+\subset\Re$ closed under $+$ and $\cdot$.  That is, for all $x,y\in\Re^+,\ x+y\in\Re$ and $x\cdot y\in\Re^+$.
13. (Trichotomy) For any $x\in\Re$, exactly one of these three cases holds: $x\in\Re^+,\ -x\in\Re^+$, or $x=0$.


### Definiton: Less Than (or Equal To) {#real-numbers-definition-less-than-equal-to}

Let $x,y\in\Re$.  We say that $x$ _is less than_ $y$, written $x<y$, provided $y+-x\in\Re^+$.  We say that $x$ is _less than or equal_ $y$ iff and only iff $x<y$ or $x=y$.  (Also, $x$ is said to be greater than $y$ if $y<x$.)

### Definition: Bounded Above (and Below)

A set $A$ of real numbers is said to be _bounded above_ if there exists some $b\in\Re$ such that $x\leq b,\ \forall x\in A$.  In this case, $b$ is called an _upper bound_ for $A$.  (Bounded below and lower bound are defined similarly.)

### Definition: Least Upper (and Lower) Bound

Let $A$ be a set of real numbers bounded above.  An element $\beta\in\Re$ is called the _least upper bound_, often written _lub_ for $A$ iff and only if

i. $\beta$ is an upper bound for $A$ and
ii. $\beta\leq b$ for every $b$ which is an upper bound for $A$. (A greatest lower bound is defined similarly.)


### Completeness Axiom

Every nonempty subset of $\Re$ having an upper bound has a least upper bound.

## Proof that the Field of Rationals is not Complete

Let $A=\left\{p\in Q^+|p^2<2\right\}$ and $B=\left\{p\in Q^+|p^2>2\right\}$.  

We claim that $A$ has no largest element and that $B$ has no smallest element; i.e., given that $p\in A$, we can find some $q\in A$ with $q>p$; and given any $p\in B$, we can find some $q\in B$ with $q<p$.

For any $p\in Q^+$, let 

$$\begin{align*}
q
  &=p-\frac{p^2-2}{p+2} \\
  &=\frac{p^2+2p-p^2+2}{p+2} \\
  &=\frac{2p+2}{p+2}
\end{align*}$$  

Then 

$$\begin{align*}
q^2-2
  &=\frac{4p^2+8p+r-2p^2-8p-8}{(p+2)^2}
  &=\frac{2p^2-4}{(p+2)^2}
  &=\frac{2(p+2)}{(p+2)^2}
\end{align*}$$
  
Now if $p\in A,\ p^2-2<0$, 

so $-\frac{p^2-2}{p+2}>0$, 

whence $q=p-\frac{p^2-2}{p+2}>p$.  

But $q^2-2=\frac{2(p^2-2)}{(p+2)^2}<0$ 

implies that $q^2<2$.  Thus $q\in A$ and $q>p$.

On the other hand, if $p\in B$, then $p^2-2>0$, 

so $q=p-\frac{p^2-2}{p+2}<p$.  

But $q^2-2=\frac{2(p^2-2)}{(p+2)^2}>0$ implies that $q\in B$.  

Here $q\in B$ and $q<p$.

Now it is clear that every member of $B$ is an upper bound for $A$, and every member of $A$ is a lower bound for set $B$.  It is also clear from the above demonstration that _among the rationals Q,_ the nonempty set $A$ has _no least upper bound_; and the nonempty set $B$ has _no greatest lower bound_ _among the rationals_ .  Therefore we have shown that the ordered field of rational numbers does not satisfy the conditions of the _Completeness Axiom_ .  Thus it is the _Completeness Axiom_ that distinguishes the ordered field of real numbers from the ordered field of rational numbers. 

## Preliminary Results in the Field of Real Numbers

### Theorem: Uniqueness of Identities

Identity elements are unique.

_Proof:_ 
Suppose $u \star a=a\star u=a$ and $e\star a=a\star e=a,\ \forall a\in\Re$.  Then $u=u\star e=e$.


### Theorem 2: Uniqueness of Inverses

If $\star$ is an associative operation, inverse elements for $\star$ are unique.

_Proof:_ 

Suppose $a\star a_1=a_1\star a=e$ and $a\star a_2=a_2\star a=e$, where $e$ is the identity element for the operation $\star$.

Then $a_1=a_1\star e=a_1\star(a\star a_2)=(a_1\star a)\star a_2)=e\star a_2=a_2$.


### Theorem: Left Cancellation Law

If $a$ has an inverse $a^\prime$ with respect to the associative operation $\star$, and $a\star b=a\star c$, then $b=c$.

_Proof:_

Suppose $a\star b=a\star c$.  Then 

$$\begin{align*}
a^\prime\star(a\star b)             &= a^\prime\star(a\star c)\\
\Rightarrow(a^\prime\star a)\star b &= (a^\prime\star a)\star c\\
\Rightarrow e\star b                &= e\star c\\
\Rightarrow b                       &= c
\end{align*}$$


### Corollary: Right Cancellation

In the field $(\Re,+,\cdot),\ a+b=a+c\Rightarrow b=c$, and if $a\neq 0$ and $ab=ac$, then $b=c$.

_Proof:_ 
The Corollary is proved using commutativity and Left Cancellation.

### Lemma

$$-0=0$$.

_Proof:_

By axiom 4 $-0+0=0$.  Because 0 is the additive identity, $-0+0=-0$.  Therefore $-0=-0+0=0$. 


### Theorem

$\forall a\in\Re,\ a>0$ if and only if $a\in\Re^+$.

_Proof:_ 
Suppose $0<a$.  Then $a-0\in\Re^+$, but $a-0=a$, so $a\in\Re^+$.  Conversely, suppose $a\in\Re^+$.  Then $a-0=a\in\Re^+$, so $0<a$. 

### Theorem

$\forall x\in\Re,\ x\cdot 0=0$.

_Proof:_ 

$$\begin{align*}
0 + x\cdot 0    &= x\cdot 0\\
                &= x\cdot(0+0)\\
                &= x\cdot 0 + x\cdot 0\\
  Rightarrow 0  &= x\cdot 0
\end{align*}$$

### Theorem

$\forall x\in\Re,\ -(-x)=x$.

_Proof:_

$-(-x)+(-x) = 0$ and $0 = x+(-x)$.  So

$$\begin{align*}
-(-x)+(-x) &= x+(-x)
  \Rightarrow -(-x) &= x 
\end{align*}$$


### Theorem {#real-numbers-theorem-4-9}

$$\forall x,y\in\Re,\ x\cdot(-y) = -(x\cdot y)=(-x)\cdot y$$

_Proof:_ 

$$\begin{align*}
            x(-y)+xy  &= x(-y+y) 
                      &= x\cdot 0 \\
                      &= 0 \\
                      &= -(xy)+xy\\
\Rightarrow x(-y)+xy  &= -(xy)+xy\\
\Rightarrow x(-y)     &= -(xy)
\end{align*}$$

Similarly, 

$$\begin{align*}
          (-x)y+xy &= (-x+x)y \\
                   &= 0\cdot y \\
                   &= 0 \\
                   &= -(xy)+xy\\
\Rightarrow (-x)y   &= -(xy)
\end{align*}$$

By transitivity, $(-x)y = -(xy) = x(-y)$. 

### Theorem {#real-numbers-theorem-4-10}

$$\forall x\in\Re,\ (-1)\cdot x=-x$$

_Proof:_ 

By Theorem \@ref(real-numbers-theorem-4-9)

$$\begin{align*}
(-1)x &= -(1x)
      &= -x
\end{align*}$$


### Corollary {#real-numbers-theorem-4-11}

$$\forall x\in\Re,\ (-1)\cdot(-x)=x$$

_Proof:_ 

By \@ref(real-numbers-theorem-4-10)
$$(-1)(-x) =  -(-x) = x $$


### Theorem

$$\forall x\in\Re,\ (-x)(-x)=x\cdot x$$

_Proof:_

By Corollary \@ref(real-numbers-theorem-4-11)

$$\begin{align*}
(-x)(-x) 
  &= -(x(-x)) \\
  &= -((-x)x) \\
  &= -(-(x\cdot x)) \\
  &= (x\cdot x) \\
  &= x\cdot x
\end{align*}$$


### Theorem

Let $x$ and $y$ be any real numbers.  Then exactly one of the following is true:

i. $x>y$
ii. $x=y$
iii. $x<y$

_Proof:_

By Axiom 1, $\Re$ is closed under addition.  Thus, since $x,y\in\Re$ , $x+(-y)\in\Re$.   By Trichotomy, $x+(-y)\in\Re,\ -(x+(-y))\in\Re$, or $x+(-y)=0$.

$x+(-y)\in\Re\ \Rightarrow x>y$ (Definition \@ref(real-numbers-definition-less-than-equal-to))

$-(x+(-y))=(-x)+y\in\Re\ \Rightarrow x<y$ (Axiom 4)

$x+(-y)=0\ \Rightarrow x=y$ (Definition \@ref(real-numbers-definition-less-than-equal-to)). 


### Theorem

$$\forall a,b,c\in\Re,\ (a<b \wedge b<c)\Rightarrow a<c$$

_Proof:_ 

$$\begin{align*}
              a<b &\wedge b<c \\ 
\Rightarrow b-a>0 &\wedge c-b>0 \\ 
\Rightarrow^{[1]}(b-a)+(c-b)>0 \\
\Rightarrow (c-b)+(b-a)>0 \\ 
\Rightarrow c-b+b-a>0 \\ 
\Rightarrow c-a>0 \\ 
\Rightarrow a<c 
\end{align*}$$

> 1. Axiom 12

### Theorem

$\forall a,b,c \in\Re,\ (a<b \ \Rightarrow a+c<b+c)$.

_Proof:_

$$\begin{align*}
            a           &< b \\
\Rightarrow b-a         &> 0 \\
\Rightarrow b-c+c-a     &> 0 \\
\Rightarrow b+c-a-c     &> 0 \\
\Rightarrow (b+c)-(a+c) &> 0 \\ 
\Rightarrow a+c         &< b+c\ 
\end{align*}$$


### Theorem

$\forall a,b,c\in\Re,\ (a<b\wedge c>0)\Rightarrow ac<bc$.\\

_Proof:_ 

$$\begin{align*}
a &< b \\
\Rightarrow b-a           &> 0 \\  
^{[1]} \Rightarrow c(b-a) &> 0 \\
	     \Rightarrow bc-ac  &> 0 \\
	     \Rightarrow ac     &< bc
\end{align*}$$

> 1. Axiom 12 states that $\Re^+$ is closed under $\cdot$.


### Theorem
$\forall a,b,c\in\Re,\ (a<b\wedge c<0)\Rightarrow bc<ba$.

_Proof:_

$$\begin{align*}
a                   &< b \\ 
\Rightarrow b-a     &> 0 \\ 
\Rightarrow c(b-a)  &< 0 \\ 
\Rightarrow bc-ba   &< 0 \\ 
\Rightarrow bc      &< ba
\end{align*}$$

> 1. Theorem \ref{RealNumbers4.9}.

### Theorem

If $a>b>0$ and $c>d>0$, then $ac>bd$.

_Proof:_ 

$$\begin{align*}
a>b &\wedge c>d \\ 
\Rightarrow a-b>0 &\wedge c-d>0 \\ 
\Rightarrow c(a-b)>0 &\wedge b(c-d)>0 \\ 
\Rightarrow ac-bc>0 &\wedge bc-bd>0 \\
\Rightarrow ac-bc+bc-bd>0 & \\ 
\Rightarrow ac-bd>0 & \\ 
\Rightarrow ac>bd &
\end{align*}$$


### Theorem

In a field containing at least two elements, $1\in\Re^+$.

_Proof:_

Since in a field of more than one element, $1\neq 0$ (Theorem \ref{RealNumbers4.1}) we may assume that 1 is not 0.  Hence, by trichotomy, either $1\in\Re^+$ or $-1\in\Re^+$.  Suppose that $-1\in\Re^+$.  Then by closure $(-1)(-1)\in\Re^+$, but by Corollary \ref{RealNumbers4.11}, $(-1)(-1)=1$, so we now have both $-1\in\Re^+$ and $1\in\Re^+$, which is a contradiction, and hence it is impossible that $-1\in\Re^+$.  Therefore, $1\in\Re^+$.  


### Theorem

If $x>0$, then $x^{-1}>0$.

_Proof:_ 

Since $x^{-1}$ has an inverse $x$, we know that $x^{-1}\neq 0$.  Hence, by Axiom 13, either $x^{-1}>0$ or $x^{-1}<0$.  Suppose $x^{-1}<0$.  Then $-x^{-1}\in\Re^+$, and since $\Re^+$ is closed under multiplication, $(-x^{-1})\cdot x\in\Re^+$.  Now by Theorem \ref{Theorem 7}, $(-x^{-1})\cdot x=-(x^{-1}\cdot x)=-1$, so this would imply that $-1\in\Re^+$, in contradction to Theorem \ref{RealNumbers4.20}.  Since $x^{-1}<0$ must be be false, we conclude that $x^{-1}>0$ whenever $x>0$. 

<!--chapter:end:RealNumbers.Rmd-->

# Sample Size Estimation

## Solving Group Sample Sizes Using Weights

Let $n$ be the total sample size obtained by adding two groups such that $n = n_1 + n_2$.  Let $w$ represent the proportion of $n$ allocated to $n_1$, referred to as the _weight_ of $n_1$.  Then $n_2 = \frac{n_1 \cdot (1 - w)}{w}$.

Furthermore, $n = n_1 + \frac{n_1 \cdot (1-w)}{w}$. 

_Proof:_

By the assumptions, we know

$$\begin{align*}
n &= n_1 + n_2 \\
  &= w \cdot n + (1 - w) \cdot n
\end{align*}$$

This implies

$$\begin{align*}
n_1 &= w \cdot n\\
n_2 &= (1-w) \cdot n
\end{align*}$$

We observe the following:

$$\begin{align*}
\frac{n_2}{n_1} 
  &= \frac{(1-w) \cdot n}{w \cdot n} \\
  &= \frac{(1-w)}{w} \\
\Rightarrow n_2 &= \frac{n_1 \cdot (1-w)}{w}
\end{align*}$$

This further implies

$$ n = n_1 + \frac{n_1 \cdot (1-w)}{w}$$

Notice now that both $n$ and $n_2$ are defined as functions of $n_1$ and $w$. Thus, we may estimate the sample size required in each of two groups by estimating only $n_1$, provided we know the weight $w$.

### Corollary

For $k \in \mathbb{N}$, let $n$ be the total sample size of $k$ subgroups such that 

$$n = n_1 + n_2 + n_3 + ... + n_k$$

Suppose, further, that there exists a vector of weights $W$ that satisfy the following conditions:

1. For each $w_i \in W$, $0 \leq w_i \leq 1$
2. $\sum\limits_{i=1}^{k} w_i = 1$
3. $n = w_1 \cdot n + w_2 \cdot n + ... + w_k \cdot n = \sum\limits_{i=1}^{k} w_i \cdot n$

Let us assign the values $n_1 = w_1 \cdot n$, $n_2 = w_2 \cdot n$, ..., $n_k = w_k \cdot n$. Then for all $i | i \leq k$, 

$$\begin{align*}
\frac{n_i}{n_1} &= \frac{w_i \cdot n}{w_1 \cdot n} \\
                &= \frac{w_i}{w_1} \\
\Rightarrow n_i &= \frac{n_1 \cdot w_i}{w_1}
\end{align*}$$

Thus, each $n_i$ may be estimated by estimating the value of $n_1$ provided $W$ is a fully specified vector of weights for each of the $k$ groups.

<!--chapter:end:Sample_Size_Estimation.Rmd-->

# Skew-Normal Distribution

## Preliminary Theorems

## Lemma: A Symmetry Theorem

Suppose the pdf of $X$, $f_X$ is symmetric about 0.  Let $w(\cdot)$ be any odd function.  Then the pdf of $Y=w(X)$, $f_Y$, is also symmetric about 0.

_Proof:_

Recall that if a pdf is symmetric about zero, it must demonstrate the property $P(T\leq t)=P(T\geq-t)$.  Since $f_X$ is symmetric, we know

$$\begin{align*}
P(X\leq x)                            &= P(X\geq-x) \\
\Rightarrow P\big[ w(X)\leq w(x)\big] &= P\big[ w(X)\geq w(-x)\big]
	^{[1]}                              &= P\big[ w(X)\geq-w(x)\big] \\
                                      &= P(Y\leq y) \\
                            	        &= P(Y\geq-y)
\end{align*}$$

Thus $f_Y$ is symmetric about 0.

> 1. By the definition of an odd function $f(-x)=-f(x)$.


## Lemma

Let $f_0$ be a one-dimensional probability density function symmetric over 0.

Also, let $G$ be a one dimensional probability distribution function such that $G^\prime$ exists and is a density function symmetric over 0. Then 

\[ 	\begin{array}{rl}
		f(x) = 2 f_0(x) G\big(w(x)\big) & (-\infty<x<\infty)\\
	\end{array}
\]

is a probability density function for any odd fuction $w(\cdot)$.\\

_Proof:_

Let $X\sim f_0$ and $Y\sim G^\prime$.

Now consider the random variable $X-w(Y)|Y$.

When $Y$ is fixed, $X-w(Y)$ is an odd function of $X$ and, by Lemma \ref{SkewNorm1.1}, $X-w(Y)$ is symmetric over 0. Thus,

$$\begin{align*}
\frac{1}{2} 
                      	&= P\big(X-w(Y)\leq0|Y\big) \\
                ^{[1]}	&= E\Big[ P\big(X-w(Y)\leq0|Y\big) \Big] \\
\Rightarrow \frac{1}{2} &= E\big[ P\big( X\leq w(Y)|Y\big) \big]\\
\Rightarrow \frac{1}{2} &= \int\limits_{-\infty}^{\infty} P\big(X \leq w(Y)|Y\big) p(x) dx\\
\Rightarrow \frac{1}{2} &= \int\limits_{-\infty}^{\infty} G\big( w(Y) \big) f_0(x)dx\\
\Rightarrow 1           &= 2 \int\limits_{-\infty}^{\infty} f_0(X) G\big( w(Y) \big) dx
\end{align*}$$

So $f(x) = 2 f_0(x)G\big(w(y)\big)$ is a valid density function for all $x,\ x\in\Re$. 

> 1. The expected value of $P(X\leq 0)=\frac{1}{2}$ when $X$ is distributed symmetric over 0.


## Expected Values
$$\begin{align*}
E(X)
	&= \int\limits_{-\infty}^{\infty} x \cdot 2 f(x) \Phi(\alpha x)dx \\
	&= 2 \int\limits_{-\infty}^{\infty} x f(x) \Phi(\alpha x)dx \\
  &= 2 \int\limits_{-\infty}^{\infty} x \frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{x^2}{2} \bigg\}
		\Bigg[ \int\limits_{-\infty}^{\alpha x} \frac{1}{\sqrt{s\pi}} 
			\exp\bigg\{ -\frac{t^2}{2} \bigg\}dt \Bigg]dx \\
  &= 2\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x \frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{x^2}{2} \bigg\}
		\frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx \\
  &= \frac{2}{2\pi} \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x \exp\bigg\{ -\frac{x^2}{2} \bigg\} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx \\
  &= \frac{1}{\pi} \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x \exp\bigg\{ -\frac{x^2}{2} \bigg\} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx
\end{align*}$$

But $\exp\bigg\{ -\frac{t^2}{2} \bigg\}$ cannot be integrated in closed form, so the solution must be found with numerical methods.

$$\begin{align*}
E(X^2)
	&= \int\limits_{-\infty}^{\infty} x^2 \cdot 2 f(x) \Phi(\alpha x)dx \\
	&= 2 \int\limits_{-\infty}^{\infty} x^2 f(x) \Phi(\alpha x)dx \\
  &= 2 \int\limits_{-\infty}^{\infty} x^2 \frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{x^2}{2} \bigg\}
		\Bigg[ \int\limits_{-\infty}^{\alpha x} \frac{1}{\sqrt{s\pi}} 
			\exp\bigg\{ -\frac{t^2}{2} \bigg\}dt \Bigg]dx \\
  &= 2\int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x^2 \frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{x^2}{2} \bigg\}
		\frac{1}{\sqrt{2\pi}} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx \\
  &= \frac{2}{2\pi} \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x^2 \exp\bigg\{ -\frac{x^2}{2} \bigg\} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx \\
  &= \frac{1}{\pi} \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\alpha x}
		x^2 \exp\bigg\{ -\frac{x^2}{2} \bigg\} \exp\bigg\{ -\frac{t^2}{2} \bigg\} dt dx
\end{align*}$$

But $\exp\bigg\{ -\frac{t^2}{2} \bigg\}$ cannot be integrated in closed form, so the solution must be found with numerical methods.



## Estimation of $\lambda$
Using the Moment Generating Function, it was shown that the skew of the Skew-Normal distribution can be calculated as
\[
S=sign(\lambda) \bigg(2 - \frac{\pi}{2}\bigg) 
		\Bigg(\frac{\lambda^2}
			{\frac{pi}{2}+(\frac{pi}{2}-1)\lambda^2}\Bigg)^{\frac{3}{2}}
\]

where $S$ denotes the skew of the distribution.  Given a value of skew for the distribution, a link can be made back to $\lambda$.  We begin by noticing that the following process is identical regardless of the sign of $\lambda$.  It is presented here as if $\lambda>0$

$$\begin{align*}
S
	&= \bigg( 2-\frac{\pi}{2} \bigg)
		\Bigg( \frac{\lambda^2} {\frac{\pi}{2}+\big(\frac{\pi}{2}-1\big)
			\lambda^2} \Bigg)^{3/2} \\
\Rightarrow \bigg( \frac{S}{2-\frac{\pi}{2}} \bigg)
		&= \Bigg( \frac{\lambda^2} {\frac{\pi}{2}+\big(\frac{\pi}{2}-1\big)
			\lambda^2} \Bigg)^{3/2} \\
\Rightarrow \bigg( \frac{S}{2-\frac{\pi}{2}} \bigg)^{2/3}
		&= \Bigg( \frac{\lambda^2} {\frac{\pi}{2}+\big(\frac{\pi}{2}-1\big)
			\lambda^2} \Bigg) \\
\Rightarrow T 
		&= \Bigg( \frac{\lambda^2} {\frac{\pi}{2}+\big(\frac{\pi}{2}-1\big)
			\lambda^2} \Bigg) \\
\Rightarrow \bigg( \frac{\pi}{2} + \Big(\frac{\pi}{2}-1\Big) \lambda^2 \Bigg)T
		&= \lambda^2 \\
\Rightarrow \frac{\pi}{2}T + \Big( \frac{\pi}{2}-1 \Big) \lambda^2 T
		&= \lambda^2 \\
\Rightarrow \frac{\pi}{2}T
		&= \lambda^2 - \Big( \frac{\pi}{2}-1 \Big) \lambda^2 T \\
\Rightarrow \frac{\pi}{2}T
		&= \lambda^2 \bigg(1 - \Big( \frac{\pi}{2}-1 \Big) T \bigg) \\
\Rightarrow \lambda^2
		&= \frac{ \frac{\pi}{2}T }
			{ 1-\Big( \frac{\pi}{2}-1 \Big) T } \\
\Rightarrow \lambda^2
		&=  \frac{ \frac{\pi}{2} \Big( \frac{S}{2-\frac{\pi}{2}} \Big)^{2/3} }
			{ 1-\Big( \frac{\pi}{2}-1 \Big) \frac{S}{2-\frac{\pi}{2}} \bigg)^{2/3}\\ } \\
\Rightarrow \lambda
		&= \frac{ \sqrt{\frac{\pi}{2}} \Big( \frac{S}{2-\frac{\pi}{2}} \Big)^{1/3} }
			{ \sqrt{ 1-\Big( \frac{\pi}{2}-1 \Big) 
				\frac{S}{2-\frac{\pi}{2}} \bigg)^{2/3}} }
\end{align*}$$
				
> 1. Let $T=\big( \frac{S}{2-\frac{\pi}{2}} \big)^{2/3}$

This equation is only defined for certain values of $S$.  In particular, $S$ cannot be a number such that the denominator is 0, nor can the that which appears under the radical be negative.  These two restrictions can be collapsed, and the equation is defined so long as

$$\begin{align*}
1-\Big( \frac{\pi}{2}-1 \Big) \bigg(\frac{S}{2-\frac{\pi}{2}}\bigg)^{2/3} > 0 \\
\Rightarrow 1 
		> \Big( \frac{\pi}{2}-1 \Big) \bigg(\frac{S}{2-\frac{\pi}{2}}\bigg)^{2/3} \\
\Rightarrow \Big( \frac{\pi}{2}-1 \Big)^{-1}
		> \bigg( \frac{S}{2-\frac{\pi}{2}} \bigg)^{2/3} \\
\Rightarrow \frac{ \Big( 2-\frac{\pi}{2} \Big)^{2/3} }
			{ \frac{\pi}{2}-1 }
		> S^{2/3} \\
\Rightarrow \frac{ 2-\frac{\pi}{2} }
			{ \Big( \frac{\pi}{2}-1 \Big)^{3/2} }
		> S \\
\Rightarrow -.9952 < S < .9952
\end{align*}$$

We notice that the endpoints of this interval are approximations.  Ideally, the interval would span from -1 to 1, as most estimators of skew provide a value in that interval--values close to negative one denoting a strong left skew; values close to one denoting a strong right skew; 0 denoting perfect symmetry.  Although this relationship is not perfect, it is quite close to what we would like, and can be practically implemented.


<!--chapter:end:SkewNormal.Rmd-->

# Somers' D

Somers' $D$ has an asymptotically $Normal$ Distribution \ref{}.  It may take any value between -1 and 1.  It is used to measure classification agreement between a predictor and outcome variable.

Somers' $D$ is related to a form of a concordance index.  Concrodance in measured between 0 and 1 and can effectively be calculated by rescaling Somers' $D$.  The rescaling can be accomplished by:

$$ C = \frac{D+1}{2} $$

## Theorems for Somers' $D$

### Theorem: Distribution of Somers' Derived Concordance

Let $D \sim$ Normal$(\mu, \sigma^2)$.  Then $C \sim $Normal$(\frac{\mu+1}{2},\frac{\sigma^2}{4})$.

_Proof:_

$$
D \sim Normal(\mu,\sigma^2) 
		\Rightarrow (D+1) \sim Normal(\mu+1,\sigma^2) \\
(D+1) \sim Normal(\mu+1,\sigma^2) 
		\Rightarrow \frac{D+1}{2} \sim Normal(\frac{\mu+1}{2}, \frac{\sigma^2}{4})
$$

By definition, $C=\frac{D+1}{2}$, so $C \sim Normal(\frac{\mu+1}{2},\frac{\sigma^2}{4})$. 

Note: when the dependent variable is a binary response, the Concordance Index is equal to the area under the Receiver Operator Characteristic (ROC) curve, or AUC.  




<!--chapter:end:SomersD.Rmd-->

# Summation {#summation-chapter}

## Theorems of Summation

### Theorem {#summation-theorem-1}
If $c$ is a constant then $$\sum\limits_{i=1}^{n}c = nc$$

_Proof:_

$$
\sum\limits_{i=1}^{n}c
	= \underbrace{c+c+\cdots+c}_{n\ \rm terms}
	= nc
$$

### Theorem {#summation-theorem-2}

If $a_1,a_2,\ldots,a_n$ are real numbers and $c$ is a constant, then 
$$
\sum\limits_{i=1}^{n}ca_i
	= c\sum\limits_{i=1}^{n}a_i
$$

_Proof:_

$$\begin{align*} 
\sum\limits_{i=1}^{n}ca_i
	&= ca_1 + ca_2 + \cdots + ca_n \\
	&= c(a_1+a_2+\cdots+a_n) \\
	&= c\sum\limits_{i=1}^{n}a_i
\end{align*}$$


### Theorem {#summation-theorem-3}

If $a_1,_2,\ldots,a_n$ are real numbers and $b_1,b_2,\ldots,b_n$ are real numbers, then
$$
\sum\limits_{i=1}^{n}(a_i+b_i)
	= \sum\limits_{i=1}^{n}a_i + \sum\limits_{i=1}^{n}b_i
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}(a_i+b_i)
	&= a_1 + b_1 + a_2 + b_2 + \cdots + a_n + b_n \\
  &= a_1 + a_2 + \cdots + a_n + b_1 + b_2 + \cdots + b_n \\
	&= \sum\limits_{i=1}^{n}a_i + \sum\limits_{i=1}^{n}b_i
\end{align*}$$


### Theorem {#summation-theorem-4}

If $a_i$ and $b_j$ are real numbers for $i=1,2,\ldots,n$, $j=1,2,\ldots,m$, then
then 

$$
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_i b_j
	= a_{+} b_{+}
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_i b_j
	&= \sum\limits_{i=1}^{n}\bigg(a_i\sum\limits_{j=1}^{m}b_j\bigg) \\
	&= \sum\limits_{i=1}^{n}a_i b_{+} \\
	&= b_{+} \sum\limits_{i=1}^{n}a_i \\
	&= a_{+} b_{+}
\end{align*}$$



### Theorem {#summation-theorem-5}

If $a_i$ is a real number for $i=1,2,\ldots,n$ and $b$ is a real number, then 
$$
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_i b
	= m a_{+} b
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_i b
	&= \sum\limits_{i=1}^{n}m a_i b \\
	&= m b\sum\limits_{i=1}^{n}a_i \\
	&= m a_{+} b
\end{align*}$$



### Theorem {#summation-theorem-6}

If $a_j$ is a real number for $j=1,2,\ldots,m$ and $b$ is a real number, then 
$$
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_j b
	= n a_{+} b
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_j b
	&= \sum\limits_{i=1}^{n}\bigg( b \sum\limits_{j=1}^{m} a_j \bigg) \\
	&= \sum\limits_{i=1}^{n}a_{+}b \\
	&= n a_{+} b
\end{align*}$$


### Theorem {#summation-theorem-7}

If $a_i$ and $b_{ij}$ are real numbers for $i=1,2,\ldots,n$, $j=1,2,\ldots,m$, then 
$$
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_{ij}
	= \sum\limits_{i=1}^{n}a_ib_{i+}
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_ib_{ij}
	&= \sum\limits_{i=1}^{n}\bigg(a_i\sum\limits_{j=1}^{m}b_{ij}\bigg) \\
	&= \sum\limits_{i=1}^{n}a_ib_{i+}
\end{align*}$$


### Theorem {#summation-theorem-8}

If $a_j$ and $b_{ij}$ are real numbers for $i=1,2,\ldots,n$, $j=1,2,\ldots,m$, then 

$$\
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_jb_{ij}
	= \sum\limits_{i=1}^{n}a_jb_{+ j}
$$

_Proof:_

$$\begin{align*}
\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}a_jb_{ij}
  &= a_1b_{11}+a_2b_{12}+\cdots+a_mb_{1m} \\
  & \ \ \ \  +a_1b_{21}+a_2b_{22}+\cdots+a_mb_{2m} \\
  & \ \ \ \ \vdots \\
  & \ \ \ \ +a_1b_{n1}+a_1b_{n1}+\cdots+a_1b_{nm} \\
  &= a_1b_{11}+a_1b_{21}+\cdots+a_1b_{n1} \\
  & \ \ \ \ +a_2b_{12}+a_2b_{22}+\cdots+a_2b_{n2} \\
  & \ \ \ \ \vdots \\
  & \ \ \ \ +a_mb_{1m}+a_mb_{2m}+\cdots+a_nb_{nm} \\
  &= a_1(b_{11}+b_{21}+\cdots+b_{n1}) \\
  & \ \ \ \ +a_2(b_{12}+b_{22}+\cdots+b_{n2}) \\
  & \ \ \ \ \vdots \\
  & \ \ \ \ +a_m(b_{1m}+b_{2m}+\cdots+b_{nm}) \\
  &= a_1b_{+ 1}+a_2b_{+ 2}+\cdots+a_mb_{+ m} \\
  &=\sum\limits_{j=1}^{m}a_jb_{+ j}
\end{align*}$$

<!--chapter:end:Summation.Rmd-->

# T-test

## One-Sample T-test

The t-test is commonly used to look for evidence that the mean of a 
normally distributed random variable may differ from a hypothesized (or 
previously observed) value. 

### T-Statistic

The $t$-statistic is a standardized measure of the magnitude of difference between a sample's mean and some known, non-random constant.  It is similar to a $z$-statistic, but differs in that a $t$-statistic may be calculated without knowledge of the population variance.

### Definitions and Terminology

Let $\bar{x}$ be a sample mean from a sample with standard deviation $s$.  Let $\mu_0$ be a constant, and $s_\bar{x} = s/\sqrt{n}$ be the standard error of the parameter $\bar{x}$.  $t$ is defined:
$$t = \frac{\bar{x} - \mu_0}{s_\bar{x}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}$$

and has $\nu = n-1$ degrees of freedom.

### Hypotheses

The hypotheses for these test take the forms:

For a two-sided test:
$$
  \begin{align*}
  H_0: \mu &= \mu_0\\
  H_a: \mu &\neq \mu_0
  \end{align*}
$$

For a one-sided test:
$$
  \begin{align*}
  H_0: \mu &< \mu_0\\
  H_a: \mu &\geq \mu_0
  \end{align*}
$$

or 
$$
  \begin{align*}
  H_0: \mu &> \mu_0\\
  H_a: \mu &\leq \mu_0
  \end{align*}
$$

To compare a sample $(X_1, \ldots, X_n)$ against the hypothesized value, a 
T-statistic is calculated in the form:

$$T = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$$

Where $\bar{x}$ is the sample mean and $s$ is the sample standard deviation.

### Decision Rule

The decision to reject a null hypothesis is made when an observed T-value lies
in a critical region that suggests the probability of that observation is low.
We define the critical region as the upper bound we are willing to accept for
$\alpha$, the Type I Error.

In the two-sided test, $\alpha$ is shared equally in both tails.  The rejection
regions for the most common values of $\alpha$ are depicted in the figure below, 
with the sum of shaded areas on both sides equaling the corresponding $\alpha$.
It follows, then, that the decision rule is:

Reject $H_0$ when $T \leq t_{\alpha/2, \nu}$ or when $T \geq t_{1-\alpha/2, \nu}$.

By taking advantage of the symmetry of the T-distribution, we can simplify the
decision rule to:

Reject $H_0$ when $|T| \geq t_{1-\alpha/2, \nu}$

```{r, echo=FALSE, fig.height=3, fig.width=6, fig.path = 'figures/', fig.cap="The example displayed uses 25 degrees of freedom"}
RejectRegion <- 
  data.frame(x = seq(-5, 5, by = 0.01)) %>%
  mutate(y = dt(x, df = 24),
         sig = cut(pt(x, df = 24),
                   breaks = c(0, 0.005, 0.025, 0.05, 
                              0.95, 0.975, 0.995, 1),
                   labels = 1:7))
levels(RejectRegion[["sig"]]) = list("0.01" = c(1,7),
                                     "0.05" = c(2,6),
                                     "0.10" = c(3,5))

ggplot(data = RejectRegion,
       mapping = aes(x = x)) + 
  geom_line(mapping = aes(y = y)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x < 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x > 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  scale_fill_manual(values = pallette_green) +  
  xlab("T-value") + 
  ylab("Probability") + 
  labs(fill="alpha")
```

In the one-sided test, $\alpha$ is placed in only one tail.  The rejection
regions for the most common values of $\alpha$ are depicted in the figure below.
In each case, $\alpha$ is the area in the tail of the figure.
It follows, then, that the decision rule for a lower tailed test is:

Reject $H_0$ when $T \leq t_{\alpha, \nu}$.

For an upper tailed test, the decision rule is:

Reject $H_0$ when $T \geq t_{1-\alpha, \nu}$.

Using the symmetry of the T-distribution, we can simplify the 
decision rule as:

Reject $H_0$ when $|T| \geq t_{1-\alpha, \nu}$.

```{r, echo=FALSE, fig.height=3, fig.width=6, fig.path = 'figures/', fig.cap="The example displayed uses 25 degrees of freedom"}
RejectRegion <- 
  expand.grid(x = seq(-5, 5, by = 0.01),
              tail = c("Lower Tailed", "Upper Tailed")) %>%
  mutate(y = dt(x, df = 24),
         sig = cut(pt(x, df = 24),
                   breaks = c(0, 0.005, 0.025, 0.05, 
                              0.95, 0.975, 0.995, 1),
                   labels = 1:7),
         sig = as.numeric(sig),
         sig = ifelse(tail %in% "Lower Tailed" & x > 0,
                      4, sig),
         sig = ifelse(tail %in% "Upper Tailed" & x < 0,
                      4, sig),
         sig = factor(sig))
levels(RejectRegion[["sig"]]) = list("0.01" = c(1,7),
                                     "0.05" = c(2,6),
                                     "0.10" = c(3,5))

ggplot(data = RejectRegion,
       mapping = aes(x = x)) + 
  geom_line(mapping = aes(y = y)) + 
  facet_grid(~ tail) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x < 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x > 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  scale_fill_manual(values = pallette_green) +  
  xlab("T-value") + 
  ylab("Probability") + 
  labs(fill="alpha")
```

The decision rule can also be written in terms of $\bar{x}$:

Reject $H_0$ when $\bar{x} \leq \mu_0 - t_\alpha \cdot s/\sqrt{n}$ or
$\bar{x} \geq \mu_0 + t_\alpha \cdot s/\sqrt{n}$.

This change can be justified by:

$$
\begin{align*}
|T| &\geq t_{1-\alpha, \nu}\\
\Big|\frac{\bar{x} - \mu_0}{s/\sqrt{n}}\Big| &\geq t_{1-\alpha, \nu} 
\end{align*}
$$



$$
\begin{align*}
-\Big(\frac{\bar{x} - \mu_0}{s/\sqrt{n}}\Big) &\geq t_{1-\alpha, \nu} &
    \frac{\bar{x} - \mu_0}{s/\sqrt{n}} &\geq t_{1-\alpha, \nu}\\
\bar{x} - \mu_0 &\leq - t_{1-\alpha, \nu} \cdot s/\sqrt{n} &
    \bar{x} - \mu_0 &\geq t_{1-\alpha, \nu} \cdot s/\sqrt{n}\\
\bar{x} &\leq \mu_0 - t_{1-\alpha, \nu} \cdot s/\sqrt{n} &
    \bar{x} &\geq \mu_0 + t_{1-\alpha, \nu} \cdot s/\sqrt{n} 
\end{align*}
$$

For a two-sided test, both the conditions apply.  The left side condition is used
for a left-tailed test, and the right side condition for a right-tailed test.

### Power

The derivations below make use of the following symbols:

* $\bar{x}$: The sample mean
* $s$: The sample standard deviation
* $n$: The sample size
* $\mu_0$: The value of population mean under the null hypothesis
* $\mu_a$: The value of the population mean under the alternative hypothesis.
* $\alpha$: The significance level
* $\gamma(\mu)$: The power of the test for the parameter $\mu$.
* $t_{\alpha, \nu}$: A quantile of the central t-distribution for a probability,
    $\alpha$ and $n-1$ degrees of freedom.
* $T$: A calculated value to be compared against a t-distribution.
* $C$: The critical region (rejection region) of the test.


**Two-Sided Test**

$$
\begin{align*}
\gamma(\mu_a) &= P_{\mu_a}(\bar{x} \in C)\\
&= P_\mu\big(\bar{x} \leq \mu_0 - t_{\alpha/2, \nu} \cdot s/\sqrt{n}\big) +
               P_{\mu_a}\big(\bar{x} \geq \mu_0 + t_{1-\alpha/2, \nu} \cdot s/\sqrt{n}\big)\\
&= P_{\mu_a}\big(\bar{x} - \mu_a \leq \mu_0 - \mu_a - t_{\alpha/2, \nu} \cdot s/\sqrt{n}\big) +
   P_{\mu_a}\big(\bar{x} - \mu_a \geq \mu_0 - \mu_a + t_{1-\alpha/2, \nu} \cdot s/\sqrt{n}\big)\\
&= P_{\mu_a}\Big(\frac{\bar{x} - \mu}{s/\sqrt{n}} \leq 
        \frac{\mu_0 - \mu_a - t_{\alpha/2, \nu} \cdot s/\sqrt{n}}{s/\sqrt{n}}\Big) +
   P_{\mu_a}\Big(\frac{\bar{x} - \mu}{s/\sqrt{n}} \geq 
        \frac{\mu_0 - \mu_a + t_{1-\alpha/2, \nu} \cdot s/\sqrt{n}}{s/\sqrt{n}}\Big)\\
&= P_{\mu_a}\Big(T \leq \frac{\mu_0 - \mu_a}{s/\sqrt{n}} - t_{\alpha/2, \nu}\Big) + 
   P_{\mu_a}\Big(T \geq \frac{\mu_0 - \mu_a}{s/\sqrt{n}} + t_{1-\alpha/2, \nu}\Big)\\
&= P_{\mu_a}\Big(T \leq -t_{\alpha/2, \nu} + \frac{\mu_0 - \mu_a}{s/\sqrt{n}}\Big) + 
   P_{\mu_a}\Big(T \geq t_{1-\alpha/2, \nu} + \frac{\mu_0 - \mu_a}{s/\sqrt{n}}\Big)\\
&= P_{\mu_a}\Big(T \leq -t_{\alpha/2, \nu} + \frac{\sqrt{n} \cdot (\mu_0 - \mu_a)}{s}\Big) + 
   P_{\mu_a}\Big(T \geq t_{1-\alpha/2, \nu} + \frac{\sqrt{n} \cdot (\mu_0 - \mu_a)}{s}\Big)
\end{align*}
$$

Both $t_{\alpha/2, \nu}$ and $t_{1-\alpha/2, \nu}$ have non-central T-distributions
with non-centrality parameter $\frac{\sqrt{n} (\mu_0 -\mu_a)}{s}$.
   

**One-Sided Test**

For convenience, the power for only the upper tailed test is derived here.  
Recall that the symmetry of the t-test allows us to use the decision rule:
Reject $H_0$ when $|T| \geq t_{1-\alpha}$.  Thus, where $T$ occurs in the 
derivation below, it may reasonably be replaced with $|T|$.

$$
\begin{align*}
\gamma(\mu_a)  &= P_{\mu_a}(\bar{x} \in C)\\
&= P_{\mu_a}\big(\bar{x} \geq \mu_0 + t_{1-\alpha, \nu} \cdot s / \sqrt{n}\big)\\
&= P_{\mu_a}\big(\bar{x} - \mu_a \geq \mu_0 - \mu_a + t_{1-\alpha, \nu} \cdot s / \sqrt{n}\big)\\
&= P_{\mu_a}\Big(\frac{\bar{x} - \mu_a}{s/\sqrt{n}} \geq 
    \frac{\mu_0 - \mu_a + t_{1-\alpha, \nu} \cdot s / \sqrt{n}}{s / \sqrt{n}}\Big)\\
&= P_{\mu_a}\Big(T \geq \frac{\mu_0 - \mu_a}{s/\sqrt{n}} + t_{1-\alpha, \nu} \Big)\\
&= P_{\mu_a}\Big(T \geq t_{1-\alpha, \nu} + \frac{\mu_0 - \mu_a}{s/\sqrt{n}}\Big)\\
&= P_{\mu_a}\Big(T \geq t_{1-\alpha, \nu} + \frac{\sqrt{n} \cdot (\mu_0 -\mu_a)}{s}\Big)
\end{align*}
$$

Where $t_{1-\alpha, \nu} + \frac{\sqrt{n} (\mu_0 -\mu_a)}{s}$ has a non-central 
t-distribution with non-centrality parameter $\frac{\sqrt{n} (\mu_0 -\mu_a)}{s}$

### Confidence Interval

The confidence interval for $\theta$ is written:
$$\bar{x} \pm t_{1-\alpha/2} \cdot \frac{s}{\sqrt{n}}$$

The value of the expression on the right is often referred to as the _margin of error_, and we will refer to this value as 
$$E = t_{1-\alpha/2} \cdot \frac{s}{\sqrt{n}}$$

## Two-Sample T-test

The two sample t-test is commonly used to look for evidence that the mean of one normally distributed random variable may differ from that of another normally distributed random variable.  The hypotheses for this test take the forms:

### T-Statistic

The $t$-statistic is a standardize measure of the magnitude of difference between two sample means and some known, non-random difference of population means. It is similar to a two sample $z$-statistic, but differes in that a $t$-statistic may be calculated without knowledge of the population variances.

### Definitions and Terminology

Let $\bar{x_1}$ and $\bar{x_2}$ be sample means from two independent samples with standard deviations $s_1$ and $s_2$.  Let $\mu_1$ and $\mu_2$ be constants representing the means of the populations from which $\bar{x_1}$ and $\bar{x_2}$ obtained. $t$ is defined:

$$ t = \frac{(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2)}{SE^*}$$

Where 

\[ SE^* = 
  \left\{ 
    \begin{array}{rl}
      \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}, & \sigma_1^2 \neq \sigma_2^2 \\
      \sqrt{\frac{(n_1-1) \cdot s_1^2 + (n_2-1) \cdot s_2^2}
                 {n_1 + n_2 - 2}} \cdot
            \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}, & \sigma_1^2 = \sigma_2^2 
    \end{array}
  \right.
\]

and the degrees of freedom $\nu$ are (by the Welch-Satterthwaite equation)

\[ \nu = 
  \left\{
    \begin{array}{rl}
      \frac{(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2})^2}
          {\frac{s_1^2/n_1}{n_1 - 1} + 
            \frac{s_2^2/n_2}{n_2 - 1}}, & \sigma_1^2 \neq \sigma_2^2 \\
      n_1 + n_2 - 2, & \sigma_1^2 = \sigma_2^2 
    \end{array}
  \right.
\]


### Hypotheses

For a two-sided test:

$$H_0 : \mu_1 = \mu_2 \\
  H_a : \mu_1 \neq \mu_2 $$
  
For a one-sided test:

$$ H_0 : \mu_1 \leq \mu_2 \\
   H_a : \mu_1 > \mu_2 $$
   
or

$$ H_0 : \mu_1 \geq \mu_1 \\
   H_a : \mu_1 < \mu_1 $$
   
### Decision Rule

The decision to reject a null hypothesis is made when an observed T-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for $\alpha$, the Type I Error.

#### Two Sided Test

In the two-sided test, $\alpha$ is shared equally in both tails. The rejection regions for the most common values of $\alpha$ are depicted in the figure below, with the sum of the shaded areas on both sides equally the corresponding $\alpha$. It follows then that the decision rule is:

Reject $H_0$ when $T \leq t_{\alpha/2, \nu}$ or when $T \geq t_{1 - \alpha/2, \nu}$.

By taking advantage of the symmetry of the T-distribution, we can simplify the decision rule to:

Reject $H_0$ when $|T| \geq t_{1-\alpha/2, \nu}$

```{r, echo=FALSE, fig.height=3, fig.width=6, fig.path = 'figures/', fig.cap="The example displayed uses 25 degrees of freedom"}
RejectRegion <- 
  data.frame(x = seq(-5, 5, by = 0.01)) %>%
  mutate(y = dt(x, df = 24),
         sig = cut(pt(x, df = 24),
                   breaks = c(0, 0.005, 0.025, 0.05, 
                              0.95, 0.975, 0.995, 1),
                   labels = 1:7))
levels(RejectRegion[["sig"]]) = list("0.01" = c(1,7),
                                     "0.05" = c(2,6),
                                     "0.10" = c(3,5))

ggplot(data = RejectRegion,
       mapping = aes(x = x)) + 
  geom_line(mapping = aes(y = y)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x < 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x > 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  scale_fill_manual(values = pallette_green) +  
  xlab("T-value") + 
  ylab("Probability") + 
  labs(fill="alpha")
```

## One Sided Test

In the one sided test, $\alpha$ is placed in only one tail. The rejection regions for the most common values of $\alpha$ are depicted in the figure below. In each case, $\alpha$ is the area in the tail of the figure. It follow, then, that the decision rule for a lower tailed test is:

Reject $H_0$ when $T \leq t_{\alpha, \nu}$.

For an upper tailed test, the decision rule is:

Reject $H_0$ when $T \geq t_{1-\alpha, \nu}$.

Using the symmetry of the $T$-distribution, we can simplify the decision rule as:

Reject $H_0$ when $|T| \geq t_{1-\alpha, \nu}$.

```{r, echo=FALSE, fig.height=3, fig.width=6, fig.path = 'figures/', fig.cap="The example displayed uses 25 degrees of freedom"}
RejectRegion <- 
  expand.grid(x = seq(-5, 5, by = 0.01),
              tail = c("Lower Tailed", "Upper Tailed")) %>%
  mutate(y = dt(x, df = 24),
         sig = cut(pt(x, df = 24),
                   breaks = c(0, 0.005, 0.025, 0.05, 
                              0.95, 0.975, 0.995, 1),
                   labels = 1:7),
         sig = as.numeric(sig),
         sig = ifelse(tail %in% "Lower Tailed" & x > 0,
                      4, sig),
         sig = ifelse(tail %in% "Upper Tailed" & x < 0,
                      4, sig),
         sig = factor(sig))
levels(RejectRegion[["sig"]]) = list("0.01" = c(1,7),
                                     "0.05" = c(2,6),
                                     "0.10" = c(3,5))

ggplot(data = RejectRegion,
       mapping = aes(x = x)) + 
  geom_line(mapping = aes(y = y)) + 
  facet_grid(~ tail) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x < 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  geom_area(data = filter(RejectRegion, !is.na(sig) & x > 0),
            mapping = aes(y = y,
                          ymax = y,
                          fill = sig)) + 
  scale_fill_manual(values = pallette_green) +  
  xlab("T-value") + 
  ylab("Probability") + 
  labs(fill="alpha")
```

The decision rule can also be written in terms of $\bar{x_1}$ and $\bar{x_2}$.

Reject $H_0$ when $\bar{x_1} - \bar{x_2} \leq (\mu_1 - \mu_2) - t_{\alpha, \nu} \cdot SE^*$ or $\bar{x_1} - \bar{x_2} \geq (\mu_1 - \mu_2) + t_{\alpha, \nu} \cdot SE^*$

This change can be justified by:

$$\begin{aligned}
|T| & \geq t_{1 - \alpha, \nu} \\
\Big| \frac{(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2)}{SE^*} \Big |
    & \geq t_{1 - \alpha, \nu} 
\end{aligned}$$
    
    
    
$$ \begin{aligned}
-\Big(\frac{(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2)}{SE^*}\Big) & \geq t_{1-\alpha, \nu}
    & \Big(\frac{(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2)}{SE^*}\Big) & \geq t_{1-\alpha, \nu} \\
(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2) & \leq -t_{1 - \alpha, \nu} \cdot SE^* 
    & (\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2) &\geq t_{1 - \alpha, \nu} \cdot SE^* \\
\bar{x_1} - \bar{x_2} &\leq (\mu_1 - \mu_2) - t_{1-\alpha, \nu} \cdot SE^* 
    & \bar{x_1} - \bar{x_2} &\leq (\mu_1 - \mu_2) + t_{1-\alpha, \nu} \cdot SE^* 
\end{aligned}$$

### Power

**Two Sided Test**

$$
\begin{align*}
\gamma(\mu_{1a} - \mu_{2a}) 
  &= P_{\mu_{1a} - \mu_{2a}}(\bar{x} \in C)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2}) \leq 
      (\mu_1 - \mu_2) - t_{\alpha/2, \nu} \cdot SE^*\big) + \\
  &\ \ \ \ P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2} \geq 
      (\mu_1 - \mu_2) + t_{1-\alpha/2, \nu} \cdot SE^*\big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2}) - (\mu_{1a} - \mu_{2a}) \leq 
      (\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) - t_{\alpha/2, \nu} \cdot SE^*\big) + \\
  &\ \ \ \ P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2}) - (\mu_{1a} - \mu_{2a}) \geq 
      (\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) + t_{1-\alpha/2, \nu} \cdot SE^*\big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\Big(\frac{(\bar{x_1} - \bar{x_2}) - (\mu_1 - \mu_2)}{SE^*} \leq 
      \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) - t_{\alpha/2, \nu} \cdot SE^*}{SE^*}\Big) + \\
  &\ \ \ \ P_{\mu_{1a} - \mu_{2a}}\Big(\frac{(\bar{x_1} - \bar{x_2}) - (\mu_{1a} - \mu_{2a})}{SE^*} \geq
      \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) + t_{1-\alpha/2, \nu} \cdot SE^*}{SE^*}\Big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\Big(T \leq \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*} - t_{\alpha/2, \nu}\Big) + \\
  &\ \ \ \ P_{\mu_{1a} - \mu_{2a}}\Big(T \geq \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*} + t_{1-\alpha/2, \nu}\Big)\\
  &= P_{\mu_{1a} - \mu_{2a}}\Big(T \leq -t_{\alpha/2, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}\Big) + \\
  &\ \ \ \ P_{\mu_{1a} - \mu_{2a}}\Big(T \geq t_{1-\alpha/2, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}\Big)
\end{align*}
$$

Both $-t_{\alpha/2, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}$ and $t_{1-\alpha/2, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}$ have non-central T-distributions with non-centrality parameter $\frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}$.


**One Sided Test**

For convenience, the power for only the upper tailed test is derived here.  
Recall that the symmetry of the t-test allows us to use the decision rule:
Reject $H_0$ when $|T| \geq t_{1-\alpha}$.  Thus, where $T$ occurs in the 
derivation below, it may reasonably be replaced with $|T|$.

$$
\begin{align*}
\gamma(\mu_{1a} - \mu_{2a}) 
  &= P_{\mu_{1a} - \mu_{2a}}(\bar{x} \in C)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2} \geq 
      (\mu_1 - \mu_2) + t_{1-\alpha, \nu} \cdot SE^*\big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\big((\bar{x_1} - \bar{x_2}) - (\mu_{1a} - \mu_{2a}) \geq 
      (\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) + t_{1-\alpha, \nu} \cdot SE^*\big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\Big(\frac{(\bar{x_1} - \bar{x_2}) - (\mu_{1a} - \mu_{2a})}{SE^*} \geq
      \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a}) + t_{1-\alpha, \nu} \cdot SE^*}{SE^*}\Big)\\
%%%
  &= P_{\mu_{1a} - \mu_{2a}}\Big(T \geq \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*} + t_{1-\alpha, \nu}\Big)\\
  &= P_{\mu_{1a} - \mu_{2a}}\Big(T \geq t_{1-\alpha, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}\Big)
\end{align*}
$$

$t_{1-\alpha/2, \nu} + \frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}$ has a non-central T-distribution with non-centrality parameter $\frac{(\mu_1 - \mu_2) - (\mu_{1a} - \mu_{2a})}{SE^*}$.

### Confidence Interval

The confidence interval for $\mu_1 - \mu_2$ is written:
$$(\bar{x_1} - \bar{x_2}) \pm t_{1-\alpha/2} \cdot SE^*$$

The value of the expression on the right is often referred to as the _margin of error_, and we will refer to this value as 
$$E = t_{1-\alpha/2} \cdot SE^*$$

## References

1. Wackerly, Mendenhall, Scheaffer, _Mathematical Statistics with Applications_, 6th ed., Duxbury, 2002, ISBN 0-534-37741-6.
2. Daniel, _Biostatistics_, 8th ed., John Wiley & Sons, Inc., 2005, ISBN: 0-471-45654-3.
3. Hogg, McKean, Craig, _Introduction to Mathematical Statistics_, 6th ed., Pearson, 2005, ISBN: 0-13-008507-3
4. Wikipedia, "Student's T test", https://en.wikipedia.org/wiki/Student%27s_t-test
5. Wikipedia, "Welch-Satterthwaite Equation", https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation

<!--chapter:end:T_test.Rmd-->

# Method of Transformations

Suppose we wish to find the distribution funciton for the random variable $Y$ that is either a strictly increasing or strictly decreasing function (Such a function is sure to have an inverse, whereas a function like $Y=X^2$ does not have an inverse)..  If we know the distribution of $X$, we may use the following to determine the cdf of $Y$.

$$\begin{align*} 
P(Y\leq y)
	&= P(h(X)\leq y) \\
	&= P(h^{-1}(h(y))\leq h^{-1}(x)) \\
	&= P(Y\leq h^{-1}(x)) \\
\Rightarrow F_Y(y) &= F_X(h^{-1}(x))
\end{align*}$$

The pdf can now be found by taking the deriviative of the cdf.

$$\begin{align*}
f_Y(y)
	&= \frac{d(F_Y(y))}{d y} \\
	&= \frac{d F_Y(h^{-1}(y))}{d y} \\
	&= f_X(h^{-1}(y))\frac{d(h^{-1}(y))}{dy}
\end{align*}$$


## Example: Cauchy Distribution

Let $X$ have the Uniform pdf

\[f(x)=\left\{
	\begin{array}{ll}
		\frac{1}{\pi}, & \frac{-\pi}{2}<x<\frac{\pi}{2}\\
		0 & otherwise  
	\end{array}\right. 
\]

Let $Y = \tan (X)$.  The pdf of $Y$ can be found as follows:

$$\begin{align*} 
h(x)                                &= \tan (x) \\
\Rightarrow h^{-1}(x)               &= \tan^{-1}(x) \\
\Rightarrow \frac{d h^{-1}(x)}{d x} &= \frac{1}{1+x^2} \\
\Rightarrow f_Y(y)                  &= f_X(h^{-1}(y))\frac{d(h^{-1}(y))}{d y} \\
                                  	&= f_X(tan^{-1}(x))\frac{1}{1+y^2} \\
                                  	&= \frac{1}{\pi}\frac{1}{1+y^2} \\
                                  	&= \frac{1}{\pi(1+y^2)}
\end{align*}$$

The domain of $Y$ is transformed

$$
\frac{-\pi}{2}                            < x        < \frac{-\pi}{2} \\
\Rightarrow \tan\big(\frac{-\pi}{2}\big)  < \tan(x)  < \tan\big(\frac{-\pi}{2}\big) \\
\Rightarrow -\infty                       < y        < \infty
$$

Thus the pdf of the $Y$, known as the Cauchy distribution, is

\[f_Y(y) = \frac{1}{\pi(1+y^2)},\ \ -\infty<y<\infty \]
	

<!--chapter:end:Transformations.Rmd-->

# Uniform Distribution

## Probability Density Function
A random variable $X$ is said to have a Uniform Distribution with parameters $a$ and $b$ if its pdf is

\[f(x)=\left\{
	\begin{array}{ll}
		\frac{1}{b-a}, & a\leq x \leq b\\
		0 & elsewhere 
	\end{array} \right. 
\]

## Cumulative Density Function

$$\begin{align*}
F(x)
	&= \int\limits_{a}^{x}\frac{1}{b-a}dt \\
	&= \frac{t}{b-a}|_{a}^{x} \\
	&= \frac{x}{b-a}-\frac{a}{b-a} \\
	&= \frac{x-a}{b-a}
\end{align*}$$

\[F(x)=\left\{
	\begin{array}{lll}
		0 & x<a\\
		\frac{x-a}{b-a},& a\leq x\leq b\\
		1 & elsewhere 
	\end{array}\right. 
\]

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.path = 'figures/', fig.cap = 'The figures on the left and right display the Uniform probability and cumulative distirubtion functions, respectively, for $a=0, b=5$.'}
Uniform <- 
  data.frame(x = seq(0, 5, by = .01)) %>%
  mutate(pmf = dunif(x, min = 0, max = 5),
         cmf = punif(x, min = 0, max = 5)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             c("pmf", "cmf"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Uniform,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob), 
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:5) + 
  ylab("P(x)") + 
  theme_bw()
```


## Expected Values

$$\begin{align*} 
E(X)
	&= \int\limits_{a}^{b}x\frac{1}{b-a}dx \\
	&= \frac{1}{b-a}\int\limits_{a}^{b}x\ dx \\
	&= \frac{1}{b-a}\cdot \Big[\frac{x^2}{2}\Big]_a^b \\
  &= \frac{1}{b-a}\cdot\Big[\frac{b^2}{2}-\frac{a^2}{2}\Big] \\
	&= \frac{1}{b-a}\cdot \frac{b^2-a^2}{2} \\
	&= \frac{b^2-a^2}{2(b-a)} \\
	&= \frac{(b-a)(b+a)}{2(b-a)} \\
	&= \frac{b+a}{2} \\
\\
\\ 
E(X^2)
	&= \int\limits_{a}^{b}x^2\frac{1}{b-a}dx \\
	&= \frac{1}{b-a}\int\limits_{a}^{b}x^2\ \frac{1}{b-a}dx \\
	&= \frac{1}{b-a}\Big[\frac{x^3}{3}\Big]_a^b \\
	&= \frac{1}{b-a}\Big[\frac{b^3-a^3}{3}\Big] \\
  &= \frac{1}{b-a}\Big[\frac{(b-a)(b^2+ab+a^2)}{3}\Big] \\
	&= \frac{(b-a)(b^2+ab+a^2)}{3(b-a)} \\
	&= \frac{(b^2+ab+a^2)}{3} \\
\\
\\
\mu
	&= E(X) \\
	&= \frac{b+a}{1} \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \frac{b^2+ab+a^2}{3} - \frac{(b-a)^2}{4} \\
	&= \frac{4(b^2+ab+a^2)-3(b+a)^2}{12} \\
  &= \frac{4(b^2+ab+a^2-3(b^2+2ab+a^2)}{12} \\
	&= \frac{4b^2+4ab+4a^2-3b^2-6ab-3a^2)}{12} \\
  &= \frac{4b^2-3b^2+4ab-6ab+4a^2-3a^2}{12} \\
	&= \frac{b^2-2a+a^2}{12}=\frac{(b-a)^2}{12} 
\end{align*}$$


## Moment Generating Function

$$\begin{align*} 
M_X(t) 
	&= E(e^{tX})=\int\limits_{a}^{b}e^{tx}\frac{1}{b-a}dx \\
	&= \frac{1}{b-a}\int\limits_{a}^{b}e^{tx}dx \\
  &= \frac{1}{b-a}\Big[\frac{e^{tb}-e^{ta}}{t}\Big] \\
	&= \frac{e^{t(b-a)}}{t(b-a)}
\end{align*}$$

$M_X^{(k)}(0)$ will lead to an undefined operation (division by 0).  Thus, in the case of the Uniform distribution, we are unable to use the method of moments to identify parameter values.


## Theorems for the Uniform Distribution

## Validity of the Distribution

$$\int\limits_{a}^{b}\frac{1}{b-a} = 1$$

_Proof:_

$$\begin{align*} 
\int\limits_{a}^{b}\frac{1}{b-a}
	&= \frac{x}{b-a}\Big|_a^b \\
	&= \frac{b}{b-a}-\frac{a}{b-a} \\
	&= \frac{b-a}{b-a} \\
	&= 1
\end{align*}$$

<!--chapter:end:Uniform.Rmd-->

# Variance Parameter 

## Defining Variance With Expected Values

In the case of a discrete random variable, the variance is
$$\begin{align*} 
\sigma^2
	&= \sum\limits_{x=0}^{\infty}(x-\mu)^2p(x) \\
	&= \sum\limits_{x=0}^{\infty}(x^2-2\mu x+\mu^2)p(x) \\
	&= \sum\limits_{x=0}^{\infty}(x^2p(x)-2\mu x\cdot p(x)+\mu^2p(x)) \\
  &= \sum\limits_{x=0}^{\infty}x^2p(x)-\sum\limits_{x=0}^{\infty}2\mu x\cdot p(x)
		+ \sum\limits_{x=0}^{\infty}\mu^2p(x) \\
  &= \sum\limits_{x=0}^{\infty}x^2p(x)-2\mu\sum\limits_{x=0}^{\infty}x\cdot p(x)
		+ \mu^2\sum\limits_{x=0}^{\infty}p(x) \\
  &= \sum\limits_{x=0}^{\infty}x^2p(x)-2\mu\cdot\mu+\mu^2 \\
	&= \sum\limits_{x=0}^{\infty}x^2p(x)-\mu^2 \\
	&= E(X^2)-E(X)^2\\
\end{align*}$$

In the case of a continuous random variable, the variance is
$$\begin{align*}
\sigma^2 
	&= \int\limits_{-\infty}^{\infty}(x-\mu)^2f(x)dx \\
	&= \int\limits_{-\infty}^{\infty}(x^2-2\mu x+\mu^2)f(x)dx \\
  &= \int\limits_{-\infty}^{\infty}(x^2f(x)-2\mu x\cdot f(x)+\mu^2f(x))dx \\
  &= \int\limits_{-\infty}^{\infty}x^2f(x)dx-\int\limits_{-\infty}^{\infty}2\mu x\cdot f(x)dx
		+ \int\limits_{-\infty}^{\infty}\mu^2f(x)dx \\
  &= \int\limits_{-\infty}^{\infty}x^2f(x)dx-2\mu\int\limits_{-\infty}^{\infty}x\cdot f(x)dx
		+ \mu^2\int\limits_{-\infty}^{\infty}f(x)dx \\
  &= \int\limits_{-\infty}^{\infty}x^2f(x)dx-2\mu\cdot\mu+\mu^2 \\
	&= \int\limits_{-\infty}^{\infty}x^2f(x)dx-\mu^2 \\
	&= E(X^2)-E(X)^2
\end{align*}$$

In general, these results may be summarized as follows:

$$\begin{align*}
\sigma^2
	&= E[(X-\mu)^2] \\
	&= E[(X^2-2\mu X+\mu^2)] \\
	&= E(X^2) - E(2\mu X) + E(\mu^2) \\
	&= E(X^2) - 2\mu E(X) + \mu^2 \\
  &= E(X^2) - 2\mu\cdot\mu + \mu^2 \\
	&= E(X^2) - 2\mu^2 + \mu \\
  &= E(X^2) - \mu^2 \\
	&= E(X^2) - E(X)^2
\end{align*}$$



## Unbiased Estimator

$$\begin{align*}
E\Bigg(\frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n}\Bigg)
      	&= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}(x_i-\bar x)^2\Big) \\
      	&= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}(x_i^2-2\bar x x_i+\bar x^2)\Big) \\
        &= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}x_i^2
      		- \sum\limits_{i=1}^{n}2\bar x x_i+\sum\limits_{i=1}^{n}\bar x^2\Big) \\
      	&= \frac{1}{n}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2-2
      	          \frac{\sum\limits_{i=1}^{n}x_i}{n}\sum\limits_{i=1}^{n}	+ n\bar x^2\Big) \\
      	&= \frac{1}{n}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2-2
      	          \frac{\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n}+n\bar x^2\Big) \\
      	&= \frac{1}{n}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2-2
      	          \frac{n(\sum\limits_{i=1}^{n}x_i)^2}{n^2}+n\bar x^2\Big) \\
        &= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}x_i^2-2n\bar x^2+n\bar x^2\Big) \\
      	&= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}x_i^2-n\bar x^2\Big) \\
      	&= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}x_i^2\Big)-E(n\bar x^2) \\
        &= \frac{1}{n}E\Big(\sum\limits_{i=1}^{n}x_i^2)-nE(\bar x^2)\Big) \\
      	&= \frac{1}{n}\Big[\sum\limits_{i=1}^{n}E(x_i^2)-nE(\bar x^2)\Big] \\
^{[1]}  &= \frac{1}{n}\Big[\sum\limits_{i=1}^{n}\Big(\sigma^2+\mu^2\Big) - nE(\bar x^2)\Big] \\
^{[2]}  &= \frac{1}{n}\Big[\sum\limits_{i=1}^{n}\Big(\sigma^2+\mu^2\Big) - 
            n(\frac{\sigma^2}{n}+\mu^2)\Big]\\\\
        &= \frac{1}{n}(n\sigma^2-n\mu^2+\sigma^2-n\mu^2) \\
        &=\frac{1}{n}(n\sigma^2-\sigma) \\
        &= \frac{1}{n}(n-1)\sigma^2 \\
        &= \frac{n-1}{n}\sigma^2
\end{align*}$$

> 1. $V(X)=E(X^2)-E(X)^2$  
>    $\ \ \ \ \Rightarrow E(X^2)=V(X)+E(X)^2=\sigma^2+\mu^2$
>    $V(\bar X)=E(\bar X^2)-E(\bar X)^2$  
>    $\ \ \ \ \Rightarrow E(\bar X^2)=V(\bar X)+E(\bar X)^2 = \frac{\sigma^2}{n}+\mu^2$
> 2. By the Central Limit Theorem, $V(\bar X)=\frac{\sigma^2}{n}$

Since $E\Bigg(\frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n}\Bigg)\neq\sigma^2$ it is a biased estimator.  Notice, however, that the bias can be eliminated by dividing by $n-1$ instead of by $n$

$$\begin{align*} 
E\Bigg(\frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n-1}\Bigg)
      	&= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}(x_i-\bar x)^2\Big) \\
      	&= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}(x_i^2-2\bar x x_i+\bar x^2)\Big) \\
        &= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}x_i^2
      		- \sum\limits_{i=1}^{n}2\bar x x_i+\sum\limits_{i=1}^{n}\bar x^2\Big) \\
      	&= \frac{1}{n-1}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2 -
      	            2\frac{\sum\limits_{i=1}^{n}x_i}{n}\sum\limits_{i=1}^{n} + n\bar x^2\Big) \\
      	&= \frac{1}{n-1}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2-
      	      2\frac{(\sum\limits_{i=1}^{n}x_i)^2}{n}+n\bar x^2\Big) \\
      	&= \frac{1}{n-1}
      	    E\Big(\sum\limits_{i=1}^{n}x_i^2-
      	      2\frac{n\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n^2} + n\bar x^2\Big) \\
        &= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}x_i^2-2n\bar x^2+n\bar x^2\Big) \\
      	&= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}x_i^2-n\bar x^2\Big) \\
        &= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}x_i^2\Big)-E(n\bar x^2) \\
        &= \frac{1}{n-1}E\Big(\sum\limits_{i=1}^{n}x_i^2\Big)-nE(\bar x^2) \\
      	&= \frac{1}{n-1}\Big[\sum\limits_{i=1}^{n}E(x_i^2)-nE(\bar x^2)\Big] \\
^{[1]}  &= \frac{1}{n-1}\Big[\sum\limits_{i=1}^{n}(\sigma^2+\mu^2)-nE(\bar x^2)\Big] \\
^{[2]}	&= \frac{1}{n-1}\Big[\sum\limits_{i=1}^{n}(\sigma^2+\mu^2) -
                n(\frac{\sigma^2}{n}+\mu^2)\Big] \\
        &= \frac{1}{n-1}(n\sigma^2-n\mu^2+\sigma^2-n\mu^2) \\
        &= \frac{1}{n}(n\sigma^2-\sigma) \\
        &= \frac{1}{n-1}(n-1)\sigma^2 \\
        &= \frac{n-1}{n-1}\sigma^2 \\
        &=\sigma^2
\end{align*}$$

> 1. $V(X)=E(X^2)-E(X)^2$  
>    $\ \ \ \ \Rightarrow E(X^2)=V(X)+E(X)^2=\sigma^2+\mu^2$
>    $V(\bar X)=E(\bar X^2)-E(\bar X)^2$  
>    $\ \ \ \ \Rightarrow E(\bar X^2)=V(\bar X)+E(\bar X)^2 = \frac{\sigma^2}{n}+\mu^2$
> 2. By the Central Limit Theorem, $V(\bar X)=\frac{\sigma^2}{n}$

Thus $E\Bigg(\frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n-1}\Bigg)$ is an unbiased estimator of $\sigma^2$, and we define the estimator 

$$s^2= \frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n-1}$$

## Computational Formulae

### Computational Formula for $\sigma$^2 {#computational-formula-population-variance}

$$\begin{align*} 
\sigma^2
	&= \frac{\sum\limits_{i=1}^{N}(x_i-\mu)^2}{N} \\
	&= \frac{\sum\limits_{i=1}^{N}x_i^2-\frac{\Big(\sum\limits_{i=1}^{N}x_i\Big)^2}{N}}{N}
\end{align*}$$

_Proof:_

$$\begin{align*} 
\frac{\sum\limits_{i=1}^{N}(x_i-\mu)^2}{N}
	&= \frac{\sum\limits_{i=1}^{N}(x_i^2-2\mu x_i+\mu^2)}{N} \\
	&= \frac{\sum\limits_{i=1}^{N}
	          x_i^2-\sum\limits_{i=1}^{N}2\mu x_i
	          + \sum\limits_{i=1}^{N}\mu^2}{N}  \\
	&= \frac{\sum\limits_{i=1}^{N}
	          x_i^2-2\mu\sum\limits_{i=1}^{N}x_i+N\mu^2}{N} \\
	&= \frac{\sum\limits_{i=1}^{N}x_i^2
	          -2\frac{\sum\limits_{i=1}^{N}x_i}{N}\sum\limits_{i=1}^{N}x_i
		        + N\Big(\frac{\sum\limits_{i=1}^{N}x_i}{N}\Big)^2}{N} \\
  &= \frac{\sum\limits_{i=1}^{N}
            x_i^2-2\frac{\Big(\sum\limits_{i=1}^{N}x_i\Big)^2}{N}
            + \frac{\Big(\sum\limits_{i=1}^{N}x_i\Big)^2}{N}}{N} \\
	&= \frac{\sum\limits_{i=1}^{N}x_i^2-\frac{\Big(\sum\limits_{i=1}^{N}x_i\Big)^2}{N}}{N}
\end{align*}$$



### Computational Formula for $s$^2 {#computational-formula-sample-variance}

$$\begin{align*} 
s^2
	&= \frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n-1} \\
	&= \frac{\sum\limits_{i=1}^{n}x_i^2-\frac{\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n}}{n-1}
\end{align*}$$

_Proof:_

$$\begin{align*} 
\frac{\sum\limits_{i=1}^{n}(x_i-\bar x)^2}{n-1}
	&= \frac{\sum\limits_{i=1}^{n}(x_i^2-2\bar x x_i+\bar x^2)}{n-1} \\
	&= \frac{\sum\limits_{i=1}^{n}x_i^2-\sum\limits_{i=1}^{n}2\bar x x_i
		+ \sum\limits_{i=1}^{n}\bar x^2}{n-1} \\
  &= \frac{\sum\limits_{i=1}^{n}x_i^2-2\bar x\sum\limits_{i=1}^{n}x_i+n\bar x^2}{n-1} \\
	&= \frac{\sum\limits_{i=1}^{n}x_i^2-2\frac{\sum\limits_{i=1}^{n}x_i}{n}\sum\limits_{i=1}^{n}x_i
		+ n\Big(\frac{\sum\limits_{i=1}^{n}x_i}{n}\Big)^2}{n-1} \\
  &= \frac{\sum\limits_{i=1}^{n}x_i^2-2\frac{\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n}
		+ \frac{\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n}}{n-1} \\
	&= \frac{\sum\limits_{i=1}^{n}x_i^2-\frac{\Big(\sum\limits_{i=1}^{n}x_i\Big)^2}{n}}{n-1}
\end{align*}$$

<!--chapter:end:Variance.Rmd-->

# Weibull Distribution

## Probability Distribution Function
A random variable $X$ is said to have a Weibull Distribution with parameters $\alpha$ and $\beta$ if its probability density function is:

\[f(x)=\left\{
	\begin{array}{ll}
		\alpha\beta x^{\beta-1}e^{-\alpha x^{\beta}},&0<x,\ 0<\alpha,\ 0<\beta\\
		0 & otherwise 
	\end{array}\right.
\]

## Cumulative Distribution Function

$$\begin{align*} 
\int\limits_{0}^{x}\alpha\beta t^{\beta-1}e^{-\alpha t^\beta}dt
	&= \alpha\beta\int\limits_{0}^{x}t^{\beta-1}e^{-\alpha t^\beta}dt \\
	&= \alpha\beta\Big[\frac{-1}{\alpha\beta}e^{-\alpha t^\beta}\Big]_0^x \\
  &= \alpha\beta\Big[\frac{-1}{\alpha\beta}e^{-\alpha x^\beta}
		+ \frac{1}{\alpha\beta}\Big] \\
	&= \frac{\alpha\beta}{\alpha\beta}\big(-e^{-\alpha x^\beta}+1\big) \\
	&= 1-e^{-\alpha x^\beta}
\end{align*}$$

Using this result, we can write the Cumulative Distribution Function as

\[F(x)=\left\{
	\begin{array}{ll}
		1-e^{-\alpha x^\beta},& 0<x,\ 0<\alpha,\ 0<\beta\\
		0 & otherwise 
	\end{array}\right.
\]

## Expected Values

$$\begin{align*} 
E(X)
	&= \int\limits_{0}^{\infty}x\alpha\beta 
	 	x^{\beta-1}e^{-\alpha x^{\beta}}dx \\
	&= \alpha\beta\int\limits_{0}^{\infty}x 
		x^{\beta-1}e^{-\alpha x^{\beta}}dx \\
^{[1]}  &= \alpha\beta\int\limits_{0}^{\infty}
		\Big(\big(\frac{y}{\alpha}\big)^\frac{1}{\beta}\Big)^\beta
		e^{-y}\frac{1}{\alpha\beta}
		\Big(\frac{y}{\alpha}\Big)^{\frac{1}{\beta}-1}dy \\
	&= \frac{\alpha\beta}{\alpha\beta}\int\limits_{0}^{\infty}
		\Big(\frac{y}{\alpha}\Big)^\frac{\beta+1}{\beta}
		\Big(\frac{y}{\alpha}\Big)^{\frac{1}{\beta}-1}e^{-y}dy \\
  &= \int\limits_{0}^{\infty}\Big(\frac{y}{\alpha}\Big)
		^{\frac{\beta+1}{\beta}-\frac{1}{\beta}-1}e^{-y}dy \\
	&= \int\limits_{0}^{\infty}\Big(\frac{y}{\alpha}\Big)
		^{\frac{\beta+1}{\beta}-1}e^{-y}dy \\
	&= \frac{1}{\alpha^{\frac{\beta+1}{\beta}-\frac{\beta}{\beta}}}
		\int\limits_{0}^{\infty}y^{\frac{\beta+1}{\beta}-1}e^{-y} \\
  &= \alpha^{-\frac{1}{\beta}}
		\int\limits_{0}^{\infty}y^{\frac{\beta+1}{\beta}-1}e^{-y} \\
^{[2]}	&= \alpha^{-\frac{1}{\beta}}\Gamma\Big(\frac{\beta+1}{\beta}\Big)
\end{align*}$$

> 1. $y=\alpha x^\beta\ \Rightarrow 
		x=(\frac{y}{\alpha})^\frac{1}{\beta}\ \Rightarrow
		dx=\frac{1}{\alpha\beta}(\frac{y}{\alpha})^{\frac{1}{\beta}-1}$
> 2. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-x}dx
		=\Gamma(\alpha)$
		
$$\begin{align*} 
E(X^2)
	&= \alpha\beta\int\limits_{0}^{\infty}x^2x^{\beta-1}e^{-\alpha x^\beta}dx \\
	&= \alpha\beta\int\limits_{0}^{\infty}x^{\beta+1}e^{-\alpha x^\beta}dx \\
^{[1]} &= \alpha\beta\int\limits_{0}^{\infty}\Big(\big(\frac{y}{\alpha}\big)^\frac{1}{\beta}\Big)^{\beta+1} 
		e^{-y}\frac{1}{\alpha\beta}\big(\frac{y}{\alpha}\big)^{\frac{1}{\beta}-1}dy \\
	&= \frac{\alpha\beta}{\alpha\beta}\int\limits_{0}^{\infty}\bigg(\frac{y}{\alpha}\bigg)^{\frac{\beta+1}{\beta}}
		\bigg(\frac{y}{\alpha}\bigg)^{\frac{1}{\beta}-1}e^{-y}dy \\
  &= \int\limits_{0}^{\infty}\bigg(\frac{y}{\alpha}\bigg)^{\frac{\beta+1}{\beta}+\frac{1}{\beta}-1}e^{-y}dy \\
	&= \frac{1}{\alpha^{\frac{\beta+2}{\beta}-\frac{\beta}{\beta}}}
		\int\limits_{0}^{\infty}y^{\frac{\beta+2}{\beta}-1}e^{-y}dy \\
	&= \alpha^{-\frac{2}{\beta}}\int\limits_{0}^{\infty}y^{\frac{\beta+2}{\beta}-1}e^{-y}dy \\
^{[2]} = \alpha^{-\frac{2}{\beta}}\Gamma\Big(\frac{\beta+2}{\beta}\Big)
\end{align*}$$

> 1. $y=\alpha x^\beta\ \Rightarrow 
		x=(\frac{y}{\alpha})^\frac{1}{\beta}\ \Rightarrow
		dx=\frac{1}{\alpha\beta}(\frac{y}{\alpha})^{\frac{1}{\beta}-1}$
> 2. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-x}dx
		=\Gamma(\alpha)$
		

$$\begin{align*} 
\mu
	&= E(X) \\
	&= \alpha^{-\frac{1}{\beta}}\Gamma\Big(\frac{\beta+1}{\beta}\Big)\\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \alpha^{-\frac{2}{\beta}}\Gamma\Big(\frac{\beta+2}{\beta}\Big)
		- \alpha^{-\frac{2}{\beta}}\Gamma\Big(\frac{\beta+1}{\beta}\Big)^2 \\
  &= \alpha^{-\frac{2}{\beta}}\Big[\Gamma\Big(\frac{\beta+2}{\beta}\Big)
		- \Gamma\Big(\frac{\beta+1}{\beta}\Big)^2\Big]
\end{align*}$$

## Theorems for the Weibull Distribution

### Validity of the Distribution

$$ 
\int\limits_{0}^{\infty}\alpha\beta x^{\beta-1}e^{-\alpha x^\beta}dx
	= 1
$$

_Proof:_

$$\begin{align*} 
\int\limits_{0}^{\infty}\alpha\beta x^{\beta-1}e^{-\alpha x^\beta}dx
	      &= \alpha\beta\int\limits_{0}^{\infty}x^{\beta-1}e^{-\alpha x^\beta}dx \\
^{[1]}  &= \alpha\beta\int\limits_{0}^{\infty}\Big(\big(\frac{y}{\alpha}\big)^
            \frac{1}{\beta}\Big)^{\beta-1}
		        e^{-y}\big(\frac{y}{\alpha}\big)^{\frac{1}{\beta}-1}\frac{1}{\alpha\beta}dy
      	&= \frac{\alpha\beta}{\alpha\beta}\int\limits_{0}^{\infty}\big(\frac{y}{\alpha}\big)^\frac{\beta}{-1}
      		\big(\frac{y}{\alpha}\big)^{\frac{1}{\beta}-1}e^{-y}dy \\
        &= \int\limits_{0}^{\infty}\big(\frac{y}{\alpha}\big)^{\frac{\beta-1}{\beta}+\frac{1-\beta}{\beta}}e^{-y}dy \\
      	&= \int\limits_{0}^{\infty}\frac{y^0}{\alpha^0}e^{-y}dy \\
      	&= \int\limits_{0}^{\infty}y^{1-1}e^{-y}dy \\
^{[2]}	&= \Gamma(1)=1
\end{align*}$$

> 1. $y=\alpha x^\beta\ \Rightarrow 
		x = (\frac{y}{\alpha})^\frac{1}{\beta}\ \Rightarrow
		dx = \frac{1}{\alpha\beta}(\frac{y}{\alpha})^{\frac{1}{\beta}-1}$
> 2. $\int\limits_{0}^{\infty}x^{\alpha-1}e^{-x}dx
		= \Gamma(\alpha)$

<!--chapter:end:Weibull.Rmd-->

# References

<!--chapter:end:zzz-references.Rmd-->

