# Bernoulli Distribution

## Probability Mass Function

A random variable is said to have a Bernoulli Distribution with parameter $p$ if its probability mass function is:
\[\Pr(X = x)=\left\{ 
		\begin{array}{ll}
			\pi^x(1-\pi)^{1-x},	& x=0,1\\
			0 		& \mathrm{otherwise}
		\end{array} 
	\right. \]
Where  $\pi$ is the probability of a success.

## Cumulative Mass Function

\label{Bernoulli1}
\[\Pr(X \leq x)=\left\{
		\begin{array}{lll}
			0   & x<0\\
			1-\pi & x=0\\
			1   & 1\leq x
		\end{array}
	\right. \]
	

```{r Bernoulli_Distribution, echo = FALSE, fig.path= 'figures/', fig.cap = 'The graphs on the left and right show a Bernoulli Probability Distribution and Cumulative Distribution Function, respectively, with $\\pi=.4$.  Note that this is identical to a Binomial Distribution with parameters $n=1$ and $\\pi=.4$.'}
Bernoulli <- 
  data.frame(x = 0:1) %>%
  mutate(pmf = dbinom(x, size = 1, p = 0.4),
         cmf = pbinom(x, size = 1, p = 0.4)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             c("pmf", "cmf"),
                             c("Probability Mass",
                               "Cumulative Mass")))

ggplot(data = Bernoulli,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob), 
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:1) + 
  ylab("P(x)") + 
  theme_bw()
```

## Expected Values

$$ 
\begin{aligned}
E(X) 
  &= \sum\limits_{i=0}^{1} x\cdot \Pr(X = x) \\
	&= \sum\limits_{i=0}^{1} x \cdot \pi^{x} (1-\pi)^{1-x}\\
  &= 0 \cdot \pi^{0} (1-\pi)^{1-0} + 1 \cdot \pi^{1} (1-\pi)^{1-1}\\
  &= 0 + \pi (1-\pi)^{0}\\
	&= \pi\\
	\\
	\\
E(X^{2}) 
  &= \sum\limits_{i=0}^{1} x^2 \cdot \Pr(X = x)\\
  &= \sum\limits_{i=0}^{1} x^{2} \cdot \pi^x (1-\pi)^{1-x}\\
  &= \sum\limits_{i=0}^{1} 0^{2} \cdot \pi^0 (1-\pi)^{1-0} + 1^2 \cdot \pi^1 (1-\pi)^{1-1}\\
  &= 0 \cdot 1 \cdot 1 + 1 \cdot \pi \cdot 1 \\
	&= 0 + \pi\\
 	&= \pi\\
 	\\
 	\\
\mu	&= E(X) = \pi\\
  \\
  \\
\sigma^2 
  &= E(X^2) - E(X)^2 \\
  &= \pi-\pi^2 \\
  &= \pi(1-\pi)
\end{aligned}
$$

## Moment Generating Function

$$\begin{aligned}
M_{X}(t)
	&= E(e^{tX}) \\
	&= \sum\limits_{i=0}^{1} e^{tx} p(x) \\
	&= \sum\limits_{i=0}^{1} e^{tx} \pi^{x} (1-\pi)^{1-x}\\
	&= e^{t0} \pi^0 (1-\pi)^{1-0} + e^t \pi^t (1-\pi)^{1-1}\\
	&= (1-\pi) + e^t \pi\\
	&=\pi e^t + (1-\pi) \\
	\\
	\\
M^{(1)}_X(t) &= \pi e^t\\
  \\
  \\
M^{(2)}_X(t) &= \pi e^t\\
  \\
  \\
E(X)
	&=M^{(1)}_X(0)\\
	&= \pi e^0\\
	&= \pi e^0\\
	&= \pi\\
	\\
	\\
E(X^2)
	&= M^{(2)}_X(0)\\
	&= \pi e^0\\
	&= \pi\\
	\\
	\\
\mu
  &= E(X)\\
	&= \pi\\
  \\
  \\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= \pi - \pi^2 \\
	&= \pi (1-\pi)
\end{aligned}
$$


## Theorems for the Bernoulli Distribution

### Validity of the Distribution

$$\sum\limits_{x=0}^{1}\pi^x(1-\pi)^{1-x}=1$$

_Proof:_

$$\begin{aligned}
\sum\limits_{x=0}^{1} \pi^x (1-\pi)^{1-x}
	&= \pi^0 (1-\pi)^1 + \pi^1 (1-\pi)^0 \\
	&= (1-\pi) + \pi \\
	&= 1
\end{aligned}$$


### Sum of Bernoulli Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.\\
Then $Y\sim$ Binomial$(n,\pi)$

_Proof:_
$$\begin{aligned}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_n}) \\
	&= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n}) \\
  &= (\pi e^t+(1-\pi)) (\pi e^t+(1-\pi)) \cdots (\pi e^t+(1-\pi)) \\
	&= (\pi e^t+(1-\pi))^n
\end{aligned}$$

Which is the moment generating function of a Binomial random variable with parameters $n$ and $\pi$.  Thus, $Y\sim$ Binomial$(n,\pi)$. 
