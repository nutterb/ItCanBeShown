[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2016-08-12 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["analysis-of-variance.html", "2 Analysis of Variance 2.1 One-Way Design 2.2 Computational Formulas 2.3 Randomized Complete Block Design", " 2 Analysis of Variance 2.1 One-Way Design 2.1.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+} + \\bar x_{i+} - x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (x_{ij} - \\bar x{i+}) + (\\bar x_{i+} - \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++}) + (x_{ij} - \\bar x{i+}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + (x_{ij} - \\bar x_{i+})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\bigg(\\sum\\limits_{j=1}^{n_i} x_{ij} - \\sum\\limits_{j=1}^{n_i}\\bar x_{i+}\\bigg) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - n_i \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\big(x_{i+} - n_i \\frac{x_{i+}}{n_i}\\big) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\cdot 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2\\\\ \\end{align*}\\] The components are commonly referred to as \\[ SS_{Factor} = \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2$ and \\] \\[ SS_{Error} = \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2$ \\] Notice that \\(SS_{Factor}\\) compares the factor means to the overall mean, and it can be said that \\(SS_{Factor}\\) measures the variation between factors. \\(SS_{Error}\\) compares each observation to the overall mean, and can be said to describe the variation within factors. When \\(n_1 = n_2 = \\cdots n_i = n\\), the design is said to be balanced. 2.2 Computational Formulas \\(SS_{Total}\\) and \\(SS_{Factor}\\) can be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} x_{ij}^2 - x_{++} \\sum\\limits_{j=1}^{n_i}\\frac{1}{n_i}\\\\ \\end{align*}\\] See Theorem 7.3.1 \\[\\begin{align*} SS_{Factor} &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\frac{\\bar x_{i+}^2}{n_i} - \\bar x_{++} \\sum\\limits_{i=1}^{a}\\frac{1}{n_i} \\end{align*}\\] See Theorem 7.3.1 \\(SS_{Error}\\) does not simplify to a convenient form, but \\[\\begin{align*} SS_{Total} &amp;= SS_{Factor} + SS_{Error} \\\\ \\Rightarrow SS_{Error} &amp;= SS_{Total} - SS_{Factor} \\end{align*}\\] 2.3 Randomized Complete Block Design Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable. To implement this design, suppose we have \\(a\\) treatments we want to compare, and \\(b\\) blocks. We may analyze the data by use of the sums of squares, similar to the one-way design. 2.3.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} + \\bar x_{i+} - \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{+ j} + \\bar x_{++} - \\bar x_{+ +} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++}) + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++}) (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big] \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 + 0 + 0 + 0 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 + a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] It is shown that the cross products are equal to zero in Section 2.3.3 These terms are commonly referred to as \\[\\begin{align*} SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ SS_{Error} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] 2.3.2 Computational Formulae \\(SS_{Total}\\), \\(SS_{Factor}\\), and \\(SS_{Block}\\) can all be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} x_{ij}^2 - \\frac{x_{++}}{ab}\\\\ \\\\ SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{b}\\sum\\limits_{i=1}^{a}x_{i+}^2 - \\frac{x_{++}^2}{ab} \\\\ \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{a}\\sum\\limits_{j=1}^{b} x_{+ j}^2 - \\frac{x_{++}^2}{ab} \\end{align*}\\] See Theorem 7.3.1 \\(SS_{Error}\\) does not simplify to any convenient form, but may be calculated from the other terms as \\(SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}\\) 2.3.3 RCBD Cross Products The cross products of the RCBD design \\[\\begin{align*} 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp;= 0 \\end{align*}\\] Proof: \\[ 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\\\ \\ \\ = 2\\bigg(\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\bigg)\\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ \\bar x_{i+}\\bar x_{+ j} - \\bar x_{i+}\\bar x_{++} - \\bar x_{+ j}\\bar x_{++} + \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}\\bar x_{+ j} - \\bar x_{+ j}^2 + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}^2 - \\bar x_{i+}\\bar x_{+ j} + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}( -\\bar x_{++}^2 - \\bar x_{i+}^2 - \\bar x_{+ j}^2 + x_{ij}\\bar x_{i+} + x_{ij}\\bar x_{+ j} - 2 x_{ij}\\bar x_{++} - \\bar x_{i+}\\bar x_{+ j} \\\\ \\ \\ \\ \\ + 2\\bar x_{i+}\\bar x_{++} + 2\\bar x_{+ j}\\bar x_{++} ) \\\\ \\ \\ = 2\\bigg(-\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{++}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j} \\\\ \\ \\ \\ \\ - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2 x_{ij}\\bar x_{++} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ = 2\\bigg( \\frac{ab\\bar x_{++}^2}{a^2b^2} - \\frac{b}{b^2}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{a}{a^2}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[1]} =2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[2]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[3]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ ^{[4]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\] \\[ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[5]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\frac{2\\bar x_{++}^2}{ab} \\bigg)\\\\ \\ \\ = 2\\bigg(\\frac{4\\bar x_{++}^2}{ab} - \\frac{4\\bar x_{++}^2}{ab} + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\bigg)\\\\ \\ \\ = 2(0 + 0 + 0) \\\\ = 2(0) \\\\ = 0 \\] See Summation Theorem 6.1.7 See Summation Theorem 6.1.8 See Summation Theorem 6.1.4 See Summation Theorem 6.1.5 See Summation Theorem 6.1.6 Using the theorems in Chapter it is can be shown that each of the three cross products is equal to zero. However, the physical tedium of reducing each cross product is much greater than the approach taken above. "],
["bernoulli-distribution.html", "3 Bernoulli Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function 3.3 Expected Values 3.4 Moment Generating Function 3.5 Theorems for the Bernoulli Distribution", " 3 Bernoulli Distribution 3.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} p^x(1-p)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(p\\) is the probability of a success. 3.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-p &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] Figure 3.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(p=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(p=.4\\). 3.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot p^{x} (1-p)^{1-x}\\\\ &amp;= 0 \\cdot p^{0} (1-p)^{1-0} + 1 \\cdot p^{1} (1-p)^{1-1}\\\\ &amp;= 0 + p (1-p)^{0}\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot p^x (1-p)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot p^0 (1-p)^{1-0} + 1^2 \\cdot p^1 (1-p)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot p \\cdot 1 \\\\ &amp;= 0 + p\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X) = p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p-p^2 \\\\ &amp;= p(1-p) \\end{align*} \\] 3.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\\\ &amp;= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}\\\\ &amp;= (1-p) + e^t p\\\\ &amp;=pe^t + (1-p) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p (1-p) \\end{align*} \\] 3.5 Theorems for the Bernoulli Distribution 3.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}p^x(1-p)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} p^x (1-p)^{1-x} &amp;= p^0 (1-p)^1 + p^1 (1-p)^0 \\\\ &amp;= (1-p) + p \\\\ &amp;= 1 \\end{align*}\\] 3.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p)) (pe^t+(1-p)) \\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-distribution.html", "4 Binomial Distribution 4.1 Probability Mass Function 4.2 Cumulative Mass Function 4.3 Expected Values 4.4 Moment Generating Function 4.5 Maximum Likelihood Estimator 4.6 Theorems for the Binomial Distribution", " 4 Binomial Distribution 4.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(p\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} p^x (1-p)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(p\\) is the probability of a success on each individual trial. 4.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] Figure 4.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(p=.4\\). 4.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\ &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\ &amp;= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\ &amp;= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1} + \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\ &amp;= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x}) \\\\ ^{[2]} &amp;= np(p+q)^{n-1} \\\\ ^{[1]} &amp;= np(p+(1-p))^{n-1} \\\\ &amp;= np(p+1-p)^{n-1} \\\\ &amp;= np(1)^{n-1} \\\\ &amp;= np(1) \\\\ &amp;= np \\end{align*}\\] Let \\(q = (1 - p)\\) By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1} + \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n} \\\\ &amp;= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1} + 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big] \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\ &amp;= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\\\ ^{[2]} &amp;= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y} \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big] \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big] \\\\ ^{[3]} &amp;= np(mp+1) \\\\ &amp;= np[(n-1)p+1] \\\\ &amp;=np(np-p+1) \\\\ &amp;=n^2p^2 - np^2 + np \\end{align*}\\] \\(q = (1 - p)\\) Let \\(y = x - 1\\) and \\(n = m + 1\\) \\(\\Rightarrow\\) \\(x = y + 1\\) and \\(m = n - 1\\) \\(\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}\\) is of the form of the expected value of \\(Y\\), and \\(E(Y)=mp=(n-1)p\\). \\(\\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}\\) is the sum of all probabilities over the domain of \\(Y\\) which is 1. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n^2p^2 - np^2 + np - n^2p^2 \\\\ &amp;= -np^2 + np \\\\ &amp;= np(-p-1) \\\\ &amp;= np(1-p) \\end{align*}\\] 4.4 Moment Generating Function \\[ \\begin{align*} M_X(t) &amp;= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x} \\\\ ^{[1]} &amp;= [(1-p)+pe^t]^n \\end{align*} \\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[ \\begin{align*} M_X^{(1)}(t) &amp;= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\\\ \\\\ M_X^{(2)}(t) &amp;= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\\\ &amp;= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= n[(1-p)+pe^0]^{n-1}pe^0 \\\\ &amp;= n[1-p+p^{n-1}p\\\\ &amp;= n(1)^{n-1}p &amp;= np\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\\cdot0}[(1-p) + pe^0]^{n-2} \\\\ &amp;= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\\\ &amp;= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\\\ &amp;= np+n(n-1)p^2 \\\\ &amp;= np+(n^2-n)p^2 \\\\ &amp;= np + n^2 + n^2p^2 - np^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= np + n^2p^2 - np^2 - n^2p^2 \\\\ &amp;= np - np^2\\\\ &amp;= np(1-p) \\end{align*}\\] 4.5 Maximum Likelihood Estimator Since \\(n\\) is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for \\(n\\). The mean and variance can both be estimated from the single parameter \\(p\\). Let \\(X\\) be a Binomial random variable with parameter \\(p\\) and \\(n\\) outcomes \\((x_1,x_2,\\ldots,x_n)\\). Let \\(x_i=0\\) for a failure and \\(x_i=1\\) for a success. In other words, \\(X\\) is the sum of \\(n\\) Bernoulli trials with equal probability of success and \\(X=\\sum\\limits_{i=1}^{n}x_i\\). 4.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\ &amp;= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\ &amp;= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\ &amp;= \\theta^X(1-\\theta)^{n-X} \\end{align*}\\] 4.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln L(\\theta) \\\\ &amp;= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\ &amp;= X\\ln(\\theta)+(n-X)\\ln(1-\\theta) \\end{align*}\\] 4.5.3 MLE for p \\[\\begin{align*} \\frac{d\\ell(p)}{d p} &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ 0 &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\Rightarrow \\frac{X}{p} &amp;= \\frac{n-X}{1-p} \\\\ \\Rightarrow (1-p)X &amp;= p(n-X) \\\\ \\Rightarrow X-pX &amp;= np-pX \\\\ \\Rightarrow X &amp;= np \\\\ \\Rightarrow \\frac{X}{n} &amp;= p \\\\ \\end{align*}\\] So \\(\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(p\\). 4.6 Theorems for the Binomial Distribution 4.6.1 Validity of the Distribution \\[\\begin{align*} \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x} = 1 \\end{align*}\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\big(p + (1-p)\\big)^n \\\\ &amp;= (1)^n \\\\ &amp;= 1 \\end{align*}\\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) 4.6.2 Sum of Binomial Random Variables Let \\(X_1,X_2,\\ldots,X_k\\) be independent random variables where \\(X_i\\) comes from a Binomial distribution with parameters \\(n_i\\) and \\(p\\). That is \\(X_i\\sim(n_i,p)\\). Let \\(Y = \\sum\\limits_{i=1}{k} X_i\\). Then \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\ &amp;= \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\\\ &amp;= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i} \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(\\sum\\limits_{i=1}^{k}n_i\\) and \\(p\\). Thus \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). 4.6.3 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["the-binomial-theorem.html", "5 The Binomial Theorem 5.1 Traditional Proof 5.2 General Approach 5.3 Other Theorems", " 5 The Binomial Theorem The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions. Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach. Other useful theorems are provided at the end of this chapter. 5.1 Traditional Proof 5.1.1 Lemma: Pascal’s rule Let \\(n\\) and \\(x\\) be non-negative integers such that \\(x\\leq n\\). Then \\({n-1\\choose x} + {n-1\\choose x-1} = {n\\choose x}\\). Proof: \\[\\begin{align*} {n-1\\choose x} + {n-1\\choose x-1} &amp;= \\frac{(n-1)!}{x!(n-1-x)!} + \\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)!}\\\\ &amp;= \\frac{(n-1)!}{x(x-1)!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!} +\\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{n!}{x!(n-x)!} \\\\ &amp;= {n\\choose x} \\end{align*}\\] 5.1.2 The Binomial Theorem Let \\(a\\) and \\(b\\) be constants and let \\(n\\) be any positive integer. Then \\[(a+b)^n = \\sum\\limits_{x=0}^{n} {n\\choose x} a^{n-x} b^x\\] Proof: This proof is completed by mathematical induction. Base Step: \\(n=1\\) \\[\\begin{align*} (a+b)^1 &amp;= \\sum\\limits_{x=0}^{1} {1\\choose x} a^{1-x} b^x \\\\ &amp;= {1\\choose 0} a^{1-0} b^0 + {1\\choose 1} a^{1-1} b^1 \\\\ &amp;= 1\\cdot a\\cdot 1 + 1\\cdot 1\\cdot b \\\\ &amp;= a+b \\end{align*}\\] Inductive Step: Assume that the Theorem holds for \\(n\\), and show it is true for \\(n+1\\). \\[\\begin{align*} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= a(a+b)^n + b(a+b)^n \\\\ &amp;= a(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x + b^n) + b(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^n) \\\\ &amp;= (a^{n+1}+a\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^{n+1}) \\\\ &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x+1}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[1]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}a^{n-x+1}b^x) + (\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[2]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}{n\\choose x}a^{n-x+1}b^x) + \\sum\\limits_{x-1}^{n-1}{n\\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\\\ ^{[3]} &amp;= a^{n+1} + \\sum\\limits_{x+1}^{n}{n+1\\choose x}a^{n-x+1}b^x + b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{x=1}^{n}{n+1\\choose x}a^{(n+1)-x}b^x+b^{n+1} \\\\ ^{[4]} &amp;= \\sum\\limits_{x=0}^{n+1}{n+1\\choose x}a^{(n+1)-x}b^x \\end{align*}\\] This completes both the inductive step and the proof. \\(ab^n={n\\choose n}a^{n-n+1}b^n\\) which is the term for \\(x=n\\) in the first summation. \\(a^nb={n\\choose 0}a^{n-0}b^1\\) which is the term for \\(x=0\\) in the second summation. \\(\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-x+1}b^x\\) This step is made using Pascal’s Rule with \\(n=n-1\\). \\(a^{n+1}={n+1\\choose 0}a^{(n+1)-0}b^0\\) which is the term for \\(x=0\\) in the summation. \\(\\ \\ b^{n+1}={n+1\\choose n+1}a^{(n+1)-(n+1)}b^{n+1}\\) which is the term for \\(x=n+1\\) in the summation 5.2 General Approach 5.2.1 A Binomial Expansion Theorem This theorem and its corrolary are provided by Brunette.\\ For any positive integer \\(n\\), let \\(B_n = (x_1+y_1) (x_2+y_2) \\cdots (x_n+y_n)\\). In the expansion \\(B_n\\), before combining possible like terms, the following are true: There will be \\(2^n\\) terms. Each of these terms will be a product of \\(n\\) factors. In each such product there will be one factor from each binomial (in \\(B_n\\)). Every such product of \\(n\\) factors, one from each binomial, is represented in the expansion. Proof: Proof is done by induction. For the case \\(n=1\\), the result is clear. Now assume that the theorem is true for a particular \\(n\\) and consider \\(B_{n+1}\\). \\[ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} \\] By the inductive assumption, \\(B_n = T_1 + T_2 + \\cdots + T_{2^n}\\) where each \\(T_i\\) is a product of \\(n\\) factors, one factor from each binomial. It follows that every term in the expansion of \\(B_n+1\\) is either of the type \\(T_ix_{n+1}\\) or \\(T_iy_{n+1}\\), for some \\(1\\leq i \\leq 2^n\\). But each term of either of the above types is clearly a product of \\(n+1\\) factors with one factor coming from each binomial. thus, if (ii) and (iii) are true for \\(B_n\\), then they are true for \\(B_n+1\\). Next, by the inductive assumption, the expansion of \\(B_n\\) is a sum of \\(2^n+2^n\\) terms, i.e., \\(2^{n+1}\\) terms. This completes the inductive step for (i). Lastly, it remains for us to consider a product of the type \\(p_1 p_2 \\cdots p_n p_{n+1}\\) where, for each \\(1\\leq i\\leq n+1\\), \\(p_i = x_i\\) or \\(p_i = y_i\\). By the inductive hypothesis, \\(p_1 p_2 \\cdots p_n\\) is a term in the expansion of \\(B_n\\). If \\(p_{n+1} = x_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_nx_{n+1}\\), and so of \\(B_{n+1}\\). Likewise, if \\(p_{n+1}=y_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_n y_{n+1}\\), and so of \\(B_{n+1}\\). This completes the inductive step and the proof. 5.2.2 Corollary: Binomial Theorem Let \\(x\\) and \\(y\\) be constants and let \\(n\\) be any positive integer.\\ Then \\(\\displaystyle (x+y)^n = \\sum\\limits_{i=0}^{n} {n\\choose i} x^{n-i} y^i\\\\\\) Proof: Since each term in the expansion will have \\(n\\) terms, each term must follow the form \\(x^{n-i} y^i\\) for \\(0 \\leq i \\leq n\\), and in all, there are \\(2^n\\) such terms. For any given value of \\(i\\), the number of terms of the form \\(x^{n-i}y^i\\) is clearly the number of ways one can choose the \\(i\\) factors of \\(y\\) from the \\(n\\) available binomials, i.e., \\({n\\choose i}\\), which gives \\[(x+y)^n = \\sum\\limits_{i=0}^{n}{n\\choose i} x^{n-i} y^i\\] 5.3 Other Theorems 5.3.1 Theorem \\[{N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n}\\] where \\(0 \\leq n \\leq N_1 + N_2\\). Proof: Using the Binomial Theorem we establish \\[ (1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\\\ \\Rightarrow [{N_1\\choose 0}a^0+\\cdots+{N_1\\choose N_1}a^{N_1}]\\cdot [{N_2\\choose 0}a^0+\\cdots+{N_2\\choose N_2}a^{N_2}] \\\\ \\ \\ \\ \\ ={N_1+N_2\\choose 0}+{N_1+N_2\\choose 1}a+\\cdots +{N_1+N_2\\choose N_1+N_2}a^{N_1+N_2} \\] Expanding the left side of the equation gives \\[ {N_1\\choose 0}{N_2\\choose 0} + {N_1\\choose 0}{N_2\\choose 1}a + \\cdots + {N_1\\choose 0}{N_2\\choose N_2}a^{N_2} + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose 1}{N_2\\choose N_2}a^{N_2+1} + \\cdots + {N_1\\choose N_1}{N_2\\choose 0}a^{N_1} + {N_1\\choose N_1}{N_2\\choose 1}a^{N_1+1} \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\\\ = {N_1\\choose 0}{N_2\\choose 0}+{N_1\\choose 0}{N_2\\choose 1}a + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + {N_1\\choose 0}{N_2\\choose 2}a^2+{N_1\\choose 1}{N_2\\choose 1}a^2 + {N_1\\choose 2}{N_2\\choose 0}a^2 \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\] Notice that for any \\(n\\) where \\(0 \\leq n \\leq N_1 + N_2\\), the coefficient for \\(a^n\\), found by combining like terms, is \\({N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 1}{N_2\\choose n-1} + \\cdots+{N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose 0}{N_2\\choose n}\\) and, by the equivalence of the first equation in the proof, is equal to the coefficient \\({N_1 + N_2\\choose n}\\). 5.3.2 Theorem \\[\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}} = 1\\] for \\(0 \\leq n \\leq N_1 + N_2\\).\\ Proof: Theorem 5.3.1 establishes the equality \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\frac{\\sum\\limits_{i=1}^{n} {N_1\\choose i}{N_2\\choose n-i}} {{N_1+N_2\\choose n}} = 1 \\] "],
["summation.html", "6 Summation 6.1 Theorems of Summation", " 6 Summation 6.1 Theorems of Summation 6.1.1 Theorem If \\(c\\) is a constant then \\[\\sum\\limits_{i=1}^{n}c = nc\\] Proof: \\[ \\sum\\limits_{i=1}^{n}c = \\underbrace{c+c+\\cdots+c}_{n\\ \\rm terms} = nc \\] 6.1.2 Theorem If \\(a_1,a_2,\\ldots,a_n\\) are real numbers and \\(c\\) is a constant, then \\[ \\sum\\limits_{i=1}^{n}ca_i = c\\sum\\limits_{i=1}^{n}a_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}ca_i &amp;= ca_1 + ca_2 + \\cdots + ca_n \\\\ &amp;= c(a_1+a_2+\\cdots+a_n) \\\\ &amp;= c\\sum\\limits_{i=1}^{n}a_i \\end{align*}\\] 6.1.3 Theorem If \\(a_1,_2,\\ldots,a_n\\) are real numbers and \\(b_1,b_2,\\ldots,b_n\\) are real numbers, then \\[ \\sum\\limits_{i=1}^{n}(a_i+b_i) = \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}(a_i+b_i) &amp;= a_1 + b_1 + a_2 + b_2 + \\cdots + a_n + b_n \\\\ &amp;= a_1 + a_2 + \\cdots + a_n + b_1 + b_2 + \\cdots + b_n \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\end{align*}\\] 6.1.4 Theorem If \\(a_i\\) and \\(b_j\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j = a_{+} b_{+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_j\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i b_{+} \\\\ &amp;= b_{+} \\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= a_{+} b_{+} \\end{align*}\\] 6.1.5 Theorem If \\(a_i\\) is a real number for \\(i=1,2,\\ldots,n\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b = m a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b &amp;= \\sum\\limits_{i=1}^{n}m a_i b \\\\ &amp;= m b\\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= m a_{+} b \\end{align*}\\] 6.1.6 Theorem If \\(a_j\\) is a real number for \\(j=1,2,\\ldots,m\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b = n a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b &amp;= \\sum\\limits_{i=1}^{n}\\bigg( b \\sum\\limits_{j=1}^{m} a_j \\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_{+}b \\\\ &amp;= n a_{+} b \\end{align*}\\] 6.1.7 Theorem If \\(a_i\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} = \\sum\\limits_{i=1}^{n}a_ib_{i+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_{ij}\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_ib_{i+} \\end{align*}\\] 6.1.8 Theorem If \\(a_j\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[\\ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} = \\sum\\limits_{i=1}^{n}a_jb_{+ j} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} &amp;= a_1b_{11}+a_2b_{12}+\\cdots+a_mb_{1m} \\\\ &amp; \\ \\ \\ \\ +a_1b_{21}+a_2b_{22}+\\cdots+a_mb_{2m} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_1b_{n1}+a_1b_{n1}+\\cdots+a_1b_{nm} \\\\ &amp;= a_1b_{11}+a_1b_{21}+\\cdots+a_1b_{n1} \\\\ &amp; \\ \\ \\ \\ +a_2b_{12}+a_2b_{22}+\\cdots+a_2b_{n2} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_mb_{1m}+a_mb_{2m}+\\cdots+a_nb_{nm} \\\\ &amp;= a_1(b_{11}+b_{21}+\\cdots+b_{n1}) \\\\ &amp; \\ \\ \\ \\ +a_2(b_{12}+b_{22}+\\cdots+b_{n2}) \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_m(b_{1m}+b_{2m}+\\cdots+b_{nm}) \\\\ &amp;= a_1b_{+ 1}+a_2b_{+ 2}+\\cdots+a_mb_{+ m} \\\\ &amp;=\\sum\\limits_{j=1}^{m}a_jb_{+ j} \\end{align*}\\] "],
["concerning-the-variance-parameter.html", "7 Concerning the Variance Parameter 7.1 Defining Variance With Expected Values 7.2 Unbiased Estimator 7.3 Computational Formulae", " 7 Concerning the Variance Parameter 7.1 Defining Variance With Expected Values In the case of a discrete random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\sum\\limits_{x=0}^{\\infty}(x-\\mu)^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2-2\\mu x+\\mu^2)p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2p(x)-2\\mu x\\cdot p(x)+\\mu^2p(x)) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\sum\\limits_{x=0}^{\\infty}2\\mu x\\cdot p(x) + \\sum\\limits_{x=0}^{\\infty}\\mu^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\sum\\limits_{x=0}^{\\infty}x\\cdot p(x) + \\mu^2\\sum\\limits_{x=0}^{\\infty}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2\\\\ \\end{align*}\\] In the case of a continuous random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}(x-\\mu)^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2-2\\mu x+\\mu^2)f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2f(x)-2\\mu x\\cdot f(x)+\\mu^2f(x))dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\int\\limits_{-\\infty}^{\\infty}2\\mu x\\cdot f(x)dx + \\int\\limits_{-\\infty}^{\\infty}\\mu^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\int\\limits_{-\\infty}^{\\infty}x\\cdot f(x)dx + \\mu^2\\int\\limits_{-\\infty}^{\\infty}f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2 \\end{align*}\\] In general, these results may be summarized as follows:\\ \\[\\begin{align*} \\sigma^2 &amp;= E[(X-\\mu)^2] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)] \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align*}\\] 7.2 Unbiased Estimator \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg) &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{n(\\sum\\limits_{i=1}^{n}x_i)^2}{n^2}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2)-nE(\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big]\\\\\\\\ &amp;= \\frac{1}{n}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;=\\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n}\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Since \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg)\\neq\\sigma^2\\) it is a biased estimator. Notice, however, that the bias can be eliminated by dividing by \\(n-1\\) instead of by \\(n\\) \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg) &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - 2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{(\\sum\\limits_{i=1}^{n}x_i)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{n\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n^2} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-nE(\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2)-nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big] \\\\ &amp;= \\frac{1}{n-1}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\ &amp;= \\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n-1}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n-1}\\sigma^2 \\\\ &amp;=\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Thus \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg)\\) is an unbiased estimator of \\(\\sigma^2\\), and we define the estimator \\[s^2= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\] 7.3 Computational Formulae 7.3.1 Computational Formula for \\(\\sigma\\)^2 \\[\\begin{align*} \\sigma^2 &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i^2-2\\mu x_i+\\mu^2)}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-\\sum\\limits_{i=1}^{N}2\\mu x_i + \\sum\\limits_{i=1}^{N}\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\mu\\sum\\limits_{i=1}^{N}x_i+N\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2 -2\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\sum\\limits_{i=1}^{N}x_i + N\\Big(\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\Big)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N} + \\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] 7.3.2 Computational Formula for \\(s\\)^2 \\[\\begin{align*} s^2 &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\sum\\limits_{i=1}^{n}2\\bar x x_i + \\sum\\limits_{i=1}^{n}\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\bar x\\sum\\limits_{i=1}^{n}x_i+n\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n}x_i + n\\Big(\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\Big)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n} + \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] "]
]
