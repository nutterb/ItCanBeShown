[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2016-08-10 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["bernoulli-distribution.html", "2 Bernoulli Distribution 2.1 Probability Mass Function 2.2 Cumulative Mass Function 2.3 Expected Values 2.4 Moment Generating Function 2.5 Theorems for the Bernoulli Distribution", " 2 Bernoulli Distribution 2.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} p^x(1-p)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(p\\) is the probability of a success. 2.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-p &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] Figure 2.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(p=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(p=.4\\). 2.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot p^{x} (1-p)^{1-x}\\\\ &amp;= 0 \\cdot p^{0} (1-p)^{1-0} + 1 \\cdot p^{1} (1-p)^{1-1}\\\\ &amp;= 0 + p (1-p)^{0}\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot p^x (1-p)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot p^0 (1-p)^{1-0} + 1^2 \\cdot p^1 (1-p)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot p \\cdot 1 \\\\ &amp;= 0 + p\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X) = p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p-p^2 \\\\ &amp;= p(1-p) \\end{align*} \\] 2.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\\\ &amp;= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}\\\\ &amp;= (1-p) + e^t p\\\\ &amp;=pe^t + (1-p) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p (1-p) \\end{align*} \\] 2.5 Theorems for the Bernoulli Distribution 2.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}p^x(1-p)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} p^x (1-p)^{1-x} &amp;= p^0 (1-p)^1 + p^1 (1-p)^0 \\\\ &amp;= (1-p) + p \\\\ &amp;= 1 \\end{align*}\\] 2.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p)) (pe^t+(1-p)) \\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-distribution.html", "3 Binomial Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function", " 3 Binomial Distribution 3.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(p\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} p^x (1-p)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(p\\) is the probability of a success on each individual trial. 3.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] Figure 3.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(p=.4\\). "]
]
