[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2016-08-14 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["analysis-of-variance.html", "2 Analysis of Variance 2.1 One-Way Design 2.2 Computational Formulas 2.3 Randomized Complete Block Design", " 2 Analysis of Variance 2.1 One-Way Design 2.1.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+} + \\bar x_{i+} - x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (x_{ij} - \\bar x{i+}) + (\\bar x_{i+} - \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++}) + (x_{ij} - \\bar x{i+}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + (x_{ij} - \\bar x_{i+})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\bigg(\\sum\\limits_{j=1}^{n_i} x_{ij} - \\sum\\limits_{j=1}^{n_i}\\bar x_{i+}\\bigg) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - n_i \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\big(x_{i+} - n_i \\frac{x_{i+}}{n_i}\\big) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\cdot 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2\\\\ \\end{align*}\\] The components are commonly referred to as \\[ SS_{Factor} = \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\] and \\[ SS_{Error} = \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\] Notice that \\(SS_{Factor}\\) compares the factor means to the overall mean, and it can be said that \\(SS_{Factor}\\) measures the variation between factors. \\(SS_{Error}\\) compares each observation to the overall mean, and can be said to describe the variation within factors. When \\(n_1 = n_2 = \\cdots n_i = n\\), the design is said to be balanced. 2.2 Computational Formulas \\(SS_{Total}\\) and \\(SS_{Factor}\\) can be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} x_{ij}^2 - x_{++} \\sum\\limits_{j=1}^{n_i}\\frac{1}{n_i}\\\\ \\end{align*}\\] See Theorem 16.3.1 \\[\\begin{align*} SS_{Factor} &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\frac{\\bar x_{i+}^2}{n_i} - \\bar x_{++} \\sum\\limits_{i=1}^{a}\\frac{1}{n_i} \\end{align*}\\] See Theorem 16.3.1 \\(SS_{Error}\\) does not simplify to a convenient form, but \\[\\begin{align*} SS_{Total} &amp;= SS_{Factor} + SS_{Error} \\\\ \\Rightarrow SS_{Error} &amp;= SS_{Total} - SS_{Factor} \\end{align*}\\] 2.3 Randomized Complete Block Design Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable. To implement this design, suppose we have \\(a\\) treatments we want to compare, and \\(b\\) blocks. We may analyze the data by use of the sums of squares, similar to the one-way design. 2.3.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} + \\bar x_{i+} - \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{+ j} + \\bar x_{++} - \\bar x_{+ +} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++}) + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++}) (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big] \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 + 0 + 0 + 0 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 + a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] It is shown that the cross products are equal to zero in Section 2.3.3 These terms are commonly referred to as \\[\\begin{align*} SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ SS_{Error} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] 2.3.2 Computational Formulae \\(SS_{Total}\\), \\(SS_{Factor}\\), and \\(SS_{Block}\\) can all be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} x_{ij}^2 - \\frac{x_{++}}{ab}\\\\ \\\\ SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{b}\\sum\\limits_{i=1}^{a}x_{i+}^2 - \\frac{x_{++}^2}{ab} \\\\ \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{a}\\sum\\limits_{j=1}^{b} x_{+ j}^2 - \\frac{x_{++}^2}{ab} \\end{align*}\\] See Theorem 16.3.1 \\(SS_{Error}\\) does not simplify to any convenient form, but may be calculated from the other terms as \\(SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}\\) 2.3.3 RCBD Cross Products The cross products of the RCBD design \\[\\begin{align*} 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp;= 0 \\end{align*}\\] Proof: \\[ 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\\\ \\ \\ = 2\\bigg(\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\bigg)\\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ \\bar x_{i+}\\bar x_{+ j} - \\bar x_{i+}\\bar x_{++} - \\bar x_{+ j}\\bar x_{++} + \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}\\bar x_{+ j} - \\bar x_{+ j}^2 + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}^2 - \\bar x_{i+}\\bar x_{+ j} + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}( -\\bar x_{++}^2 - \\bar x_{i+}^2 - \\bar x_{+ j}^2 + x_{ij}\\bar x_{i+} + x_{ij}\\bar x_{+ j} - 2 x_{ij}\\bar x_{++} - \\bar x_{i+}\\bar x_{+ j} \\\\ \\ \\ \\ \\ + 2\\bar x_{i+}\\bar x_{++} + 2\\bar x_{+ j}\\bar x_{++} ) \\\\ \\ \\ = 2\\bigg(-\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{++}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j} \\\\ \\ \\ \\ \\ - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2 x_{ij}\\bar x_{++} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ = 2\\bigg( \\frac{ab\\bar x_{++}^2}{a^2b^2} - \\frac{b}{b^2}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{a}{a^2}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[1]} =2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[2]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[3]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ ^{[4]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\] \\[ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[5]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\frac{2\\bar x_{++}^2}{ab} \\bigg)\\\\ \\ \\ = 2\\bigg(\\frac{4\\bar x_{++}^2}{ab} - \\frac{4\\bar x_{++}^2}{ab} + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\bigg)\\\\ \\ \\ = 2(0 + 0 + 0) \\\\ = 2(0) \\\\ = 0 \\] See Summation Theorem 15.1.7 See Summation Theorem 15.1.8 See Summation Theorem 15.1.4 See Summation Theorem 15.1.5 See Summation Theorem 15.1.6 Using the theorems in Chapter it is can be shown that each of the three cross products is equal to zero. However, the physical tedium of reducing each cross product is much greater than the approach taken above. "],
["bernoulli-distribution.html", "3 Bernoulli Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function 3.3 Expected Values 3.4 Moment Generating Function 3.5 Theorems for the Bernoulli Distribution", " 3 Bernoulli Distribution 3.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} p^x(1-p)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(p\\) is the probability of a success. 3.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-p &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] Figure 3.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(p=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(p=.4\\). 3.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot p^{x} (1-p)^{1-x}\\\\ &amp;= 0 \\cdot p^{0} (1-p)^{1-0} + 1 \\cdot p^{1} (1-p)^{1-1}\\\\ &amp;= 0 + p (1-p)^{0}\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot p^x (1-p)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot p^0 (1-p)^{1-0} + 1^2 \\cdot p^1 (1-p)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot p \\cdot 1 \\\\ &amp;= 0 + p\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X) = p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p-p^2 \\\\ &amp;= p(1-p) \\end{align*} \\] 3.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\\\ &amp;= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}\\\\ &amp;= (1-p) + e^t p\\\\ &amp;=pe^t + (1-p) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p (1-p) \\end{align*} \\] 3.5 Theorems for the Bernoulli Distribution 3.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}p^x(1-p)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} p^x (1-p)^{1-x} &amp;= p^0 (1-p)^1 + p^1 (1-p)^0 \\\\ &amp;= (1-p) + p \\\\ &amp;= 1 \\end{align*}\\] 3.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p)) (pe^t+(1-p)) \\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-distribution.html", "4 Binomial Distribution 4.1 Probability Mass Function 4.2 Cumulative Mass Function 4.3 Expected Values 4.4 Moment Generating Function 4.5 Maximum Likelihood Estimator 4.6 Theorems for the Binomial Distribution", " 4 Binomial Distribution 4.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(p\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} p^x (1-p)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(p\\) is the probability of a success on each individual trial. 4.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] Figure 4.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(p=.4\\). 4.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\ &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\ &amp;= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\ &amp;= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1} + \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\ &amp;= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x}) \\\\ ^{[2]} &amp;= np(p+q)^{n-1} \\\\ ^{[1]} &amp;= np(p+(1-p))^{n-1} \\\\ &amp;= np(p+1-p)^{n-1} \\\\ &amp;= np(1)^{n-1} \\\\ &amp;= np(1) \\\\ &amp;= np \\end{align*}\\] Let \\(q = (1 - p)\\) By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1} + \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n} \\\\ &amp;= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1} + 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big] \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\ &amp;= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\\\ ^{[2]} &amp;= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y} \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big] \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big] \\\\ ^{[3]} &amp;= np(mp+1) \\\\ &amp;= np[(n-1)p+1] \\\\ &amp;=np(np-p+1) \\\\ &amp;=n^2p^2 - np^2 + np \\end{align*}\\] \\(q = (1 - p)\\) Let \\(y = x - 1\\) and \\(n = m + 1\\) \\(\\Rightarrow\\) \\(x = y + 1\\) and \\(m = n - 1\\) \\(\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}\\) is of the form of the expected value of \\(Y\\), and \\(E(Y)=mp=(n-1)p\\). \\(\\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}\\) is the sum of all probabilities over the domain of \\(Y\\) which is 1. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n^2p^2 - np^2 + np - n^2p^2 \\\\ &amp;= -np^2 + np \\\\ &amp;= np(-p-1) \\\\ &amp;= np(1-p) \\end{align*}\\] 4.4 Moment Generating Function \\[ \\begin{align*} M_X(t) &amp;= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x} \\\\ ^{[1]} &amp;= [(1-p)+pe^t]^n \\end{align*} \\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[ \\begin{align*} M_X^{(1)}(t) &amp;= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\\\ \\\\ M_X^{(2)}(t) &amp;= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\\\ &amp;= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= n[(1-p)+pe^0]^{n-1}pe^0 \\\\ &amp;= n[1-p+p^{n-1}p\\\\ &amp;= n(1)^{n-1}p &amp;= np\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\\cdot0}[(1-p) + pe^0]^{n-2} \\\\ &amp;= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\\\ &amp;= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\\\ &amp;= np+n(n-1)p^2 \\\\ &amp;= np+(n^2-n)p^2 \\\\ &amp;= np + n^2 + n^2p^2 - np^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= np + n^2p^2 - np^2 - n^2p^2 \\\\ &amp;= np - np^2\\\\ &amp;= np(1-p) \\end{align*}\\] 4.5 Maximum Likelihood Estimator Since \\(n\\) is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for \\(n\\). The mean and variance can both be estimated from the single parameter \\(p\\). Let \\(X\\) be a Binomial random variable with parameter \\(p\\) and \\(n\\) outcomes \\((x_1,x_2,\\ldots,x_n)\\). Let \\(x_i=0\\) for a failure and \\(x_i=1\\) for a success. In other words, \\(X\\) is the sum of \\(n\\) Bernoulli trials with equal probability of success and \\(X=\\sum\\limits_{i=1}^{n}x_i\\). 4.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\ &amp;= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\ &amp;= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\ &amp;= \\theta^X(1-\\theta)^{n-X} \\end{align*}\\] 4.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln L(\\theta) \\\\ &amp;= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\ &amp;= X\\ln(\\theta)+(n-X)\\ln(1-\\theta) \\end{align*}\\] 4.5.3 MLE for p \\[\\begin{align*} \\frac{d\\ell(p)}{d p} &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\\\ \\\\ 0 &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\Rightarrow \\frac{X}{p} &amp;= \\frac{n-X}{1-p} \\\\ \\Rightarrow (1-p)X &amp;= p(n-X) \\\\ \\Rightarrow X-pX &amp;= np-pX \\\\ \\Rightarrow X &amp;= np \\\\ \\Rightarrow \\frac{X}{n} &amp;= p \\\\ \\end{align*}\\] So \\(\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(p\\). 4.6 Theorems for the Binomial Distribution 4.6.1 Validity of the Distribution \\[\\begin{align*} \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x} = 1 \\end{align*}\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\big(p + (1-p)\\big)^n \\\\ &amp;= (1)^n \\\\ &amp;= 1 \\end{align*}\\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) 4.6.2 Sum of Binomial Random Variables Let \\(X_1,X_2,\\ldots,X_k\\) be independent random variables where \\(X_i\\) comes from a Binomial distribution with parameters \\(n_i\\) and \\(p\\). That is \\(X_i\\sim(n_i,p)\\). Let \\(Y = \\sum\\limits_{i=1}{k} X_i\\). Then \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\ &amp;= \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\\\ &amp;= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i} \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(\\sum\\limits_{i=1}^{k}n_i\\) and \\(p\\). Thus \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). 4.6.3 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-theorem.html", "5 Binomial Theorem 5.1 Traditional Proof 5.2 General Approach 5.3 Other Theorems", " 5 Binomial Theorem The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions. Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach. Other useful theorems are provided at the end of this chapter. 5.1 Traditional Proof 5.1.1 Lemma: Pascal’s rule Let \\(n\\) and \\(x\\) be non-negative integers such that \\(x\\leq n\\). Then \\({n-1\\choose x} + {n-1\\choose x-1} = {n\\choose x}\\). Proof: \\[\\begin{align*} {n-1\\choose x} + {n-1\\choose x-1} &amp;= \\frac{(n-1)!}{x!(n-1-x)!} + \\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)!}\\\\ &amp;= \\frac{(n-1)!}{x(x-1)!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!} +\\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{n!}{x!(n-x)!} \\\\ &amp;= {n\\choose x} \\end{align*}\\] 5.1.2 The Binomial Theorem Let \\(a\\) and \\(b\\) be constants and let \\(n\\) be any positive integer. Then \\[(a+b)^n = \\sum\\limits_{x=0}^{n} {n\\choose x} a^{n-x} b^x\\] Proof: This proof is completed by mathematical induction. Base Step: \\(n=1\\) \\[\\begin{align*} (a+b)^1 &amp;= \\sum\\limits_{x=0}^{1} {1\\choose x} a^{1-x} b^x \\\\ &amp;= {1\\choose 0} a^{1-0} b^0 + {1\\choose 1} a^{1-1} b^1 \\\\ &amp;= 1\\cdot a\\cdot 1 + 1\\cdot 1\\cdot b \\\\ &amp;= a+b \\end{align*}\\] Inductive Step: Assume that the Theorem holds for \\(n\\), and show it is true for \\(n+1\\). \\[\\begin{align*} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= a(a+b)^n + b(a+b)^n \\\\ &amp;= a(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x + b^n) + b(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^n) \\\\ &amp;= (a^{n+1}+a\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^{n+1}) \\\\ &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x+1}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[1]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}a^{n-x+1}b^x) + (\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[2]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}{n\\choose x}a^{n-x+1}b^x) + \\sum\\limits_{x-1}^{n-1}{n\\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\\\ ^{[3]} &amp;= a^{n+1} + \\sum\\limits_{x+1}^{n}{n+1\\choose x}a^{n-x+1}b^x + b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{x=1}^{n}{n+1\\choose x}a^{(n+1)-x}b^x+b^{n+1} \\\\ ^{[4]} &amp;= \\sum\\limits_{x=0}^{n+1}{n+1\\choose x}a^{(n+1)-x}b^x \\end{align*}\\] This completes both the inductive step and the proof. \\(ab^n={n\\choose n}a^{n-n+1}b^n\\) which is the term for \\(x=n\\) in the first summation. \\(a^nb={n\\choose 0}a^{n-0}b^1\\) which is the term for \\(x=0\\) in the second summation. \\(\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-x+1}b^x\\) This step is made using Pascal’s Rule with \\(n=n-1\\). \\(a^{n+1}={n+1\\choose 0}a^{(n+1)-0}b^0\\) which is the term for \\(x=0\\) in the summation. \\(\\ \\ b^{n+1}={n+1\\choose n+1}a^{(n+1)-(n+1)}b^{n+1}\\) which is the term for \\(x=n+1\\) in the summation 5.2 General Approach 5.2.1 A Binomial Expansion Theorem This theorem and its corrolary are provided by Brunette. For any positive integer \\(n\\), let \\(B_n = (x_1+y_1) (x_2+y_2) \\cdots (x_n+y_n)\\). In the expansion \\(B_n\\), before combining possible like terms, the following are true: There will be \\(2^n\\) terms. Each of these terms will be a product of \\(n\\) factors. In each such product there will be one factor from each binomial (in \\(B_n\\)). Every such product of \\(n\\) factors, one from each binomial, is represented in the expansion. Proof: Proof is done by induction. For the case \\(n=1\\), the result is clear. Now assume that the theorem is true for a particular \\(n\\) and consider \\(B_{n+1}\\). \\[ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} \\] By the inductive assumption, \\(B_n = T_1 + T_2 + \\cdots + T_{2^n}\\) where each \\(T_i\\) is a product of \\(n\\) factors, one factor from each binomial. It follows that every term in the expansion of \\(B_n+1\\) is either of the type \\(T_ix_{n+1}\\) or \\(T_iy_{n+1}\\), for some \\(1\\leq i \\leq 2^n\\). But each term of either of the above types is clearly a product of \\(n+1\\) factors with one factor coming from each binomial. thus, if (ii) and (iii) are true for \\(B_n\\), then they are true for \\(B_n+1\\). Next, by the inductive assumption, the expansion of \\(B_n\\) is a sum of \\(2^n+2^n\\) terms, i.e., \\(2^{n+1}\\) terms. This completes the inductive step for (i). Lastly, it remains for us to consider a product of the type \\(p_1 p_2 \\cdots p_n p_{n+1}\\) where, for each \\(1\\leq i\\leq n+1\\), \\(p_i = x_i\\) or \\(p_i = y_i\\). By the inductive hypothesis, \\(p_1 p_2 \\cdots p_n\\) is a term in the expansion of \\(B_n\\). If \\(p_{n+1} = x_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_nx_{n+1}\\), and so of \\(B_{n+1}\\). Likewise, if \\(p_{n+1}=y_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_n y_{n+1}\\), and so of \\(B_{n+1}\\). This completes the inductive step and the proof. 5.2.2 Corollary: Binomial Theorem Let \\(x\\) and \\(y\\) be constants and let \\(n\\) be any positive integer. Then \\(\\displaystyle (x+y)^n = \\sum\\limits_{i=0}^{n} {n\\choose i} x^{n-i} y^i\\\\\\) Proof: Since each term in the expansion will have \\(n\\) terms, each term must follow the form \\(x^{n-i} y^i\\) for \\(0 \\leq i \\leq n\\), and in all, there are \\(2^n\\) such terms. For any given value of \\(i\\), the number of terms of the form \\(x^{n-i}y^i\\) is clearly the number of ways one can choose the \\(i\\) factors of \\(y\\) from the \\(n\\) available binomials, i.e., \\({n\\choose i}\\), which gives \\[(x+y)^n = \\sum\\limits_{i=0}^{n}{n\\choose i} x^{n-i} y^i\\] 5.3 Other Theorems 5.3.1 Theorem \\[{N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n}\\] where \\(0 \\leq n \\leq N_1 + N_2\\). Proof: Using the Binomial Theorem we establish \\[ (1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\\\ \\Rightarrow [{N_1\\choose 0}a^0+\\cdots+{N_1\\choose N_1}a^{N_1}]\\cdot [{N_2\\choose 0}a^0+\\cdots+{N_2\\choose N_2}a^{N_2}] \\\\ \\ \\ \\ \\ ={N_1+N_2\\choose 0}+{N_1+N_2\\choose 1}a+\\cdots +{N_1+N_2\\choose N_1+N_2}a^{N_1+N_2} \\] Expanding the left side of the equation gives \\[ {N_1\\choose 0}{N_2\\choose 0} + {N_1\\choose 0}{N_2\\choose 1}a + \\cdots + {N_1\\choose 0}{N_2\\choose N_2}a^{N_2} + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose 1}{N_2\\choose N_2}a^{N_2+1} + \\cdots + {N_1\\choose N_1}{N_2\\choose 0}a^{N_1} + {N_1\\choose N_1}{N_2\\choose 1}a^{N_1+1} \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\\\ = {N_1\\choose 0}{N_2\\choose 0}+{N_1\\choose 0}{N_2\\choose 1}a + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + {N_1\\choose 0}{N_2\\choose 2}a^2+{N_1\\choose 1}{N_2\\choose 1}a^2 + {N_1\\choose 2}{N_2\\choose 0}a^2 \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\] Notice that for any \\(n\\) where \\(0 \\leq n \\leq N_1 + N_2\\), the coefficient for \\(a^n\\), found by combining like terms, is \\({N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 1}{N_2\\choose n-1} + \\cdots+{N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose 0}{N_2\\choose n}\\) and, by the equivalence of the first equation in the proof, is equal to the coefficient \\({N_1 + N_2\\choose n}\\). 5.3.2 Theorem \\[\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}} = 1\\] for \\(0 \\leq n \\leq N_1 + N_2\\).\\ Proof: Theorem 5.3.1 establishes the equality \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\frac{\\sum\\limits_{i=1}^{n} {N_1\\choose i}{N_2\\choose n-i}} {{N_1+N_2\\choose n}} = 1 \\] "],
["chebychevs-theorem.html", "6 Chebychev’s Theorem 6.1 Chebychev’s Theorem 6.2 Alternate Proof of Chebychev’s Theorem 6.3 Chebychev’s Theorem for Absolute Deviation", " 6 Chebychev’s Theorem 6.1 Chebychev’s Theorem In any finite set of numbers and for any real number \\(h &gt; 1\\), at least \\((1 - \\frac{1}{h^2}) \\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Under these conditions we know \\[|x_1-\\mu| &gt; h\\sigma,\\ |x_2-\\mu| &gt; h\\sigma, \\ldots,\\ |x_r-\\mu| &gt; h\\sigma\\] Squaring gives \\[(x_1-\\mu)^2 &gt; h^2\\sigma^2,\\ (x_2-\\mu)^2 &gt; h^2\\sigma^2,\\ldots,\\ (x_r-\\mu)^2 &gt; h^2\\sigma^2\\\\ \\ \\ \\ \\ \\Rightarrow\\sum\\limits_{i=1}^{r}(x_1-\\mu)^2 &gt; \\sum\\limits_{i=1}^{r}h^2\\sigma^2 = rh^2\\sigma^2 \\] Since all \\((x_i-\\mu)^2\\) must necessarily be positive, \\[\\begin{align*} \\sum\\limits_{i=1}^{r}(x_i-\\mu)^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2\\sigma^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ ^{[1]} \\Rightarrow rh^2\\sigma^2 &amp;&lt; n\\sigma^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2 &amp;&lt; n \\\\ \\ \\ \\ \\ \\Rightarrow\\frac{r}{n} &amp;&lt; \\frac{1}{h^2} \\end{align*}\\] \\(\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) \\(\\ \\ \\ \\ \\Rightarrow n\\sigma^2 = \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) and \\(\\frac{r}{n}\\) is the fraction of numbers outside \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). By the law of complements, the fraction of numbers inside the interval is \\(1 - \\frac{r}{n}\\), which implies \\(1 - \\frac{r}{n} &gt; 1 - \\frac{1}{h^2}\\). Thus, more than \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the points lie within \\(h\\) standard deviations of the mean, or within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). 6.2 Alternate Proof of Chebychev’s Theorem In any finite set of numbers and for any real number \\(h&gt;1\\), at least \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma,\\mu+h\\cdot\\sigma)\\).\\ Proof: The proof here is done for the discrete case, but is applicable also in the continuous case by replacing the summations with integrals (with integrals, the limits will be from \\(-\\infty\\) to \\(\\infty\\)). \\[\\begin{align*} \\sigma^2 &amp;= E(x-\\mu)^2 \\\\ &amp;= \\sum\\limits_{y=0}^{\\infty}(y-\\mu)^2p(y) \\\\ &amp;= \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu-h\\sigma+1}^{\\mu+h\\sigma-1}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y) \\\\ ^{[1]} \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y)\\\\ \\end{align*}\\] Since all the \\((y-\\mu)^2\\) must be positive, removing the middle term will surely result in this inequality. In both of these summations \\(y\\) is outside the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\), so \\[\\begin{align*} |y-\\mu| &amp;\\geq h\\sigma \\\\ \\Rightarrow (y-\\mu^2) &amp;\\geq h^2\\sigma^2 \\\\ \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}h^2\\sigma^2p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}h^2\\sigma^2p(y) \\\\ \\Rightarrow\\sigma^2 &amp;\\geq h^2\\sigma^2\\Big[\\sum\\limits_{y=0}^{\\mu-h\\sigma}p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}p(y)\\Big] \\end{align*}\\] The first summation is the sum of all probabilities that \\(y-\\mu &lt; h\\sigma\\), i.e. \\(P(y-\\mu &lt; h\\sigma)\\). Likewise, the second summation is \\(P(y-\\mu &gt; h\\sigma)\\). \\[\\begin{align*} \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(y-\\mu&lt;h\\sigma) + P(y-\\mu&gt;h\\sigma)] \\\\ \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(|y-\\mu|&gt;h\\sigma)] \\\\ \\Rightarrow \\frac{1}{h^2} &amp;\\geq P(|y-\\mu|&gt;h\\sigma) \\\\ \\Rightarrow 1-\\frac{1}{h^2} &amp;\\leq P(|y-\\mu|&gt;h\\sigma) \\end{align*}\\] 6.3 Chebychev’s Theorem for Absolute Deviation This theorem is provided by Brunette In any finite set of numbers, and for any real number \\(h &gt; 1\\), at least \\(1 - \\frac{1}{h}\\) of the numbers lie within \\(h\\) absolute deviations of the mean, where the absolute deviation is defined \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}{n}|x_i-\\bar x|\\). In other words, \\(1-\\frac{1}{h}\\) of the numbers are in the interval \\((\\bar x-h\\cdot Ab , \\bar x+h\\cdot Ab)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Accordingly, \\[h \\cdot Ab \\leq |x_1-\\bar x| ,\\ h \\cdot Ab \\leq |x_1-\\bar x| ,\\ldots ,\\ h \\cdot Ab \\leq |x_1-\\bar x| \\] \\[\\begin{align*} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{r}|x_i-\\bar x| \\\\ \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{n}|x_i-\\bar x| \\\\ ^{[1]} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq n \\cdot Ab\\\\ \\Rightarrow \\frac{r}{n} &amp;\\leq \\frac{1}{h}\\\\ \\Rightarrow -\\frac{r}{n} &amp;\\geq -\\frac{1}{h}\\\\ \\Rightarrow 1-\\frac{r}{n} &amp;\\geq 1-\\frac{1}{h} \\end{align*}\\] \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) \\(\\Rightarrow n \\cdot Ab = \\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) Now \\(\\frac{r}{n}\\) is the fraction of numbers outside the interval. So \\(1-\\frac{r}{n}\\) is the fraction of numbers within \\(h\\) absolute deviations of the mean, or within the interval \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). "],
["chi-square-distribution.html", "7 Chi-Square Distribution 7.1 Probability Distribution Function 7.2 Cumulative Distribution Function 7.3 Expected Values 7.4 Moment Generating Function 7.5 Maximum Likelihood Function 7.6 Theorems for the Chi-Square Distribution", " 7 Chi-Square Distribution 7.1 Probability Distribution Function A random variable \\(X\\) is said to have a Chi-Square Distribution with parameter \\(\\nu\\) if its probability distribution function is \\[f(x) = \\left\\{ \\begin{array}{ll} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} &amp; 0&lt;x,\\ 0&lt;\\nu\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\(\\nu\\) is commonly referred to as the degrees of freedom. 7.2 Cumulative Distribution Function The cumulative distribution function for the Chi-Square Distribution cannot be written in closed form. It’s integral form is expressed as \\[ F(x) = \\left\\{ \\begin{array}{ll} \\displaystyle\\int\\limits_{0}^{x} \\frac{t^{\\frac{\\nu}{2}-1}e^{-\\frac{t}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} dt &amp; 0&lt;x,\\ 0&lt;\\nu\\\\\\\\ 0 &amp; otherwise \\end{array} \\right. \\] Figure 7.1: The graphs on the top and bottom depict the Chi-Square probability distribution and cumulative distribution functions, respectively, for \\(\\nu=4,7,10\\). As \\(\\nu\\) gets larger, the distribution becomes flatter with thicker tails. 7.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma\\Big(\\frac{\\nu}{2}+1\\Big)2^{\\frac{\\nu}{2}+1}\\Big] \\\\ &amp;= \\frac{\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{2\\nu}{2} \\\\ &amp;= \\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}+1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma(\\frac{\\nu}{2}+2)2^{\\frac{\\nu}{2}+2}\\Big] \\\\ &amp;= \\frac{\\Gamma\\Big(\\frac{\\nu}{2}+2\\Big)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{(\\frac{\\nu}{2}+1)\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\cdot 2^2=2\\Big(\\frac{\\nu}{2}+1\\Big)\\nu \\\\ &amp;= (\\nu+2)\\nu=\\nu^2+2\\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}e^{tx}\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{2tx}{2}-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-\\frac{2tx-x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{-2t+1}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{1-2t}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{-x}{\\frac{2}{1-2t}}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Big(\\frac{2}{1-2t}\\Big)^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big]\\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= (1-2t)^{-\\frac{\\nu}{2}} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\frac{\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1}(-2) \\\\ &amp;= \\frac{2\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= (-\\frac{\\nu}{2}-1)\\nu(1-2t)^{-\\frac{\\nu}{2}-2}(-2) \\\\ &amp;= (\\frac{2\\nu}{2}+2)\\nu(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu+2)\\nu)(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-2t)^{-\\frac{\\nu}{2}-2}\\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= \\nu(1-2\\cdot 0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu \\\\ M_X^{(2)}(0) &amp;= (\\nu^2+2\\nu)(1-2\\cdot 0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\nu\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.5 Maximum Likelihood Function Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from a Chi-square distribution with parameter \\(\\nu\\). 7.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\nu/2-1}e^{-x_1/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdot \\frac{x_2^{\\nu/2-1}e^{-x_2/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdots \\frac{x_n^{\\nu/2-1}e^{-x_n/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\nu/2-1}e^{-x_i/2}} {2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}e^{-x_i/2} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\sum\\limits_{i=1}^{n}\\frac{x_i}{2} \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\end{align*}\\] 7.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\big(L(\\theta)\\big) \\\\ &amp;= \\ln\\Bigg[ \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\Bigg] \\\\ &amp;= \\ln\\Bigg[ \\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) \\Bigg] + \\ln\\Bigg( \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\Bigg) + \\ln\\bigg(\\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}\\bigg) \\\\ &amp;= -n \\ln\\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i \\bigg) \\\\ &amp;= -n\\bigg( \\ln(2^{\\nu/2}) + \\Gamma\\Big(\\frac{\\nu}{2}\\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\bigg(\\frac{\\nu}{2} \\ln 2 + \\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n\\nu}{2} \\ln 2 - n\\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 7.5.3 MLE for \\(\\nu\\) \\[\\begin{align*} \\frac{d\\ell}{d\\nu} &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) \\cdot \\frac{1}{2} + 0 + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\Rightarrow \\frac{n}{2} \\ln 2 - \\frac{1}{2}\\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow n\\ln 2 - \\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln 2}{n} &amp;= \\frac{\\Gamma^\\prime\\big(\\frac{\\nu}{2}\\big)}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\end{align*}\\] Due to the complexity of the Gamma function in this equation, no solution can be developed for \\(\\nu\\) in closed form. Thus, we have to rely on numerical methods to obtain a solution to the equation and find the maximum likelihood estimator. 7.6 Theorems for the Chi-Square Distribution 7.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big] \\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}{2^{\\frac{\\nu}{2}} \\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 7.6.2 Sum of Chi-Square Random Variables Let \\(X_1 , X_2 , \\ldots , X_n\\) be independent Chi-Square random variables with parameter \\(\\nu_i\\), that is \\(X_i\\sim\\chi^2(\\nu_i),\\ i=1,2,\\ldots,n\\). Suppose \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\chi^2(\\sum\\limits_{i=1}^{n}\\nu_i)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}=E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-2t)^{-\\frac{\\nu_1}{2}}(1-2t)^{-\\frac{\\nu_2}{2}}\\cdots (1-2t)^{-\\frac{\\nu_n}{2}} \\\\ &amp;= (1-2t)^{\\sum\\limits_{i=1}^{n}\\nu_i} \\end{align*}\\] Which is the mgf of a Chi-Square random variable with parameter \\(\\sum\\limits_{i=1}^{n}\\nu_i\\). Thus \\(Y\\sim\\chi^2\\bigg(\\sum\\limits_{i=1}^{n}\\nu_i\\bigg)\\). 7.6.3 Square of a Standard Normal Random Variable If \\(Z\\sim N(0,1)\\), then \\(Z^2\\sim\\chi^2(1)\\). Proof: \\[\\begin{align*} M_{Z^2}(t) &amp;= E(e^{tZ^2}) \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}e^{tz^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty}e^{tz^2} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(-2t+1)}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[1]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[2]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}e^{-u} \\frac{\\sqrt{2}u^{-\\frac{1}{2}}}{2(1-2t)^{\\frac{1}{2}}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}e^{-u}u^{-\\frac{1}{2}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-u}du \\\\ ^{[3]} &amp;= \\frac{1}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}}\\Gamma(\\frac{1}{2}) \\\\ &amp;= \\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{1}{2}}}=(1-2t)^{-\\frac{1}{2}} \\\\ \\end{align*}\\] \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\) when f(x) is an even function () Let \\(u=\\frac{z^2}{2}(1-2t) \\ \\ \\ \\ \\Rightarrow z=\\frac{\\sqrt{2}u^{\\frac{1}{2}}}{(1-2t)^{\\frac{1}{2}}}\\) So \\(dz=\\frac{\\sqrt{2}u^{-\\frac{1}{2}}} {2(1-2t)^{\\frac{1}{2}}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) Which is the mgf of a Chi-Square random variable with 1 degree of freedom. Thus \\(Z^2\\sim\\chi^2(1)\\). "],
["combinations.html", "8 Combinations", " 8 Combinations 8.0.1 Lemma A set of \\(n\\) elements may be partitioned into \\(m\\) distinct groups containing \\(k_1 , k_2 , \\ldots , k_m\\) objects, respectively, where each object appears in exactly one group and \\(\\sum\\limits_{i=1}^{m}k_i=n\\), in \\(\\displaystyle N={n\\choose k_1k_2\\ldots k_m}=\\frac{n!}{k_1!k_2!\\ldots k_m!}\\) ways.\\ Proof: \\(N\\) is the number of ways all \\(n\\) of the elements of the set can be arranged in \\(m\\) groups where the order within each group is not important (i.e. rearrangements of elements in a group do not qualify as distinct groups). The number of distinct arrangements of the \\(n\\) elements in which the order of selection is important, \\(P_k^n\\), is equal to \\(N\\) multiplied by the number of ways each individual group of \\(k_i\\) can be selected in which the order is important, i.e. \\[\\begin{align*} P_n^n &amp;= N \\cdot P_{k_1}^{k_1} P_{k_2}^{k_2} \\cdots P_{k_m}^{k_m} \\\\ \\Rightarrow n! &amp;= N \\cdot k_1! k_2! \\cdots k_m! \\\\ \\Rightarrow N &amp;= \\frac{n!}{k_1! k_2! \\cdots k_m!} \\end{align*}\\] 8.0.2 Combinations Theorem Given a set of \\(n\\) elements, the number of possible ways to select a subset of size \\(k\\), without regard to the order of their selection, is \\(\\frac{n!}{k!(n-k)!}\\).\\ Proof: This theorem is a special case of the Lemma with \\(n=n\\), \\(m=2\\), \\(k_1=k\\) and \\(k_2=n-k\\). thus, \\[\\displaystyle N=\\frac{n!}{k!(n-k)!}\\] The formula \\(\\displaystyle \\frac{n!}{k!(n-k)!}\\) is denoted in a number of ways, depending on the author. Denotations may be \\(C_k^n\\), \\(_nC_k\\), \\(C_{n,k}\\), \\(C(n,k)\\), and \\({n\\choose k}\\). Throughout this book, the form \\({n\\choose k}\\) is used and may be read “\\(n\\) choose \\(k\\) objects.” 8.0.3 Theorem For any integer \\(a\\) such that \\(0\\leq a\\leq k\\), \\[ {n\\choose k} = \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)}{n-a\\choose k-a} \\] Proof: \\[\\begin{align*} {n\\choose k} &amp;= \\frac{n!}{k!(n-k)!} \\\\ &amp;= \\frac{n(n-1)!}{k(k-1)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)!}{k(k-1)(k-2)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)(n-a)!}{k(k-1)(k-2)\\cdots(k-a+1)(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-a+a-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)+(a-k)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)-(k-a)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot {n-a\\choose k-a} \\end{align*}\\] "],
["correlation-pearsons.html", "9 Correlation (Pearson’s) 9.1 Theorems on Pearson’s Correlation 9.2 Computational Formula for \\(\\rho\\)", " 9 Correlation (Pearson’s) Pearson’s correlation coefficient of the variables \\(X\\) and \\(Y\\) is a measure of the linear relationship between \\(X\\) and \\(Y\\). It is defined \\[\\rho = \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\cdot \\sigma_Y^2}}\\] Notice that if \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y,)=0\\) and \\(\\rho=0\\) and there is no linear relationship between the variables. 9.1 Theorems on Pearson’s Correlation 9.2 Computational Formula for \\(\\rho\\) \\[\\rho = \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\sum\\limits_{i=1}^{n}(x_i-\\mu_X)\\sum\\limits_{j=1}^{m}(y_i-\\mu_Y)}\\] Proof: \\[\\begin{align*} \\rho &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)\\frac{1}{N}} {\\sqrt{\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2}{N}\\frac{\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}{N}}} \\\\ &amp;= \\frac{\\frac{1}{N}\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\frac{1}{N}\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)} {\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)}} \\end{align*}\\] "],
["covariance.html", "10 Covariance 10.1 Definition of Covariance 10.2 Theorems on Covariance", " 10 Covariance 10.1 Definition of Covariance For any two random variables \\(X\\) and \\(Y\\), the covariance of \\(X\\) and \\(Y\\) is defined as \\[Cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)]\\] 10.2 Theorems on Covariance 10.2.1 Theorem Let \\(X\\) be a random variable. Then \\[Cov(X,X) = V(X)\\] Proof: \\[\\begin{align*} Cov(X,X) &amp;= E[(X-\\mu)(X-\\mu)] \\\\ &amp;= E[(X-\\mu)^2] \\\\ &amp;= V(X) \\end{align*}\\] 10.2.2 Theorem Let \\(X\\) and \\(Y\\) be random variables. Then \\[Cov(X,Y) = E(XY)-E(X)E(Y)\\] Proof: \\[\\begin{align*} Cov(X,Y) &amp;= E[(X-\\mu_x)(Y-\\mu_Y)] \\\\ &amp;= E[XY - X\\mu_y - Y\\mu_X + \\mu_X\\mu_Y] \\\\ &amp;= E(XY) - E(X)\\mu_Y - \\mu_XE(Y) + \\mu_X\\mu_Y \\\\ &amp;= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - 2E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - E(X)E(Y) \\end{align*}\\] 10.2.3 Covariance2.3 Let \\(X\\) and \\(Y\\) be random variables and let \\(a\\) and \\(b\\) be constants. Then \\[Cov(aX,bY) = abCov(X,Y)\\] Proof: \\[\\begin{align*} Cov(aX,bY) &amp;= E(aXbY) - E(aX)E(bY) \\\\ &amp;= abE(XY) - abE(X)E(Y) \\\\ &amp;= ab[E(XY) - E(X)E(Y)] \\\\ &amp;= abCov(X,Y) \\end{align*}\\] 10.2.4 Theorem Let \\(X_1 , X_2 , \\ldots , X_n\\) be random variables with \\(E(X_i) = \\mu_i\\) for \\(i = 1,2,\\ldots,n\\) and let \\(Y_1,Y_2,\\ldots,Y_m\\) be random variables with \\(E(Y_j) = \\phi_j\\) for \\(j=1,2,\\ldots,m\\). Also, let \\(a_1,a_2,\\ldots,a_n\\) and \\(b_1,b_2,\\ldots,b_m\\) be constants.\\ If \\(U_1 = \\sum\\limits_{i=1}^{n}a_iX_i\\) and \\(U_2 = \\sum\\limits_{i=1}^{m}b_iY_i\\), then \\[Cov(U_1,U_2) = \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jCov(X_i,Y_j)\\] Proof: \\[\\begin{align*} Cov(U_1,U_2) &amp;= E[(U_1-E(U_1))(U_2-E(U_2))] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-\\sum\\limits_{i=1}^{n}a_i\\mu_i\\big) \\big(\\sum\\limits_{j=1}^{m}b_jY_j-\\sum\\limits_{j=1}^{m}b_j\\phi_j\\big)\\Big] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_i(X_i-\\mu_i)\\big)\\big(\\sum\\limits_{j=1}^{m}b_j(Y_j-\\phi_j)\\big)\\Big] \\\\ &amp;= E\\Big[\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j(X_i-\\mu_i)(Y_j-\\phi_j)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jE[(X_i-\\mu_i)(Y_j-\\phi_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j\\ Cov(X_i,Y_j) \\end{align*}\\] 10.2.5 Theorem Let \\(X_1,X_2,\\ldots,X_n\\) be random variables with \\(E(X_i)=\\mu_i\\) for \\(i=1,2,\\ldots,n\\) and let \\(a_1,a_2,\\ldots,a_n\\) be constants. If \\(Y = \\sum\\limits_{i=1}^{n}a_iX_i\\) then \\[V(Y) = \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ }a_ia_jCov(X_i,X_j)\\] Proof: \\[\\begin{align*} V(Y) &amp;= E[(Y-\\mu_Y)^2] \\\\ &amp;= E[(Y-\\mu_Y)(Y-\\mu_Y)] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-a_i\\mu_i\\big) \\big(\\sum\\limits_{n=1}^{n}a_jX_j-a_j\\mu_j\\big)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2Cov(X_i,X_i)+ \\sum\\limits_{\\ \\ i\\neq}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+ 2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jCov(X_i,X_j) \\end{align*}\\] "],
["experimental-designs.html", "11 Experimental Designs 11.1 Designs in Categorical Data Analysis", " 11 Experimental Designs 11.1 Designs in Categorical Data Analysis Studies in Categorical Data Analysis can be classified into several designs. These designs fall into the following two categories: Retrospective Design: looks at and analyzes measurements that have already been taken. Prospective Design: specifies the measurements to be collected at a future time. 11.1.1 Case Control Study In case control studies, the marginal distribution of the response variable is fixed by the sampling design. In other words, researchers select particular numbers of each category of the response variable in order to ensure that enough of each case are included in the sample. The result is that the marginal distribution of the response is non-random. Unfortunately, in order to calculate conditional probabilities, the marginal distribution of interest must be random. The difference of proportions for the response and the relative risk are both based on the marginal distribution of the response, and are both invalid procedures in case-control studies. In taking the measurements, researchers idenitfy people who are already classified into the response variable, making the design retrospective. 11.1.2 Cross Sectional Study 11.1.3 Cohort Study In Cohort Studies, subjects make their own choice about which group in the explanatory variable to join and researchers monitor the subjects with respect to a response variable over a period of time. Both the explanatory and response variables are random and only the total sample size is fixed by the researcher. Thus, conditional probabilities may be computed for both the predictor and response variables; differences in proportions may be estimated; and the relative risk is defined for the response variable. Since subjects select the group in which they will be and a measurement of their response is taken later, cohort studies are prospective. 11.1.4 Randomized Study In randomized Studies, the researcher randomly assigns subjects to the explanatory variable and then observes their response (making this a prospective study). The marginal distribution of the explanatory variable is therefore fixed, and conditional probabilities may not be computed. The response variable, on the other hand, is random and conditional probabilites may be computed, as well as the difference of proportions and relative risk. 11.1.5 Summary of Designs "],
["exponential-distribution.html", "12 Exponential Distribution 12.1 Probability Distribution Function 12.2 Cumulative Distribution Function 12.3 Expected Values 12.4 Moment Generating Function 12.5 Maximum Likelihood Estimator 12.6 Theorems for the Exponential Distribution", " 12 Exponential Distribution 12.1 Probability Distribution Function A random variable is said to have an Exponential Distribution with parameter \\(\\beta\\) if its probability distribution function is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{\\beta}e^{\\frac{-x}{B}}, &amp; 0&lt;x,\\ \\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 12.2 Cumulative Distribution Function \\[\\begin{align*} F(x) &amp;= \\int\\limits_{0}^{x}\\frac{1}{\\beta}\\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}dt \\\\ &amp;= \\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}\\Big|_0^x \\\\ &amp;= \\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\}-\\exp\\Big\\{{\\frac{-0}{\\beta}}\\Big\\} \\\\ &amp;= \\exp\\Big\\{{\\frac{0}{\\beta}}\\Big\\}-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\\\ &amp;= 1-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\end{align*}\\] And so the cumulative distribution function is given by \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{\\frac{-x}{\\beta}}, &amp; 0&lt;x,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:Exponential_Distribution)The figures on the top and bottom display the Exponential probability and cumulative distirubtion functions, respectively, for \\(\\beta=1,3\\). 12.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}xf(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}xe^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{2-1}e^{\\frac{-x}{\\beta}}dx\\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^2\\Gamma(2)) \\\\ &amp;=\\frac{\\beta^2\\cdot 1!}{\\beta} \\\\ &amp;=\\beta \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2f(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^2e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{3-1}e^{\\frac{-x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^3\\Gamma(3)) \\\\ &amp;= \\frac{\\beta^3\\cdot 2!}{\\beta} \\\\ &amp;= 2\\beta^2 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= 2\\beta^2-\\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 12.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx}{\\beta}-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{-x(\\beta 1-\\beta t}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}(\\frac{-\\beta}{1-\\beta t})e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}\\cdot 0-\\frac{-1}{1-\\beta t}e^0 \\\\ &amp;= \\frac{1}{1-\\beta t}=(1-\\beta t)^{-1} \\\\ \\\\ \\\\ M_X^{(1)}(t) &amp;= -1(1-\\beta t)^{-2}(-\\beta) \\\\ &amp;= \\beta(1-\\beta t)^{-2} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= -2\\beta(1-\\beta t)^{-3}(-\\beta) \\\\ &amp;= 2\\beta^2(1-\\beta t)^{-3} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\beta(1-\\beta\\cdot 0)^{-2} \\\\ &amp;= \\beta(1-0)^{-2} \\\\ &amp;= \\beta(1)^{-2} \\\\ &amp;= \\beta \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= 2\\beta^2(1-\\beta\\cdot 0)^{-3} \\\\ &amp;= 2\\beta^2(1-0)^{-3} \\\\ &amp;= 2\\beta^2(1)^{-3} \\\\ &amp;= 2\\beta^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= 2\\beta^2 - \\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 12.5 Maximum Likelihood Estimator Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from an Exponential distribution with parameter \\(\\beta\\). 12.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= f(x_1|\\theta)f(x_2|\\theta)\\cdots f(x_n|\\theta)\\\\ &amp;= \\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_1}{\\theta}\\bigg\\} \\cdot\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\cdots\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\\\ &amp;= \\frac{1}{\\theta^n}\\exp\\bigg\\{-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\end{align*}\\] 12.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln(L(\\theta)) \\\\ &amp;= \\ln(1)-n\\ln(\\theta)-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= 0-n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= -n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] 12.5.3 MLE for \\(\\beta\\) \\[\\begin{align*} \\frac{d\\ell(\\beta)}{d\\beta} &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n}{\\beta} &amp;= \\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n\\beta^2}{\\beta} &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow n\\beta &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] So \\(\\hat\\beta=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(\\beta\\). 12.6 Theorems for the Exponential Distribution 12.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{B}}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx &amp;= -e^{\\frac{-x}{\\beta}}\\Big|_0^\\infty \\\\ &amp;= -e^{\\frac{-\\infty}{\\beta}}-(-e^{\\frac{-0}{\\beta}}) \\\\ &amp;= e^{\\frac{0}{\\beta}}-e^{\\frac{-\\infty}{\\beta}} \\\\ &amp;= 1-0 \\\\ &amp;= 1 \\end{align*}\\] 12.6.2 Sum of Exponential Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from an Exponential distribution with parameter \\(\\beta\\), i.e. \\(X_i\\sim\\)Exponential\\((\\beta)\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Gamma\\((n,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-1}(1-\\beta t)^{-1}\\cdots(1-\\beta t)^{-1} \\\\ &amp;= (1-\\beta t)^{-n} \\end{align*}\\] Which is the mgf for a Gamma random variable with parameters \\(n\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((n,\\beta)\\). "],
["functions.html", "13 Functions 13.1 Fundamental Concepts and Definitions 13.2 Identities and Inverses 13.3 Odd and Even Functions 13.4 Theorems", " 13 Functions 13.1 Fundamental Concepts and Definitions Much of this chapter is taken from the lectures of Dr. John Brunette, University of Southern Maine A function is a collection of ordered pairs in which no two pairs have the same first element. The set of all first members of the pairs is called the domain. The set of all second members of the pairs is called the range. Suppose now that for any function \\(f\\) we have two items \\(x\\) and \\(y\\) such that \\(x\\in dom(f)\\) and \\(y\\in ran(x)\\) where \\(dom(f)\\) and \\(ran(f)\\) denote the domain and range of \\(f\\), respectively. It is said that \\(f\\) maps \\(x\\) onto \\(y\\), written \\[f:\\ x\\mapsto y\\] It is common to write the \\(ran(f)\\) as some expression of \\(x\\). For example, \\(f: x\\mapsto x^2\\) takes each element in the domain, and pairs it with it’s square. The common shorthand for this is \\(f(x)=x^2\\), meaning that whatever appears between the parentheses following the \\(f\\) is to be squared. 13.1.1 Function Operations The three basic operations that can be performed on functions are addition, multipilication, and composition. For any two functions \\(f\\) and \\(g\\) these operations are defined as: Addition \\(\\lbrack f+g\\rbrack(x)=:\\big\\{\\big(x,f(x)+g(x)\\big)\\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Multiplication \\(\\lbrack f\\cdot g\\rbrack(x):=\\big\\{\\big(x,f(x)\\cdot g(x)\\big) \\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Composition \\(\\lbrack f\\circ g\\rbrack(x)=\\big\\{\\big(x,f\\big(g(x)\\big) \\mid x\\in dom(g)\\) and \\(g(x)\\in dom(f)\\big\\}\\) Notice that the composition \\(\\lbrack f\\circ g\\rbrack(x)=f \\circ g: g(x)\\mapsto f(x)\\). In other words, the result of \\(g\\) is then applied to \\(f\\) to produce the result of the composition. 13.2 Identities and Inverses Recall that addition and multiplication have identity properties. Specifically, for any real number \\(x\\), applying one of these identities returns the value \\(x\\), i.e. \\(x+0=x\\) and \\(x\\cdot 1\\)=x. Functions also have an identity, denoted \\(id(x)\\), that is defined as \\[id:\\ x\\mapsto x\\] Furthermore, the composition of \\(id\\) with \\(f\\) behaves in this way: \\[id\\circ f=f\\circ id=f\\] Functions may also exhibit the property of inverses that are exhibited by addition and multiplication. In the latter two, combining any real number \\(x\\) and its inverse returns the identity of that operation, i.e. \\(x+-x=0\\) and \\(x\\cdot x^{-1}=1,\\ x\\neq 0\\). Likewise, some functions have an inverse function. If a function \\(f\\) has an inverse \\(f^{-1}\\), then \\[f\\circ f^{-1}=f^{-1}\\circ f=id\\] On closer observation, we see \\[f^{-1}\\circ f\\big(dom(x)\\big)=f^{-1}\\Big(f\\big(dom(x)\\big)\\Big)=f^{-1}\\big(ran(x)\\big)=dom(x)\\] So \\(f^{-1}\\) must be the set of all ordered pairs \\((y,x)\\) where \\(x\\in dom(x)\\) and \\(y\\in ran(x)\\), i.e. \\(f^{-1}(x)=\\{(y,x) \\mid x\\in dom(x)\\) and \\(y\\in ran(x)\\}\\). By the definition of functions, no two first elements in \\(f^{-1}\\) can be the same. But the first elements in \\(f^{-1}\\) are the second elements in \\(f\\). So \\(f^{-1}\\) only exists if no two second elements in \\(f\\) are the same. We thus make the following definition: A function \\(f\\) is called a one-to-one function if it has no two ordered pairs with the same second element. For any one-to-one function \\(f\\), no two of the first elements are the same, and no two of the second elements are the same. Thus, \\(f^{-1}\\) is a function, because no two of its first elements are the same, and because the range of \\(f^{-1}\\) is the domain of \\(f\\), no two second elements in \\(f^{-1}\\) are the same, and \\(f^{-1}\\) is a one-to-one function. Thus, every one-to-one function has an inverse. If a function \\(f\\) is not one-to-one, however, then there exist two pairs in \\(f\\) that have the same second element. The inverse \\(f^{-1}\\) therefore has two pairs where the first element is the same. When such is the case, the definition of a function is violated, and \\(f^{-1}\\) cannot be a function. Thus, if a function is invertible, it must be one-to-one. 13.3 Odd and Even Functions A function is said to be even if for any real number \\(x,\\ f(-x)=f(x)\\). A function is said to be odd if for any real number \\(x,\\ f(-x)=-f(x)\\). If neither of these criteria are met, the function is simply said to be neither odd nor even. 13.4 Theorems 13.4.1 Operations on Even Functions Let \\(f\\) and \\(g\\) both be even functions. Then: \\([f+g](x)\\) is an even function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x)+g(-x) \\\\ &amp;= f(x)+g(x) \\\\ &amp;= [f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an even function. 13.4.2 Operations on Odd Functions Let \\(f\\) and \\(g\\) both be odd functions. Then: \\([f+g](x)\\) is an odd function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an odd function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\\\ &amp;= -[f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an odd function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= -f(x)\\cdot -g(x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= -f\\big(g(x)\\big) \\\\ &amp;= -[f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an odd function. 13.4.3 Operations on an Odd and Even Function Let \\(f\\) be an even function and let \\(g\\) both be an odd function. Then: \\([f+g](x)\\) is neither an odd nor an even function \\([f\\cdot g](x)\\) is an odd function \\([f\\circ g](x)\\) is an even function \\([g\\circ f](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\end{align*}\\] so \\([f+g](x)\\) is neither an odd nor an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot -g(x) \\\\ &amp;= -\\big(f(x)\\cdot g(x)\\big) \\\\ &amp;= -[f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an odd function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\[\\begin{align*} [g\\circ f](-x) &amp;= g\\big(f(-x)\\big) \\\\ &amp;= g\\big(f(x)\\big) \\\\ &amp;= [g\\circ f](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\end{itemize} 13.4.4 Derivatives and Anti-derivatives of Odd Functions Let \\(f\\) be an odd function and let \\(f^\\prime\\) and \\(F\\) denote the derivative and anti-derivative of \\(f\\), respectively. Then \\(f^\\prime\\) and \\(F\\) are both even functions. Proof: \\[\\begin{align*} f(-x) &amp;= -f(x)\\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack f(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack-f(x)\\big\\rbrack \\\\ \\Rightarrow f^\\prime(-x)\\cdot -1 &amp;= -f^\\prime(x) \\\\ \\Rightarrow -f^\\prime(-x) &amp;= -f^\\prime(x) \\\\ \\Rightarrow f^\\prime(-x) &amp;= f^\\prime(x) \\end{align*}\\] So \\(f^\\prime\\) is an even function. \\[\\begin{align*} f(-x) &amp;= -f(x) \\\\ \\Rightarrow \\int f(-x) &amp;= \\int-f(x)\\\\ \\Rightarrow F(-x)\\cdot-1 &amp;= -F(x)\\\\ \\Rightarrow -F(-x) &amp;= -F(x)\\\\ \\Rightarrow F(-x) &amp;= F(x) \\end{align*}\\] So \\(F\\) is also an even function. 13.4.5 Derivatives and Anti-derivatives of Even Functions Let \\(g\\) be an even function, and let \\(g^\\prime\\) and \\(G\\) denote the derivative and anti-derivative of \\(g\\), respectively. Then \\(g^\\prime\\) and \\(G\\) are both odd functions. Proof: \\[\\begin{align*} g(-x) &amp;= g(x) \\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack g(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack g(x)\\big\\rbrack \\\\ \\Rightarrow g^\\prime(-x)\\cdot -1 &amp;= g^\\prime(x) \\\\ \\Rightarrow -g^\\prime(-x) &amp;= g^\\prime(x) \\\\ \\Rightarrow g^\\prime(-x) &amp;= -g^\\prime(x) \\end{align*}\\] So \\(g^\\prime\\) is an odd function. \\[\\begin{align*} g(-x) &amp;= g(x)\\\\ \\Rightarrow \\int g(-x) &amp;= \\int g(x)\\\\ \\Rightarrow G(-x)\\cdot-1 &amp;= G(x)\\\\ \\Rightarrow -G(-x) &amp;= G(x)\\\\ \\Rightarrow G(-x) &amp;= -G(x) \\end{align*}\\] So \\(G\\) is also an odd function. "],
["integration-techniques-and-theorems.html", "14 Integration: Techniques and Theorems 14.1 Elementary Theorems", " 14 Integration: Techniques and Theorems 14.1 Elementary Theorems 14.1.1 Integration of Even Functions about Zero Suppose \\(f\\) is an integratable function, and let \\(F(x)=\\int\\limits_{0}^{x_0}f(x)dx\\). Then \\(\\int\\limits_{-x_0}^{0}f(x)dx=\\int\\limits_{0}^{x_0}f(x)dx\\) if and only if \\(f\\) is an even function.\\ Proof: First, let \\(f\\) be an even function. Then, by Theorem 13.4.5, the anti-derivative \\(F\\) is an odd function. \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= F(0) + F(x_0) \\\\ ^{[1]} &amp;= F(x_0) \\\\ \\\\ \\\\ \\int\\limits_{0}^{x_0}f(x)dx &amp;= F(x_0) - F(0) \\\\ &amp;= F(x_0) \\end{align*}\\] \\(F(0)=\\int\\limits_{0}^{0}f(x)dx=0\\). So \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(x_0) \\\\ &amp;= \\int\\limits_{0}^{x_0}f(x)dx \\end{align*}\\] Now suppose \\[ \\int\\limits_{-x_0}^{0}f(x)dx = \\int\\limits_{0}^{x_0}f(x)dx \\] Then \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= -F(-x_0) \\end{align*}\\] and \\[\\begin{align*} \\int\\limits_{0}^{x_0}f(x)dx = F(x_0) - F(0) \\\\ = F(x_0)$ \\end{align*}\\] So \\[\\begin{align*} -F(-x_0) &amp;= F(x_0) \\\\ \\Rightarrow F(-x_0) &amp;= -F(x_0) \\end{align*}\\] This satisfies the definition of an odd function. So by Theorem 13.4.4, \\(f\\) must be an even function. 14.1.2 Corollary If \\(f\\) is a continuous and even function and \\(t\\in\\Re\\), then \\[ \\int\\limits_{-t}^{t}f(x)dx = 2\\int\\limits_{0}^{t}f(x)dx \\] Furthermore, \\[\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\]. Proof: Since \\(f(x)\\) is even and by Theorem 14.1.1 \\[\\begin{align*} \\int\\limits_{-t}^{0}f(-x)dx &amp;= \\int\\limits_{-t}^{0}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] It follows that \\[\\begin{align*} \\int\\limits_{-t}^{t}f(x)dx &amp;= \\int\\limits_{-t}^{0}f(-x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= 2\\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] The second statement is proven by taking the limits as \\(t\\rightarrow\\infty\\). 14.1.3 Integrals of Horizontal Translations Let \\(x\\) be any real number and \\(a,b,\\) and \\(c\\) be constants. Also, let \\(f(x)\\) be continuous on the interval \\((a,b)\\). Then \\[\\int\\limits_{a}^{b}f(x)dx = \\int\\limits_{a+c}^{b+c} f(x+c)dx\\] Proof: The proof of this theorem is completed by applying a change of variable to \\[\\int\\limits_{a}^{b}f(x)dx\\] We let \\[\\begin{align*} y &amp;= x+c \\\\ \\Rightarrow x &amp;= y-c \\end{align*}\\] So \\(dx=dy\\). \\[\\begin{align*} x &amp;= a &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= a+c\\\\ x &amp;= b &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= b+c \\end{align*}\\]. Thus \\[\\begin{align*} \\int\\limits_{a}^{b}f(x)dx &amp;= \\int\\limits_{a+c}^{b+c}f(y)dy \\\\ &amp;= \\int\\limits_{a+c}^{b+c}f(x+c)dx \\end{align*}\\] "],
["summation.html", "15 Summation 15.1 Theorems of Summation", " 15 Summation 15.1 Theorems of Summation 15.1.1 Theorem If \\(c\\) is a constant then \\[\\sum\\limits_{i=1}^{n}c = nc\\] Proof: \\[ \\sum\\limits_{i=1}^{n}c = \\underbrace{c+c+\\cdots+c}_{n\\ \\rm terms} = nc \\] 15.1.2 Theorem If \\(a_1,a_2,\\ldots,a_n\\) are real numbers and \\(c\\) is a constant, then \\[ \\sum\\limits_{i=1}^{n}ca_i = c\\sum\\limits_{i=1}^{n}a_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}ca_i &amp;= ca_1 + ca_2 + \\cdots + ca_n \\\\ &amp;= c(a_1+a_2+\\cdots+a_n) \\\\ &amp;= c\\sum\\limits_{i=1}^{n}a_i \\end{align*}\\] 15.1.3 Theorem If \\(a_1,_2,\\ldots,a_n\\) are real numbers and \\(b_1,b_2,\\ldots,b_n\\) are real numbers, then \\[ \\sum\\limits_{i=1}^{n}(a_i+b_i) = \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}(a_i+b_i) &amp;= a_1 + b_1 + a_2 + b_2 + \\cdots + a_n + b_n \\\\ &amp;= a_1 + a_2 + \\cdots + a_n + b_1 + b_2 + \\cdots + b_n \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\end{align*}\\] 15.1.4 Theorem If \\(a_i\\) and \\(b_j\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j = a_{+} b_{+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_j\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i b_{+} \\\\ &amp;= b_{+} \\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= a_{+} b_{+} \\end{align*}\\] 15.1.5 Theorem If \\(a_i\\) is a real number for \\(i=1,2,\\ldots,n\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b = m a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b &amp;= \\sum\\limits_{i=1}^{n}m a_i b \\\\ &amp;= m b\\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= m a_{+} b \\end{align*}\\] 15.1.6 Theorem If \\(a_j\\) is a real number for \\(j=1,2,\\ldots,m\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b = n a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b &amp;= \\sum\\limits_{i=1}^{n}\\bigg( b \\sum\\limits_{j=1}^{m} a_j \\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_{+}b \\\\ &amp;= n a_{+} b \\end{align*}\\] 15.1.7 Theorem If \\(a_i\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} = \\sum\\limits_{i=1}^{n}a_ib_{i+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_{ij}\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_ib_{i+} \\end{align*}\\] 15.1.8 Theorem If \\(a_j\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[\\ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} = \\sum\\limits_{i=1}^{n}a_jb_{+ j} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} &amp;= a_1b_{11}+a_2b_{12}+\\cdots+a_mb_{1m} \\\\ &amp; \\ \\ \\ \\ +a_1b_{21}+a_2b_{22}+\\cdots+a_mb_{2m} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_1b_{n1}+a_1b_{n1}+\\cdots+a_1b_{nm} \\\\ &amp;= a_1b_{11}+a_1b_{21}+\\cdots+a_1b_{n1} \\\\ &amp; \\ \\ \\ \\ +a_2b_{12}+a_2b_{22}+\\cdots+a_2b_{n2} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_mb_{1m}+a_mb_{2m}+\\cdots+a_nb_{nm} \\\\ &amp;= a_1(b_{11}+b_{21}+\\cdots+b_{n1}) \\\\ &amp; \\ \\ \\ \\ +a_2(b_{12}+b_{22}+\\cdots+b_{n2}) \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_m(b_{1m}+b_{2m}+\\cdots+b_{nm}) \\\\ &amp;= a_1b_{+ 1}+a_2b_{+ 2}+\\cdots+a_mb_{+ m} \\\\ &amp;=\\sum\\limits_{j=1}^{m}a_jb_{+ j} \\end{align*}\\] "],
["variance-parameter.html", "16 Variance Parameter 16.1 Defining Variance With Expected Values 16.2 Unbiased Estimator 16.3 Computational Formulae", " 16 Variance Parameter 16.1 Defining Variance With Expected Values In the case of a discrete random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\sum\\limits_{x=0}^{\\infty}(x-\\mu)^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2-2\\mu x+\\mu^2)p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2p(x)-2\\mu x\\cdot p(x)+\\mu^2p(x)) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\sum\\limits_{x=0}^{\\infty}2\\mu x\\cdot p(x) + \\sum\\limits_{x=0}^{\\infty}\\mu^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\sum\\limits_{x=0}^{\\infty}x\\cdot p(x) + \\mu^2\\sum\\limits_{x=0}^{\\infty}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2\\\\ \\end{align*}\\] In the case of a continuous random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}(x-\\mu)^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2-2\\mu x+\\mu^2)f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2f(x)-2\\mu x\\cdot f(x)+\\mu^2f(x))dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\int\\limits_{-\\infty}^{\\infty}2\\mu x\\cdot f(x)dx + \\int\\limits_{-\\infty}^{\\infty}\\mu^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\int\\limits_{-\\infty}^{\\infty}x\\cdot f(x)dx + \\mu^2\\int\\limits_{-\\infty}^{\\infty}f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2 \\end{align*}\\] In general, these results may be summarized as follows: \\[\\begin{align*} \\sigma^2 &amp;= E[(X-\\mu)^2] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)] \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align*}\\] 16.2 Unbiased Estimator \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg) &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{n(\\sum\\limits_{i=1}^{n}x_i)^2}{n^2}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2)-nE(\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big]\\\\\\\\ &amp;= \\frac{1}{n}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;=\\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n}\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Since \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg)\\neq\\sigma^2\\) it is a biased estimator. Notice, however, that the bias can be eliminated by dividing by \\(n-1\\) instead of by \\(n\\) \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg) &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - 2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{(\\sum\\limits_{i=1}^{n}x_i)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{n\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n^2} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-nE(\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2)-nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big] \\\\ &amp;= \\frac{1}{n-1}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;= \\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n-1}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n-1}\\sigma^2 \\\\ &amp;=\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Thus \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg)\\) is an unbiased estimator of \\(\\sigma^2\\), and we define the estimator \\[s^2= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\] 16.3 Computational Formulae 16.3.1 Computational Formula for \\(\\sigma\\)^2 \\[\\begin{align*} \\sigma^2 &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i^2-2\\mu x_i+\\mu^2)}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-\\sum\\limits_{i=1}^{N}2\\mu x_i + \\sum\\limits_{i=1}^{N}\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\mu\\sum\\limits_{i=1}^{N}x_i+N\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2 -2\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\sum\\limits_{i=1}^{N}x_i + N\\Big(\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\Big)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N} + \\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] 16.3.2 Computational Formula for \\(s\\)^2 \\[\\begin{align*} s^2 &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\sum\\limits_{i=1}^{n}2\\bar x x_i + \\sum\\limits_{i=1}^{n}\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\bar x\\sum\\limits_{i=1}^{n}x_i+n\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n}x_i + n\\Big(\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\Big)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n} + \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] "]
]
