[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2016-08-13 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["analysis-of-variance.html", "2 Analysis of Variance 2.1 One-Way Design 2.2 Computational Formulas 2.3 Randomized Complete Block Design", " 2 Analysis of Variance 2.1 One-Way Design 2.1.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+} + \\bar x_{i+} - x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (x_{ij} - \\bar x{i+}) + (\\bar x_{i+} - \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++}) + (x_{ij} - \\bar x{i+}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + (x_{ij} - \\bar x_{i+})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\bigg(\\sum\\limits_{j=1}^{n_i} x_{ij} - \\sum\\limits_{j=1}^{n_i}\\bar x_{i+}\\bigg) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - n_i \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\big(x_{i+} - n_i \\frac{x_{i+}}{n_i}\\big) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\cdot 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2\\\\ \\end{align*}\\] The components are commonly referred to as \\[ SS_{Factor} = \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2$ and \\] \\[ SS_{Error} = \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2$ \\] Notice that \\(SS_{Factor}\\) compares the factor means to the overall mean, and it can be said that \\(SS_{Factor}\\) measures the variation between factors. \\(SS_{Error}\\) compares each observation to the overall mean, and can be said to describe the variation within factors. When \\(n_1 = n_2 = \\cdots n_i = n\\), the design is said to be balanced. 2.2 Computational Formulas \\(SS_{Total}\\) and \\(SS_{Factor}\\) can be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} x_{ij}^2 - x_{++} \\sum\\limits_{j=1}^{n_i}\\frac{1}{n_i}\\\\ \\end{align*}\\] See Theorem 10.3.1 \\[\\begin{align*} SS_{Factor} &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\frac{\\bar x_{i+}^2}{n_i} - \\bar x_{++} \\sum\\limits_{i=1}^{a}\\frac{1}{n_i} \\end{align*}\\] See Theorem 10.3.1 \\(SS_{Error}\\) does not simplify to a convenient form, but \\[\\begin{align*} SS_{Total} &amp;= SS_{Factor} + SS_{Error} \\\\ \\Rightarrow SS_{Error} &amp;= SS_{Total} - SS_{Factor} \\end{align*}\\] 2.3 Randomized Complete Block Design Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable. To implement this design, suppose we have \\(a\\) treatments we want to compare, and \\(b\\) blocks. We may analyze the data by use of the sums of squares, similar to the one-way design. 2.3.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} + \\bar x_{i+} - \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{+ j} + \\bar x_{++} - \\bar x_{+ +} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++}) + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++}) (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big] \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 + 0 + 0 + 0 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 + a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] It is shown that the cross products are equal to zero in Section 2.3.3 These terms are commonly referred to as \\[\\begin{align*} SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ SS_{Error} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] 2.3.2 Computational Formulae \\(SS_{Total}\\), \\(SS_{Factor}\\), and \\(SS_{Block}\\) can all be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} x_{ij}^2 - \\frac{x_{++}}{ab}\\\\ \\\\ SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{b}\\sum\\limits_{i=1}^{a}x_{i+}^2 - \\frac{x_{++}^2}{ab} \\\\ \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{a}\\sum\\limits_{j=1}^{b} x_{+ j}^2 - \\frac{x_{++}^2}{ab} \\end{align*}\\] See Theorem 10.3.1 \\(SS_{Error}\\) does not simplify to any convenient form, but may be calculated from the other terms as \\(SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}\\) 2.3.3 RCBD Cross Products The cross products of the RCBD design \\[\\begin{align*} 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp;= 0 \\end{align*}\\] Proof: \\[ 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\\\ \\ \\ = 2\\bigg(\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\bigg)\\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ \\bar x_{i+}\\bar x_{+ j} - \\bar x_{i+}\\bar x_{++} - \\bar x_{+ j}\\bar x_{++} + \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}\\bar x_{+ j} - \\bar x_{+ j}^2 + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}^2 - \\bar x_{i+}\\bar x_{+ j} + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}( -\\bar x_{++}^2 - \\bar x_{i+}^2 - \\bar x_{+ j}^2 + x_{ij}\\bar x_{i+} + x_{ij}\\bar x_{+ j} - 2 x_{ij}\\bar x_{++} - \\bar x_{i+}\\bar x_{+ j} \\\\ \\ \\ \\ \\ + 2\\bar x_{i+}\\bar x_{++} + 2\\bar x_{+ j}\\bar x_{++} ) \\\\ \\ \\ = 2\\bigg(-\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{++}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j} \\\\ \\ \\ \\ \\ - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2 x_{ij}\\bar x_{++} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ = 2\\bigg( \\frac{ab\\bar x_{++}^2}{a^2b^2} - \\frac{b}{b^2}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{a}{a^2}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[1]} =2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[2]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[3]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ ^{[4]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\] \\[ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[5]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\frac{2\\bar x_{++}^2}{ab} \\bigg)\\\\ \\ \\ = 2\\bigg(\\frac{4\\bar x_{++}^2}{ab} - \\frac{4\\bar x_{++}^2}{ab} + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\bigg)\\\\ \\ \\ = 2(0 + 0 + 0) \\\\ = 2(0) \\\\ = 0 \\] See Summation Theorem 9.1.7 See Summation Theorem 9.1.8 See Summation Theorem 9.1.4 See Summation Theorem 9.1.5 See Summation Theorem 9.1.6 Using the theorems in Chapter it is can be shown that each of the three cross products is equal to zero. However, the physical tedium of reducing each cross product is much greater than the approach taken above. "],
["bernoulli-distribution.html", "3 Bernoulli Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function 3.3 Expected Values 3.4 Moment Generating Function 3.5 Theorems for the Bernoulli Distribution", " 3 Bernoulli Distribution 3.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} p^x(1-p)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(p\\) is the probability of a success. 3.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-p &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] Figure 3.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(p=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(p=.4\\). 3.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot p^{x} (1-p)^{1-x}\\\\ &amp;= 0 \\cdot p^{0} (1-p)^{1-0} + 1 \\cdot p^{1} (1-p)^{1-1}\\\\ &amp;= 0 + p (1-p)^{0}\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot p^x (1-p)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot p^0 (1-p)^{1-0} + 1^2 \\cdot p^1 (1-p)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot p \\cdot 1 \\\\ &amp;= 0 + p\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X) = p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p-p^2 \\\\ &amp;= p(1-p) \\end{align*} \\] 3.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\\\ &amp;= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}\\\\ &amp;= (1-p) + e^t p\\\\ &amp;=pe^t + (1-p) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p (1-p) \\end{align*} \\] 3.5 Theorems for the Bernoulli Distribution 3.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}p^x(1-p)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} p^x (1-p)^{1-x} &amp;= p^0 (1-p)^1 + p^1 (1-p)^0 \\\\ &amp;= (1-p) + p \\\\ &amp;= 1 \\end{align*}\\] 3.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p)) (pe^t+(1-p)) \\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-distribution.html", "4 Binomial Distribution 4.1 Probability Mass Function 4.2 Cumulative Mass Function 4.3 Expected Values 4.4 Moment Generating Function 4.5 Maximum Likelihood Estimator 4.6 Theorems for the Binomial Distribution", " 4 Binomial Distribution 4.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(p\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} p^x (1-p)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(p\\) is the probability of a success on each individual trial. 4.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] Figure 4.1: The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(p=.4\\). 4.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\ &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\ &amp;= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\ &amp;= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1} + \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\ &amp;= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x}) \\\\ ^{[2]} &amp;= np(p+q)^{n-1} \\\\ ^{[1]} &amp;= np(p+(1-p))^{n-1} \\\\ &amp;= np(p+1-p)^{n-1} \\\\ &amp;= np(1)^{n-1} \\\\ &amp;= np(1) \\\\ &amp;= np \\end{align*}\\] Let \\(q = (1 - p)\\) By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1} + \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n} \\\\ &amp;= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1} + 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big] \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\ &amp;= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\\\ ^{[2]} &amp;= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y} \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big] \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big] \\\\ ^{[3]} &amp;= np(mp+1) \\\\ &amp;= np[(n-1)p+1] \\\\ &amp;=np(np-p+1) \\\\ &amp;=n^2p^2 - np^2 + np \\end{align*}\\] \\(q = (1 - p)\\) Let \\(y = x - 1\\) and \\(n = m + 1\\) \\(\\Rightarrow\\) \\(x = y + 1\\) and \\(m = n - 1\\) \\(\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}\\) is of the form of the expected value of \\(Y\\), and \\(E(Y)=mp=(n-1)p\\). \\(\\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}\\) is the sum of all probabilities over the domain of \\(Y\\) which is 1. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n^2p^2 - np^2 + np - n^2p^2 \\\\ &amp;= -np^2 + np \\\\ &amp;= np(-p-1) \\\\ &amp;= np(1-p) \\end{align*}\\] 4.4 Moment Generating Function \\[ \\begin{align*} M_X(t) &amp;= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x} \\\\ ^{[1]} &amp;= [(1-p)+pe^t]^n \\end{align*} \\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[ \\begin{align*} M_X^{(1)}(t) &amp;= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\\\ \\\\ M_X^{(2)}(t) &amp;= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\\\ &amp;= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= n[(1-p)+pe^0]^{n-1}pe^0 \\\\ &amp;= n[1-p+p^{n-1}p\\\\ &amp;= n(1)^{n-1}p &amp;= np\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\\cdot0}[(1-p) + pe^0]^{n-2} \\\\ &amp;= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\\\ &amp;= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\\\ &amp;= np+n(n-1)p^2 \\\\ &amp;= np+(n^2-n)p^2 \\\\ &amp;= np + n^2 + n^2p^2 - np^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= np + n^2p^2 - np^2 - n^2p^2 \\\\ &amp;= np - np^2\\\\ &amp;= np(1-p) \\end{align*}\\] 4.5 Maximum Likelihood Estimator Since \\(n\\) is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for \\(n\\). The mean and variance can both be estimated from the single parameter \\(p\\). Let \\(X\\) be a Binomial random variable with parameter \\(p\\) and \\(n\\) outcomes \\((x_1,x_2,\\ldots,x_n)\\). Let \\(x_i=0\\) for a failure and \\(x_i=1\\) for a success. In other words, \\(X\\) is the sum of \\(n\\) Bernoulli trials with equal probability of success and \\(X=\\sum\\limits_{i=1}^{n}x_i\\). 4.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\ &amp;= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\ &amp;= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\ &amp;= \\theta^X(1-\\theta)^{n-X} \\end{align*}\\] 4.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln L(\\theta) \\\\ &amp;= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\ &amp;= X\\ln(\\theta)+(n-X)\\ln(1-\\theta) \\end{align*}\\] 4.5.3 MLE for p \\[\\begin{align*} \\frac{d\\ell(p)}{d p} &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ 0 &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\Rightarrow \\frac{X}{p} &amp;= \\frac{n-X}{1-p} \\\\ \\Rightarrow (1-p)X &amp;= p(n-X) \\\\ \\Rightarrow X-pX &amp;= np-pX \\\\ \\Rightarrow X &amp;= np \\\\ \\Rightarrow \\frac{X}{n} &amp;= p \\\\ \\end{align*}\\] So \\(\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(p\\). 4.6 Theorems for the Binomial Distribution 4.6.1 Validity of the Distribution \\[\\begin{align*} \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x} = 1 \\end{align*}\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\big(p + (1-p)\\big)^n \\\\ &amp;= (1)^n \\\\ &amp;= 1 \\end{align*}\\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) 4.6.2 Sum of Binomial Random Variables Let \\(X_1,X_2,\\ldots,X_k\\) be independent random variables where \\(X_i\\) comes from a Binomial distribution with parameters \\(n_i\\) and \\(p\\). That is \\(X_i\\sim(n_i,p)\\). Let \\(Y = \\sum\\limits_{i=1}{k} X_i\\). Then \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\ &amp;= \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\\\ &amp;= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i} \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(\\sum\\limits_{i=1}^{k}n_i\\) and \\(p\\). Thus \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). 4.6.3 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-theorem.html", "5 Binomial Theorem 5.1 Traditional Proof 5.2 General Approach 5.3 Other Theorems", " 5 Binomial Theorem The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions. Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach. Other useful theorems are provided at the end of this chapter. 5.1 Traditional Proof 5.1.1 Lemma: Pascal’s rule Let \\(n\\) and \\(x\\) be non-negative integers such that \\(x\\leq n\\). Then \\({n-1\\choose x} + {n-1\\choose x-1} = {n\\choose x}\\). Proof: \\[\\begin{align*} {n-1\\choose x} + {n-1\\choose x-1} &amp;= \\frac{(n-1)!}{x!(n-1-x)!} + \\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)!}\\\\ &amp;= \\frac{(n-1)!}{x(x-1)!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!} +\\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{n!}{x!(n-x)!} \\\\ &amp;= {n\\choose x} \\end{align*}\\] 5.1.2 The Binomial Theorem Let \\(a\\) and \\(b\\) be constants and let \\(n\\) be any positive integer. Then \\[(a+b)^n = \\sum\\limits_{x=0}^{n} {n\\choose x} a^{n-x} b^x\\] Proof: This proof is completed by mathematical induction. Base Step: \\(n=1\\) \\[\\begin{align*} (a+b)^1 &amp;= \\sum\\limits_{x=0}^{1} {1\\choose x} a^{1-x} b^x \\\\ &amp;= {1\\choose 0} a^{1-0} b^0 + {1\\choose 1} a^{1-1} b^1 \\\\ &amp;= 1\\cdot a\\cdot 1 + 1\\cdot 1\\cdot b \\\\ &amp;= a+b \\end{align*}\\] Inductive Step: Assume that the Theorem holds for \\(n\\), and show it is true for \\(n+1\\). \\[\\begin{align*} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= a(a+b)^n + b(a+b)^n \\\\ &amp;= a(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x + b^n) + b(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^n) \\\\ &amp;= (a^{n+1}+a\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^{n+1}) \\\\ &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x+1}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[1]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}a^{n-x+1}b^x) + (\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[2]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}{n\\choose x}a^{n-x+1}b^x) + \\sum\\limits_{x-1}^{n-1}{n\\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\\\ ^{[3]} &amp;= a^{n+1} + \\sum\\limits_{x+1}^{n}{n+1\\choose x}a^{n-x+1}b^x + b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{x=1}^{n}{n+1\\choose x}a^{(n+1)-x}b^x+b^{n+1} \\\\ ^{[4]} &amp;= \\sum\\limits_{x=0}^{n+1}{n+1\\choose x}a^{(n+1)-x}b^x \\end{align*}\\] This completes both the inductive step and the proof. \\(ab^n={n\\choose n}a^{n-n+1}b^n\\) which is the term for \\(x=n\\) in the first summation. \\(a^nb={n\\choose 0}a^{n-0}b^1\\) which is the term for \\(x=0\\) in the second summation. \\(\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-x+1}b^x\\) This step is made using Pascal’s Rule with \\(n=n-1\\). \\(a^{n+1}={n+1\\choose 0}a^{(n+1)-0}b^0\\) which is the term for \\(x=0\\) in the summation. \\(\\ \\ b^{n+1}={n+1\\choose n+1}a^{(n+1)-(n+1)}b^{n+1}\\) which is the term for \\(x=n+1\\) in the summation 5.2 General Approach 5.2.1 A Binomial Expansion Theorem This theorem and its corrolary are provided by Brunette. For any positive integer \\(n\\), let \\(B_n = (x_1+y_1) (x_2+y_2) \\cdots (x_n+y_n)\\). In the expansion \\(B_n\\), before combining possible like terms, the following are true: There will be \\(2^n\\) terms. Each of these terms will be a product of \\(n\\) factors. In each such product there will be one factor from each binomial (in \\(B_n\\)). Every such product of \\(n\\) factors, one from each binomial, is represented in the expansion. Proof: Proof is done by induction. For the case \\(n=1\\), the result is clear. Now assume that the theorem is true for a particular \\(n\\) and consider \\(B_{n+1}\\). \\[ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} \\] By the inductive assumption, \\(B_n = T_1 + T_2 + \\cdots + T_{2^n}\\) where each \\(T_i\\) is a product of \\(n\\) factors, one factor from each binomial. It follows that every term in the expansion of \\(B_n+1\\) is either of the type \\(T_ix_{n+1}\\) or \\(T_iy_{n+1}\\), for some \\(1\\leq i \\leq 2^n\\). But each term of either of the above types is clearly a product of \\(n+1\\) factors with one factor coming from each binomial. thus, if (ii) and (iii) are true for \\(B_n\\), then they are true for \\(B_n+1\\). Next, by the inductive assumption, the expansion of \\(B_n\\) is a sum of \\(2^n+2^n\\) terms, i.e., \\(2^{n+1}\\) terms. This completes the inductive step for (i). Lastly, it remains for us to consider a product of the type \\(p_1 p_2 \\cdots p_n p_{n+1}\\) where, for each \\(1\\leq i\\leq n+1\\), \\(p_i = x_i\\) or \\(p_i = y_i\\). By the inductive hypothesis, \\(p_1 p_2 \\cdots p_n\\) is a term in the expansion of \\(B_n\\). If \\(p_{n+1} = x_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_nx_{n+1}\\), and so of \\(B_{n+1}\\). Likewise, if \\(p_{n+1}=y_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_n y_{n+1}\\), and so of \\(B_{n+1}\\). This completes the inductive step and the proof. 5.2.2 Corollary: Binomial Theorem Let \\(x\\) and \\(y\\) be constants and let \\(n\\) be any positive integer.\\ Then \\(\\displaystyle (x+y)^n = \\sum\\limits_{i=0}^{n} {n\\choose i} x^{n-i} y^i\\\\\\) Proof: Since each term in the expansion will have \\(n\\) terms, each term must follow the form \\(x^{n-i} y^i\\) for \\(0 \\leq i \\leq n\\), and in all, there are \\(2^n\\) such terms. For any given value of \\(i\\), the number of terms of the form \\(x^{n-i}y^i\\) is clearly the number of ways one can choose the \\(i\\) factors of \\(y\\) from the \\(n\\) available binomials, i.e., \\({n\\choose i}\\), which gives \\[(x+y)^n = \\sum\\limits_{i=0}^{n}{n\\choose i} x^{n-i} y^i\\] 5.3 Other Theorems 5.3.1 Theorem \\[{N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n}\\] where \\(0 \\leq n \\leq N_1 + N_2\\). Proof: Using the Binomial Theorem we establish \\[ (1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\\\ \\Rightarrow [{N_1\\choose 0}a^0+\\cdots+{N_1\\choose N_1}a^{N_1}]\\cdot [{N_2\\choose 0}a^0+\\cdots+{N_2\\choose N_2}a^{N_2}] \\\\ \\ \\ \\ \\ ={N_1+N_2\\choose 0}+{N_1+N_2\\choose 1}a+\\cdots +{N_1+N_2\\choose N_1+N_2}a^{N_1+N_2} \\] Expanding the left side of the equation gives \\[ {N_1\\choose 0}{N_2\\choose 0} + {N_1\\choose 0}{N_2\\choose 1}a + \\cdots + {N_1\\choose 0}{N_2\\choose N_2}a^{N_2} + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose 1}{N_2\\choose N_2}a^{N_2+1} + \\cdots + {N_1\\choose N_1}{N_2\\choose 0}a^{N_1} + {N_1\\choose N_1}{N_2\\choose 1}a^{N_1+1} \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\\\ = {N_1\\choose 0}{N_2\\choose 0}+{N_1\\choose 0}{N_2\\choose 1}a + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + {N_1\\choose 0}{N_2\\choose 2}a^2+{N_1\\choose 1}{N_2\\choose 1}a^2 + {N_1\\choose 2}{N_2\\choose 0}a^2 \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\] Notice that for any \\(n\\) where \\(0 \\leq n \\leq N_1 + N_2\\), the coefficient for \\(a^n\\), found by combining like terms, is \\({N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 1}{N_2\\choose n-1} + \\cdots+{N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose 0}{N_2\\choose n}\\) and, by the equivalence of the first equation in the proof, is equal to the coefficient \\({N_1 + N_2\\choose n}\\). 5.3.2 Theorem \\[\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}} = 1\\] for \\(0 \\leq n \\leq N_1 + N_2\\).\\ Proof: Theorem 5.3.1 establishes the equality \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\frac{\\sum\\limits_{i=1}^{n} {N_1\\choose i}{N_2\\choose n-i}} {{N_1+N_2\\choose n}} = 1 \\] "],
["chebychevs-theorem.html", "6 Chebychev’s Theorem 6.1 Chebychev’s Theorem 6.2 Alternate Proof of Chebychev’s Theorem 6.3 Chebychev’s Theorem for Absolute Deviation", " 6 Chebychev’s Theorem 6.1 Chebychev’s Theorem In any finite set of numbers and for any real number \\(h &gt; 1\\), at least \\((1 - \\frac{1}{h^2}) \\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Under these conditions we know \\[|x_1-\\mu| &gt; h\\sigma,\\ |x_2-\\mu| &gt; h\\sigma, \\ldots,\\ |x_r-\\mu| &gt; h\\sigma\\] Squaring gives \\[(x_1-\\mu)^2 &gt; h^2\\sigma^2,\\ (x_2-\\mu)^2 &gt; h^2\\sigma^2,\\ldots,\\ (x_r-\\mu)^2 &gt; h^2\\sigma^2\\\\ \\ \\ \\ \\ \\Rightarrow\\sum\\limits_{i=1}^{r}(x_1-\\mu)^2 &gt; \\sum\\limits_{i=1}^{r}h^2\\sigma^2 = rh^2\\sigma^2 \\] Since all \\((x_i-\\mu)^2\\) must necessarily be positive, \\[\\begin{align*} \\sum\\limits_{i=1}^{r}(x_i-\\mu)^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2\\sigma^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ ^{[1]} \\Rightarrow rh^2\\sigma^2 &amp;&lt; n\\sigma^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2 &amp;&lt; n \\\\ \\ \\ \\ \\ \\Rightarrow\\frac{r}{n} &amp;&lt; \\frac{1}{h^2} \\end{align*}\\] \\(\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) \\(\\ \\ \\ \\ \\Rightarrow n\\sigma^2 = \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) and \\(\\frac{r}{n}\\) is the fraction of numbers outside \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). By the law of complements, the fraction of numbers inside the interval is \\(1 - \\frac{r}{n}\\), which implies \\(1 - \\frac{r}{n} &gt; 1 - \\frac{1}{h^2}\\). Thus, more than \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the points lie within \\(h\\) standard deviations of the mean, or within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). 6.2 Alternate Proof of Chebychev’s Theorem In any finite set of numbers and for any real number \\(h&gt;1\\), at least \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma,\\mu+h\\cdot\\sigma)\\).\\ Proof: The proof here is done for the discrete case, but is applicable also in the continuous case by replacing the summations with integrals (with integrals, the limits will be from \\(-\\infty\\) to \\(\\infty\\)). \\[\\begin{align*} \\sigma^2 &amp;= E(x-\\mu)^2 \\\\ &amp;= \\sum\\limits_{y=0}^{\\infty}(y-\\mu)^2p(y) \\\\ &amp;= \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu-h\\sigma+1}^{\\mu+h\\sigma-1}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y) \\\\ ^{[1]} \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y)\\\\ \\end{align*}\\] Since all the \\((y-\\mu)^2\\) must be positive, removing the middle term will surely result in this inequality. In both of these summations \\(y\\) is outside the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\), so \\[\\begin{align*} |y-\\mu| &amp;\\geq h\\sigma \\\\ \\Rightarrow (y-\\mu^2) &amp;\\geq h^2\\sigma^2 \\\\ \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}h^2\\sigma^2p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}h^2\\sigma^2p(y) \\\\ \\Rightarrow\\sigma^2 &amp;\\geq h^2\\sigma^2\\Big[\\sum\\limits_{y=0}^{\\mu-h\\sigma}p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}p(y)\\Big] \\end{align*}\\] The first summation is the sum of all probabilities that \\(y-\\mu &lt; h\\sigma\\), i.e. \\(P(y-\\mu &lt; h\\sigma)\\). Likewise, the second summation is \\(P(y-\\mu &gt; h\\sigma)\\). \\[\\begin{align*} \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(y-\\mu&lt;h\\sigma) + P(y-\\mu&gt;h\\sigma)] \\\\ \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(|y-\\mu|&gt;h\\sigma)] \\\\ \\Rightarrow \\frac{1}{h^2} &amp;\\geq P(|y-\\mu|&gt;h\\sigma) \\\\ \\Rightarrow 1-\\frac{1}{h^2} &amp;\\leq P(|y-\\mu|&gt;h\\sigma) \\end{align*}\\] 6.3 Chebychev’s Theorem for Absolute Deviation This theorem is provided by Brunette In any finite set of numbers, and for any real number \\(h &gt; 1\\), at least \\(1 - \\frac{1}{h}\\) of the numbers lie within \\(h\\) absolute deviations of the mean, where the absolute deviation is defined \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}{n}|x_i-\\bar x|\\). In other words, \\(1-\\frac{1}{h}\\) of the numbers are in the interval \\((\\bar x-h\\cdot Ab , \\bar x+h\\cdot Ab)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Accordingly, \\[h \\cdot Ab \\leq |x_1-\\bar x| ,\\ h \\cdot Ab \\leq |x_1-\\bar x| ,\\ldots ,\\ h \\cdot Ab \\leq |x_1-\\bar x| \\] \\[\\begin{align*} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{r}|x_i-\\bar x| \\\\ \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{n}|x_i-\\bar x| \\\\ ^{[1]} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq n \\cdot Ab\\\\ \\Rightarrow \\frac{r}{n} &amp;\\leq \\frac{1}{h}\\\\ \\Rightarrow -\\frac{r}{n} &amp;\\geq -\\frac{1}{h}\\\\ \\Rightarrow 1-\\frac{r}{n} &amp;\\geq 1-\\frac{1}{h} \\end{align*}\\] \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) \\(\\Rightarrow n \\cdot Ab = \\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) Now \\(\\frac{r}{n}\\) is the fraction of numbers outside the interval. So \\(1-\\frac{r}{n}\\) is the fraction of numbers within \\(h\\) absolute deviations of the mean, or within the interval \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). "],
["chi-square-distribution.html", "7 Chi-Square Distribution 7.1 Probability Distribution Function 7.2 Cumulative Distribution Function 7.3 Expected Values 7.4 Moment Generating Function 7.5 Maximum Likelihood Function 7.6 Theorems for the Chi-Square Distribution", " 7 Chi-Square Distribution 7.1 Probability Distribution Function A random variable \\(X\\) is said to have a Chi-Square Distribution with parameter \\(\\nu\\) if its probability distribution function is \\[f(x) = \\left\\{ \\begin{array}{ll} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} &amp; 0&lt;x,\\ 0&lt;\\nu\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\(\\nu\\) is commonly referred to as the degrees of freedom. 7.2 Cumulative Distribution Function The cumulative distribution function for the Chi-Square Distribution cannot be written in closed form. It’s integral form is expressed as \\[ F(x) = \\left\\{ \\begin{array}{ll} \\displaystyle\\int\\limits_{0}^{x} \\frac{t^{\\frac{\\nu}{2}-1}e^{-\\frac{t}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} dt &amp; 0&lt;x,\\ 0&lt;\\nu\\\\\\\\ 0 &amp; otherwise \\end{array} \\right. \\] Figure 7.1: The graphs on the top and bottom depict the Chi-Square probability distribution and cumulative distribution functions, respectively, for \\(\\nu=4,7,10\\). As \\(\\nu\\) gets larger, the distribution becomes flatter with thicker tails. 7.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma\\Big(\\frac{\\nu}{2}+1\\Big)2^{\\frac{\\nu}{2}+1}\\Big] \\\\ &amp;= \\frac{\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{2\\nu}{2} \\\\ &amp;= \\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}+1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma(\\frac{\\nu}{2}+2)2^{\\frac{\\nu}{2}+2}\\Big] \\\\ &amp;= \\frac{\\Gamma\\Big(\\frac{\\nu}{2}+2\\Big)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{(\\frac{\\nu}{2}+1)\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\cdot 2^2=2\\Big(\\frac{\\nu}{2}+1\\Big)\\nu \\\\ &amp;= (\\nu+2)\\nu=\\nu^2+2\\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}e^{tx}\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{2tx}{2}-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-\\frac{2tx-x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{-2t+1}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{1-2t}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{-x}{\\frac{2}{1-2t}}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Big(\\frac{2}{1-2t}\\Big)^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big]\\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= (1-2t)^{-\\frac{\\nu}{2}} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\frac{\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1}(-2) \\\\ &amp;= \\frac{2\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= (-\\frac{\\nu}{2}-1)\\nu(1-2t)^{-\\frac{\\nu}{2}-2}(-2) \\\\ &amp;= (\\frac{2\\nu}{2}+2)\\nu(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu+2)\\nu)(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-2t)^{-\\frac{\\nu}{2}-2}\\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= \\nu(1-2\\cdot 0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu \\\\ M_X^{(2)}(0) &amp;= (\\nu^2+2\\nu)(1-2\\cdot 0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\nu\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.5 Maximum Likelihood Function Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from a Chi-square distribution with parameter \\(\\nu\\). 7.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\nu/2-1}e^{-x_1/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdot \\frac{x_2^{\\nu/2-1}e^{-x_2/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdots \\frac{x_n^{\\nu/2-1}e^{-x_n/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\nu/2-1}e^{-x_i/2}} {2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}e^{-x_i/2} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\sum\\limits_{i=1}^{n}\\frac{x_i}{2} \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\end{align*}\\] 7.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\big(L(\\theta)\\big) \\\\ &amp;= \\ln\\Bigg[ \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\Bigg] \\\\ &amp;= \\ln\\Bigg[ \\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) \\Bigg] + \\ln\\Bigg( \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\Bigg) + \\ln\\bigg(\\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}\\bigg) \\\\ &amp;= -n \\ln\\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i \\bigg) \\\\ &amp;= -n\\bigg( \\ln(2^{\\nu/2}) + \\Gamma\\Big(\\frac{\\nu}{2}\\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\bigg(\\frac{\\nu}{2} \\ln 2 + \\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n\\nu}{2} \\ln 2 - n\\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 7.5.3 MLE for \\(\\nu\\) \\[\\begin{align*} \\frac{d\\ell}{d\\nu} &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) \\cdot \\frac{1}{2} + 0 + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\Rightarrow \\frac{n}{2} \\ln 2 - \\frac{1}{2}\\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow n\\ln 2 - \\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln 2}{n} &amp;= \\frac{\\Gamma^\\prime\\big(\\frac{\\nu}{2}\\big)}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\end{align*}\\] Due to the complexity of the Gamma function in this equation, no solution can be developed for \\(\\nu\\) in closed form. Thus, we have to rely on numerical methods to obtain a solution to the equation and find the maximum likelihood estimator. 7.6 Theorems for the Chi-Square Distribution 7.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big] \\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}{2^{\\frac{\\nu}{2}} \\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 7.6.2 Sum of Chi-Square Random Variables Let \\(X_1 , X_2 , \\ldots , X_n\\) be independent Chi-Square random variables with parameter \\(\\nu_i\\), that is \\(X_i\\sim\\chi^2(\\nu_i),\\ i=1,2,\\ldots,n\\). Suppose \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\chi^2(\\sum\\limits_{i=1}^{n}\\nu_i)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}=E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-2t)^{-\\frac{\\nu_1}{2}}(1-2t)^{-\\frac{\\nu_2}{2}}\\cdots (1-2t)^{-\\frac{\\nu_n}{2}} \\\\ &amp;= (1-2t)^{\\sum\\limits_{i=1}^{n}\\nu_i} \\end{align*}\\] Which is the mgf of a Chi-Square random variable with parameter \\(\\sum\\limits_{i=1}^{n}\\nu_i\\). Thus \\(Y\\sim\\chi^2\\bigg(\\sum\\limits_{i=1}^{n}\\nu_i\\bigg)\\). 7.6.3 Square of a Standard Normal Random Variable If \\(Z\\sim N(0,1)\\), then \\(Z^2\\sim\\chi^2(1)\\). Proof: \\[\\begin{align*} M_{Z^2}(t) &amp;= E(e^{tZ^2}) \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}e^{tz^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty}e^{tz^2} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(-2t+1)}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[1]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[2]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}e^{-u} \\frac{\\sqrt{2}u^{-\\frac{1}{2}}}{2(1-2t)^{\\frac{1}{2}}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}e^{-u}u^{-\\frac{1}{2}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-u}du \\\\ ^{[3]} &amp;= \\frac{1}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}}\\Gamma(\\frac{1}{2}) \\\\ &amp;= \\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{1}{2}}}=(1-2t)^{-\\frac{1}{2}} \\\\ \\end{align*}\\] \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\) when f(x) is an even function () Let \\(u=\\frac{z^2}{2}(1-2t) \\ \\ \\ \\ \\Rightarrow z=\\frac{\\sqrt{2}u^{\\frac{1}{2}}}{(1-2t)^{\\frac{1}{2}}}\\) So \\(dz=\\frac{\\sqrt{2}u^{-\\frac{1}{2}}} {2(1-2t)^{\\frac{1}{2}}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) Which is the mgf of a Chi-Square random variable with 1 degree of freedom. Thus \\(Z^2\\sim\\chi^2(1)\\). "],
["combinations.html", "8 Combinations", " 8 Combinations 8.0.1 Lemma A set of \\(n\\) elements may be partitioned into \\(m\\) distinct groups containing \\(k_1 , k_2 , \\ldots , k_m\\) objects, respectively, where each object appears in exactly one group and \\(\\sum\\limits_{i=1}^{m}k_i=n\\), in \\(\\displaystyle N={n\\choose k_1k_2\\ldots k_m}=\\frac{n!}{k_1!k_2!\\ldots k_m!}\\) ways.\\ Proof: \\(N\\) is the number of ways all \\(n\\) of the elements of the set can be arranged in \\(m\\) groups where the order within each group is not important (i.e. rearrangements of elements in a group do not qualify as distinct groups). The number of distinct arrangements of the \\(n\\) elements in which the order of selection is important, \\(P_k^n\\), is equal to \\(N\\) multiplied by the number of ways each individual group of \\(k_i\\) can be selected in which the order is important, i.e. \\[\\begin{align*} P_n^n &amp;= N \\cdot P_{k_1}^{k_1} P_{k_2}^{k_2} \\cdots P_{k_m}^{k_m} \\\\ \\Rightarrow n! &amp;= N \\cdot k_1! k_2! \\cdots k_m! \\\\ \\Rightarrow N &amp;= \\frac{n!}{k_1! k_2! \\cdots k_m!} \\end{align*}\\] 8.0.2 Combinations Theorem Given a set of \\(n\\) elements, the number of possible ways to select a subset of size \\(k\\), without regard to the order of their selection, is \\(\\frac{n!}{k!(n-k)!}\\).\\ Proof: This theorem is a special case of the Lemma with \\(n=n\\), \\(m=2\\), \\(k_1=k\\) and \\(k_2=n-k\\). thus, \\[\\displaystyle N=\\frac{n!}{k!(n-k)!}\\] The formula \\(\\displaystyle \\frac{n!}{k!(n-k)!}\\) is denoted in a number of ways, depending on the author. Denotations may be \\(C_k^n\\), \\(_nC_k\\), \\(C_{n,k}\\), \\(C(n,k)\\), and \\({n\\choose k}\\). Throughout this book, the form \\({n\\choose k}\\) is used and may be read “\\(n\\) choose \\(k\\) objects.” 8.0.3 Theorem For any integer \\(a\\) such that \\(0\\leq a\\leq k\\), \\[ {n\\choose k} = \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)}{n-a\\choose k-a} \\] Proof: \\[\\begin{align*} {n\\choose k} &amp;= \\frac{n!}{k!(n-k)!} \\\\ &amp;= \\frac{n(n-1)!}{k(k-1)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)!}{k(k-1)(k-2)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)(n-a)!}{k(k-1)(k-2)\\cdots(k-a+1)(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-a+a-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)+(a-k)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)-(k-a)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot {n-a\\choose k-a} \\end{align*}\\] "],
["summation.html", "9 Summation 9.1 Theorems of Summation", " 9 Summation 9.1 Theorems of Summation 9.1.1 Theorem If \\(c\\) is a constant then \\[\\sum\\limits_{i=1}^{n}c = nc\\] Proof: \\[ \\sum\\limits_{i=1}^{n}c = \\underbrace{c+c+\\cdots+c}_{n\\ \\rm terms} = nc \\] 9.1.2 Theorem If \\(a_1,a_2,\\ldots,a_n\\) are real numbers and \\(c\\) is a constant, then \\[ \\sum\\limits_{i=1}^{n}ca_i = c\\sum\\limits_{i=1}^{n}a_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}ca_i &amp;= ca_1 + ca_2 + \\cdots + ca_n \\\\ &amp;= c(a_1+a_2+\\cdots+a_n) \\\\ &amp;= c\\sum\\limits_{i=1}^{n}a_i \\end{align*}\\] 9.1.3 Theorem If \\(a_1,_2,\\ldots,a_n\\) are real numbers and \\(b_1,b_2,\\ldots,b_n\\) are real numbers, then \\[ \\sum\\limits_{i=1}^{n}(a_i+b_i) = \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}(a_i+b_i) &amp;= a_1 + b_1 + a_2 + b_2 + \\cdots + a_n + b_n \\\\ &amp;= a_1 + a_2 + \\cdots + a_n + b_1 + b_2 + \\cdots + b_n \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\end{align*}\\] 9.1.4 Theorem If \\(a_i\\) and \\(b_j\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j = a_{+} b_{+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_j\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i b_{+} \\\\ &amp;= b_{+} \\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= a_{+} b_{+} \\end{align*}\\] 9.1.5 Theorem If \\(a_i\\) is a real number for \\(i=1,2,\\ldots,n\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b = m a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b &amp;= \\sum\\limits_{i=1}^{n}m a_i b \\\\ &amp;= m b\\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= m a_{+} b \\end{align*}\\] 9.1.6 Theorem If \\(a_j\\) is a real number for \\(j=1,2,\\ldots,m\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b = n a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b &amp;= \\sum\\limits_{i=1}^{n}\\bigg( b \\sum\\limits_{j=1}^{m} a_j \\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_{+}b \\\\ &amp;= n a_{+} b \\end{align*}\\] 9.1.7 Theorem If \\(a_i\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} = \\sum\\limits_{i=1}^{n}a_ib_{i+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_{ij}\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_ib_{i+} \\end{align*}\\] 9.1.8 Theorem If \\(a_j\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[\\ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} = \\sum\\limits_{i=1}^{n}a_jb_{+ j} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} &amp;= a_1b_{11}+a_2b_{12}+\\cdots+a_mb_{1m} \\\\ &amp; \\ \\ \\ \\ +a_1b_{21}+a_2b_{22}+\\cdots+a_mb_{2m} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_1b_{n1}+a_1b_{n1}+\\cdots+a_1b_{nm} \\\\ &amp;= a_1b_{11}+a_1b_{21}+\\cdots+a_1b_{n1} \\\\ &amp; \\ \\ \\ \\ +a_2b_{12}+a_2b_{22}+\\cdots+a_2b_{n2} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_mb_{1m}+a_mb_{2m}+\\cdots+a_nb_{nm} \\\\ &amp;= a_1(b_{11}+b_{21}+\\cdots+b_{n1}) \\\\ &amp; \\ \\ \\ \\ +a_2(b_{12}+b_{22}+\\cdots+b_{n2}) \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_m(b_{1m}+b_{2m}+\\cdots+b_{nm}) \\\\ &amp;= a_1b_{+ 1}+a_2b_{+ 2}+\\cdots+a_mb_{+ m} \\\\ &amp;=\\sum\\limits_{j=1}^{m}a_jb_{+ j} \\end{align*}\\] "],
["variance-parameter.html", "10 Variance Parameter 10.1 Defining Variance With Expected Values 10.2 Unbiased Estimator 10.3 Computational Formulae", " 10 Variance Parameter 10.1 Defining Variance With Expected Values In the case of a discrete random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\sum\\limits_{x=0}^{\\infty}(x-\\mu)^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2-2\\mu x+\\mu^2)p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2p(x)-2\\mu x\\cdot p(x)+\\mu^2p(x)) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\sum\\limits_{x=0}^{\\infty}2\\mu x\\cdot p(x) + \\sum\\limits_{x=0}^{\\infty}\\mu^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\sum\\limits_{x=0}^{\\infty}x\\cdot p(x) + \\mu^2\\sum\\limits_{x=0}^{\\infty}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2\\\\ \\end{align*}\\] In the case of a continuous random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}(x-\\mu)^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2-2\\mu x+\\mu^2)f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2f(x)-2\\mu x\\cdot f(x)+\\mu^2f(x))dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\int\\limits_{-\\infty}^{\\infty}2\\mu x\\cdot f(x)dx + \\int\\limits_{-\\infty}^{\\infty}\\mu^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\int\\limits_{-\\infty}^{\\infty}x\\cdot f(x)dx + \\mu^2\\int\\limits_{-\\infty}^{\\infty}f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2 \\end{align*}\\] In general, these results may be summarized as follows:\\ \\[\\begin{align*} \\sigma^2 &amp;= E[(X-\\mu)^2] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)] \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align*}\\] 10.2 Unbiased Estimator \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg) &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{n(\\sum\\limits_{i=1}^{n}x_i)^2}{n^2}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2)-nE(\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big]\\\\\\\\ &amp;= \\frac{1}{n}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;=\\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n}\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Since \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg)\\neq\\sigma^2\\) it is a biased estimator. Notice, however, that the bias can be eliminated by dividing by \\(n-1\\) instead of by \\(n\\) \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg) &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - 2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{(\\sum\\limits_{i=1}^{n}x_i)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{n\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n^2} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-nE(\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2)-nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big] \\\\ &amp;= \\frac{1}{n-1}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\ &amp;= \\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n-1}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n-1}\\sigma^2 \\\\ &amp;=\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Thus \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg)\\) is an unbiased estimator of \\(\\sigma^2\\), and we define the estimator \\[s^2= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\] 10.3 Computational Formulae 10.3.1 Computational Formula for \\(\\sigma\\)^2 \\[\\begin{align*} \\sigma^2 &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i^2-2\\mu x_i+\\mu^2)}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-\\sum\\limits_{i=1}^{N}2\\mu x_i + \\sum\\limits_{i=1}^{N}\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\mu\\sum\\limits_{i=1}^{N}x_i+N\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2 -2\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\sum\\limits_{i=1}^{N}x_i + N\\Big(\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\Big)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N} + \\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] 10.3.2 Computational Formula for \\(s\\)^2 \\[\\begin{align*} s^2 &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\sum\\limits_{i=1}^{n}2\\bar x x_i + \\sum\\limits_{i=1}^{n}\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\bar x\\sum\\limits_{i=1}^{n}x_i+n\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n}x_i + n\\Big(\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\Big)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n} + \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] "]
]
