---
  title: The Bernoulli Distribution
  output:
    html_document:
      toc: true
      fig_caption: true
      number_sections: true
---

\chapter{The Bernoulli Distribution}
  \label{Bernoulli}

```{r, echo=FALSE}
library(ggplot2)
library(RColorBrewer)
greens.pal <- rev(brewer.pal(9, "Greens"))
```

\section{Probability Mass Function}
\label{Bernoulli_pmf}

A random variable is said to have a Bernoulli Distribution with parameter $p$ if its probability mass function is:
$$
p(x)=\left\{ 
  	\begin{array}{ll}
			p^x(1-p)^{1-x},	& x=0,1\\
			0 		& \mathrm{otherwise}
		\end{array} 
	\right. 
$$
Where  $p$ is the probability of a sucess.

\section{Cumulative Distribution Function}
\label{Bernoulli_cdf}

$$
P(x)=\left\{
  	\begin{array}{lll}
			0   & x<0\\
			1-p & x=0\\
			1   & 1\leq x
		\end{array}
	\right. 
$$

```{r, echo=FALSE, fig.cap='The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $p=.4$.  Note that this is identical to a Binomial Distribution with parameters $n=1$ and $p=.4$.', fig.width=6, fig.height=3}
Bern <- data.frame(x=c(0:1, 0:1), 
                   p = c(.4, .6, .4, 1.0), 
                   display = factor(rep(c("PMF", "CDF"), c(2, 2)),
                                    c("PMF", "CDF")))
ggplot(Bern, aes(x=x, y=p)) + 
  geom_bar(stat="identity", position="dodge", fill=greens.pal[1]) + 
  scale_x_continuous(breaks=0:1) +
  scale_y_continuous(breaks=seq(0, 1, by=.2)) + 
  facet_grid(~ display) + xlab("x") + ylab("Probability") 
```

\section{Expected Values}
  \label{Bernoulli-expected-values}
$$
\begin{aligned}
E(X) &= \sum\limits_{i=0}^{1} x\cdot p(x)\\
     &= \sum\limits_{i=0}^{1} x \cdot p^{x} (1-p)^{1-x}\\
     &= 0 \cdot p^{0} (1-p)^{1-0} + 1 \cdot p^{1} (1-p)^{1-1}\\
     &= 0 + p (1-p)^{0}  \\
     &= p
\end{aligned}
$$ 
  
*************************
  
$$
\begin{aligned}
E(X^{2}) &= \sum\limits_{i=0}^{1} x^2 \cdot p(x)\\
    &= \sum\limits_{i=0}^{1} x^{2} \cdot p^x (1-p)^{1-x}\\
    &= \sum\limits_{i=0}^{1} 0^{2} \cdot p^0 (1-p)^{1-0} + 
        1^2 \cdot p^1 (1-p)^{1-1}\\
    &= 0 \cdot 1 \cdot 1 + 1 \cdot p \cdot 1\\
    &= 0 + p \\
    &= p
\end{aligned}
$$

*************************  

$\mu = E(X) = p$

*************************

$\sigma^2 
  = E(X^2) - E(X)^2
  = p-p^2
  = p(1-p)
$

*************************

\section{Moment Generating Function}
  \label{Bernoulli-mgf}
$$
\begin{aligned}
M_{X}(t)
	&= E(e^{tX})
	 = \sum\limits_{i=0}^{1} e^{tx} p(x)
	 = \sum\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\
  &= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}
	 = (1-p) + e^t p
	 =pe^t + (1-p)
\end{aligned}
$$

*************************

$$M^{(1)}_X(t) = pe^t$$
$$M^{(2)}_X(t) = pe^t$$

*************************

$$E(X)
	=M^{(1)}_X(0)
	= pe^0
	= p \cdot 1
	= p$$

*************************

$$E(X^2)
	= M^{(2)}_X(0)
	= pe^0
	= p$$

*************************

$$\mu
	= E(X)
	= p$$

*************************

$$\sigma^2
	= E(X^2) - E(X)^2
	= p - p^2
	= p (1-p)$$

\section{Theorems for the Bernoulli Distribution}
	\label{Bernoulli-theorems}

\subsection{Validity of the Distribution}
	\label{Bernoulli-theorem1}
$$\sum\limits_{x=0}^{1}p^x(1-p)^{1-x}=1$$

_Proof:_

$$
\sum\limits_{x=0}^{1} p^x (1-p)^{1-x}
	= p^0 (1-p)^1 + p^1 (1-p)^0
	= (1-p) + p
	= 1$$\ \rule{.05in}{.05in}


\subsection{Sum of Bernoulli Random Variables}
	\label{Bernoulli-theorem2}
Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y=\sum\limits_{i=1}^{n}X_i$.  
Then $Y\sim$ Binomial$(n,p)$  

_Proof:_
$$
\begin{aligned}
M_Y(t)
	 = E(e^{tY})
	 = E(e^{tX_1} e^{tX_2} \cdots e^{tX_n})
	 = E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n})\\
  &= (pe^t+(1-p)) (pe^t+(1-p)) \cdots (pe^t+(1-p))
	 = (pe^t+(1-p))^n
\end{aligned}
$$
Which is the mgf of a Binomial random variable with parameters $n$ and $p$.  
	Thus, $Y\sim$ Binomial$(n,p)$. \rule{.05in}{.05in}

