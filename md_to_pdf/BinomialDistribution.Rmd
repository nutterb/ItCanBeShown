---
  title: The Binomial Distribution
  output:
    html_document:
      toc: true
      fig_caption: true
      number_sections: true
---

\chapter{The Binomial Distribution}
  \label{Binomial}
  \setcounter{section}{0}
  
```{r, echo=FALSE}
library(ggplot2)
library(plyr)
library(RColorBrewer)
greens.pal <- rev(brewer.pal(9, "Greens"))
```

\section{Probability Mass Function}
\label{BinomialDistribution_pmf}

A random variable is said to follow a Binomial distribution with parameters $n$ and $p$ if its probability mass function is:\\
$$p(x)=
	\left\{
		\begin{array}{ll}
			{n \choose x} p^x (1-p)^{n-x},	& x=0,1,2,\ldots,n\\
			0 				& \mathrm{otherwise}
		\end{array}
	\right.
$$

Where $n$ is the number of trials performed and $p$ is the probability of a success on each individual trial.

\section{Cumulative Distribution Function}
	\label{Binomial-cdf}
$$ P(x)=
	\left\{
		\begin{array} {lll}
			0							& x<0\\
			\sum\limits_{i=0}^{x} {n \choose i} p^i (1-p)^{n-i} 	& 0 \leq x=0,1,2,\ldots,n\\
			1 							& n\leq x
		\end{array}
	\right.
$$
A recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:
$$ 
\begin{aligned}
F(x+1)
	&= {n\choose x+1} p^{x+1} (1-p)^{n-(x+1)}\\
	&= \frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)}\\
  &= \frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1}\\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \cdot p^x \frac{(1-p)^{n-x}}{(1-p)}\\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)!} \cdot \frac{p}{1-p} p^x (1-p)^{n-x}\\
  &= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\\
  &= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot {n\choose x} p^x (1-p)^{n-x}\\
	&= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot F(x)
\end{aligned}
$$


```{r, echo=FALSE, fig.height=3, fig.width=6, caption='The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $n=10$ and $p=.4$.'}
Binom <- expand.grid(x=0:10, 
                     display=factor(c("PMF", "CDF"), c("PMF", "CDF")))
Binom <- transform(Binom,
                   p = ifelse(display %in% "PMF", 
                              dbinom(x, 10, .4),
                              pbinom(x, 10, .4)))

ggplot(Binom, aes(x=x, y=p)) + 
  geom_bar(stat="identity", position="dodge", fill=greens.pal[1]) + 
  facet_grid(~ display) + 
  xlab("x") + ylab("Probability") + 
  scale_x_continuous(breaks=0:10)

```

\section{Expected Values}
	\label{Binomial-expected-value}
Let $X$ be a binomial random variable with parameters $n$ and $p$.  The expected
value of $X$ is:

$$
\begin{aligned}
E(X)
	&= \sum\limits_{x=0}^n x \cdot p(x)\\
	&= \sum\limits_{x=0}^n x {n\choose x} p^x (1-p)^{n-x}\\
\color{red}{\text{For convenience, let $q = (1-p)$}} &\\
  &= \sum\limits_{x=0}^n x {n\choose x} p^x q^{n-x}\\
  &= \sum\limits_{x=0}^n x {n\choose x} p^x q^{n-x}\\
  &= 0 \cdot {n\choose 0}p^0q^n+1 \cdot {n\choose 1}p^1q^{n-1} 
      + \cdots + n{n\choose n}p^nq^{n-n}\\
  &= 0 + 1{n\choose 1}p^1q^{n-1} + 2{n\choose 2}p^2q^{n-2} 
      + \cdots + n{n\choose n}p^nq^{n-n}\\
  &= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\
  &= np [q^{n-1} + (n-1)pq^{n-2} + \cdots + p^{n-1}]\\
  &= np \Big[{n-1\choose 0}p^0q^{n-1} + {n-1\choose 1}p^1q^{(n-1)-1}
             + \cdots + {n-1\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\Big]\\
  &= np (\sum\limits_{x=0}^{n-1}{n-1\choose x}p^xq^{(n-1)-x})\\
\color{red}{\text{By the Binomial Theorem, $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$}} &\\
  &= np(p+q)^{n-1}\\
\color{red}{\text{Resubstituting $(1-p)$ for $q$ gives us}} &\\
	&= np(p+(1-p))^{n-1}\\
  &= np(p+1-p)^{n-1}\\
	&= np(1)^{n-1}\\
	&= np(1)\\
	&= np\\
\end{aligned}
$$

*************************

The Expected Value of $X^2$ is:

$$
\begin{aligned}
E(X^2)
	&= \sum\limits_{x=0}^{n} x^2 p(x)\\
	&= \sum\limits_{x=0}^{n} x^2 {n\choose x} p^x (1-p)^{n-x}\\
\color{red}{\text{For convenience, let $q=(1-p)$}} &\\
	&= \sum\limits_{x=0}^{n} x^2 {n\choose x} p^x q^{n-x}\\
  &= 0^2 \frac{n!}{0!(n-0)!} p^0q^n + 1^2 \frac{n!}{1!(n-1)!} p^1q^{n-1}
		+ \cdots + n^2 \frac{n!}{n!(n-n)!} p^nq^{n-n}\\
  &= 0 + 1 \frac{n!}{(n-1)!} pq^{n-1} + 2 \frac{n!}{1\cdot(n-2)!} p^2q^{n-2}
		+ \cdots + n \frac{n!}{(n-1)!(n-n)!} p^n\\
  &= np \Big[1 \frac{(n-1)!}{(n-1)!} p^0q^{n-1}
		+ 2 \frac{(n-1)!}{1(n-2)!} p^2q^{n-2}
		+ \cdots + n \frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\Big]\\
  &= np \Big[1 \frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + 
      \cdots + n \frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\Big]\\
  &= np \sum\limits_{x=1}^{n} x {n-1\choose x-1} p^{x-1}1^{(n-1)-(x-1)}\\
\color{red}{\text{Let $y=x-1$ and $n=m+1$}} &\\
\color{red}{\text{$\Rightarrow x=y+1$ and $m=n-1$}} &\\
	&= \sum\limits_{y=0}^{m} (y+1) {m \choose y} p^yq^{m-y}\\
  &= np \Big[ \sum\limits_{y=0}^{m} y {m \choose y} p^yq^{m-y} + {m \choose y} p^yq^{m-y}\Big]\\
  &= np \Big[ \sum\limits_{y=0}^{m} y {m \choose y} p^yq^{m-y} 
		+ \sum\limits_{y=0}^{m} {m \choose y} p^yq^{m-y}\Big]\\
\color{red}{\text{$\sum\limits_{y=0}^{m}y{m \choose y}p^yq^{m-y}$ is of the form} }&\\
\color{red}{\text{of the expected value of $Y$,} }&\\ 
\color{red}{\text{and $E(Y)=mp=(n-1)p$} }&\\
\color{red}{\text{$\sum\limits_{y=0}^{m}{m \choose y}p^yq^{m-y}$ is the sum of all} }&\\
\color{red}{\text{probabilities over the domain of $Y$,} }&\\
\color{red}{\text{which is 1.} }&\\
	&= np(mp+1)\\
	&= np[(n-1)p+1]\\
	&= np(np-p+1)\\ 
	&= n^2p^2 - np^2 + np
\end{aligned}
$$

*************************

The mean of $X$ can be calculated as 
$$\mu = E(X) = np$$

*************************

And the variance of $X$ can be calculated by 
$$
\begin{aligned}
\sigma^2
	&= E(X^2) - E(X)^2\\
	&= n^2p^2 - np^2 + np - n^2p^2\\
	&= -np^2 + np\\
	&= np(-p-1)\\
  &= np(1-p)
\end{aligned}
$$

*************************

\section{Moment Generating Function}
\label{Binomial-mgf}
$$
\begin{aligned}
M_X(t)
  &= E(e^{tX})=\sum\limits_{x=0}^{n}e^{tx}p(x)\\
	&= \sum\limits_{x=0}^{n}e^{tx}{n\choose x}p^x(1-p)^{n-x}\\
  &= \sum\limits_{x=0}^{n}{n\choose x}e^{tx}p^x(1-p)^{n-x}\\
	&= \sum\limits_{x=0}^{n}{n\choose x}(pe^{tx})^x(1-p)^{n-x}\\
\color{red}{\text{By Binomial Theorem REF} }&\\
\color{red}{\text{$\sum\limits_{x=0}^{n}{n\choose x}b^xa^{n-x}=(a+b)^n$} }&\\
	&= [(1-p)+pe^t]^n
\end{aligned}
$$

*************************

$$M_X^{(1)}(t)=n[(1-p)+pe^t]^{n-1}pe^t$$

*************************

$$
\begin{aligned}
M_X^{(2)}(t)
  &=n[(1-p)+pe^t]^{n-1}pe^t+n(n-1)[(1-p)+pe^t]^{n-2}(pe^t)^2\\
  &=npe^t[(1-p)+pe^t]^{n-1}+n(n-1)pe^{2t}[(1-p)+pe^t]^{n-2}\\
\end{aligned}
$$

*************************

$$
\begin{aligned}
E(X)
  &=M_X^{(1)}(0)\\
  &=n[(1-p)+pe^0]^{n-1}pe^0\\
  &=n[1-p+p^{n-1}p\\
  &=n(1)^{n-1}p=np
\end{aligned}
$$

*************************

$$
\begin{aligned}
E(X^2)
  &=M_X^{(2)}(0)=npe^0[(1-p)+pe^0]^{n-1}
	+n(n-2)pe^{2\cdot0}[(1-p)+pe^0]^{n-2}\\
  &=np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2}\\
  &=np(1)^{n-1}+n(n-1)p^2(1)^{n-2}=np+n(n-1)p^2=np+(n^2-n)p^2\\
  &=np+n^2+n^2p^2-np^2
\end{aligned}
$$

*************************

$$\mu=E(X)=np$$

*************************

$$
\begin{aligned}
\sigma^2
  &=E(X^2)-E(X)^2\\
  &=np+n^2p^2-np^2-n^2p^2\\
  &=np-np^2\\
  &=np(1-p)
\end{aligned}
$$


\section{Maximum Likelihood Estimator}
\label{Binomial-mle}
Since $n$ is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for $n$.  The mean and variance can both be estimated from the single parameter $p$.

Let $X$ be a Binomial random variable with parameter $p$ and $n$ outcomes $(x_1,x_2,\ldots,x_n)$.  Let $x_i=0$ for a failure and $x_i=1$ for a success.  In other words, $X$ is the sum of $n$ Bernoulli trials with equal probability of success and $X=\sum\limits_{i=1}^{n}x_i$.

\subsection{Likelihood Function}
\label{Binomial-mle-likelihood}
$$
\begin{aligned}
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta)\\
	&= P(x_1|\theta) P(x_2|\theta) \cdots P(x_n|\theta)\\
  &= [\theta^{x_1}(1-\theta)^{1-x_1}] [\theta^{x_2}(1-\theta)^{1-x_2}] \cdots [\theta^{x_n}(1-\theta)^{1-x_n}]\\
  &= \exp_\theta\bigg\{\sum\limits_{i=1}^{n}x_i\bigg\} \exp_{(1-\theta)}\bigg\{n-\sum\limits_{i=1}^{n}x_i\bigg\}\\
  &= \theta^X(1-\theta)^{n-X}
\end{aligned}
$$

\subsection{Log-likelihood Function}
\label{Binomial-mle-loglik}
$$
\begin{aligned}
\ell(\theta)
	&= \ln L(\theta)\\
	&= \ln\big(\theta^X(1-\theta)^{n-X}\big)\\
	&= X\ln(\theta)+(n-X)\ln(1-\theta)
\end{aligned}
$$

\subsection{MLE for $p$}
\label{Binomial-mle-p}
$$
\begin{aligned}
\frac{d\ell(p)}{d p}
	  &= \frac{X}{p}-\frac{n-X}{1-p}\\
  0 &= \frac{X}{p}-\frac{n-X}{1-p}\\
  \frac{X}{p} &= \frac{n-X}{1-p}\\
  (1-p)X &= p(n-X)\\
  X-pX &= np-pX\\
  X &= np\\
  \frac{X}{n} &= p
\end{aligned}
$$

So $\hat p = \frac{X}{n} = \frac{1}{n}\sum\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $p$.

\section{Theorems for the Binomial Distribution}
\label{Binomial-theorems}

\subsection{Validity of the Distribution}
\label{Binomial-theorem1}
$$\sum\limits_{x=0}^n{n\choose x}p^x(1-p)^{n-x} = 1$$

_Proof:_

$$
\begin{aligned}
  \sum\limits_{x=0}^n {n\choose x} p^x (1-p)^{n-x}
	&= \\
\color{red}{\text{$\sum\limits_{x=0}^n {n\choose x} a^x b^{n-x} (a+b)^n$.}}&\\
\color{red}{\text{See Binomial Theorem REF.}}&\\
	&= big(p + (1-p)\big)^n\\
	&= (1)^n\\
	&= 1
\end{aligned}
$$ \ \rule{.05in}{.05in}


\subsection{Sum of Binomial Random Variables}
\label{Binomial-theorem2}
Let $X_1,X_2,\ldots,X_k$ be independent random variables where $X_i$ comes from a Binomial distribution with parameters $n_i$ and $p$.  That is 
	$X_i\sim(n_i,p)$.\\
Let $Y = \sum\limits_{i=1}{k} X_i$.  Then $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,p)$.

_Proof:_

$$
\begin{aligned}
\displaystyle M_Y(t)
	&= E(e^{tY})\\
	&= E(e^{t(x_1 + X_2 + \cdots + X_k)}\\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_k})\\
  &= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_k})\\
  &= \prod\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i}\\
	&= [(1-p)+pe^t]^{\sum\limits_{i=1}^{k}n_i}
\end{aligned}
$$
\\
\\
Which is the mgf of a Binomial random variable with parameters 
	$\sum\limits_{i=1}^{k}n_i$ and \nolinebreak[4]$p$.  
	Thus $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,p)$.\rule{.05in}{.05in}

\subsection{Sum of Bernoulli Random Variables}
\label{Binomial-theorem3}
Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y = \sum\limits_{i=1}^{n}X_i$.

Then $Y\sim$Binomial$(n,p)$

_Proof:_
$$
\begin{aligned}
M_Y(t)
	&= E(e^{tY})\\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_n})\\
	&= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n})\\
  &= (pe^t+(1-p))(pe^t+(1-p))\cdots (pe^t+(1-p))\\
	&= (pe^t+(1-p))^n
\end{aligned}
$$

Which is the mgf of a Binomial random variable with parameters $n$ and $p$.  Thus, $Y\sim$ Binomial$(n,p)$. \rule{.05in}{.05in}
