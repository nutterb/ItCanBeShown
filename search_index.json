[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2018-02-02 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["analysis-of-variance.html", "2 Analysis of Variance 2.1 One-Way Design 2.2 Computational Formulas 2.3 Randomized Complete Block Design", " 2 Analysis of Variance 2.1 One-Way Design 2.1.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+} + \\bar x_{i+} - x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (x_{ij} - \\bar x{i+}) + (\\bar x_{i+} - \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++}) + (x_{ij} - \\bar x{i+}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + (x_{ij} - \\bar x_{i+})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\bigg(\\sum\\limits_{j=1}^{n_i} x_{ij} - \\sum\\limits_{j=1}^{n_i}\\bar x_{i+}\\bigg) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - n_i \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\big(x_{i+} - n_i \\frac{x_{i+}}{n_i}\\big) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\cdot 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2\\\\ \\end{align*}\\] The components are commonly referred to as \\[ SS_{Factor} = \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\] and \\[ SS_{Error} = \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\] Notice that \\(SS_{Factor}\\) compares the factor means to the overall mean, and it can be said that \\(SS_{Factor}\\) measures the variation between factors. \\(SS_{Error}\\) compares each observation to the overall mean, and can be said to describe the variation within factors. When \\(n_1 = n_2 = \\cdots n_i = n\\), the design is said to be balanced. 2.2 Computational Formulas \\(SS_{Total}\\) and \\(SS_{Factor}\\) can be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} x_{ij}^2 - x_{++} \\sum\\limits_{j=1}^{n_i}\\frac{1}{n_i}\\\\ \\end{align*}\\] See Theorem 39.3.1 \\[\\begin{align*} SS_{Factor} &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\frac{\\bar x_{i+}^2}{n_i} - \\bar x_{++} \\sum\\limits_{i=1}^{a}\\frac{1}{n_i} \\end{align*}\\] See Theorem 39.3.1 \\(SS_{Error}\\) does not simplify to a convenient form, but \\[\\begin{align*} SS_{Total} &amp;= SS_{Factor} + SS_{Error} \\\\ \\Rightarrow SS_{Error} &amp;= SS_{Total} - SS_{Factor} \\end{align*}\\] 2.3 Randomized Complete Block Design Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable. To implement this design, suppose we have \\(a\\) treatments we want to compare, and \\(b\\) blocks. We may analyze the data by use of the sums of squares, similar to the one-way design. 2.3.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} + \\bar x_{i+} - \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{+ j} + \\bar x_{++} - \\bar x_{+ +} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++}) + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++}) (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big] \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 + 0 + 0 + 0 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 + a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] It is shown that the cross products are equal to zero in Section 2.3.3 These terms are commonly referred to as \\[\\begin{align*} SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ SS_{Error} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] 2.3.2 Computational Formulae \\(SS_{Total}\\), \\(SS_{Factor}\\), and \\(SS_{Block}\\) can all be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} x_{ij}^2 - \\frac{x_{++}}{ab}\\\\ \\\\ SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{b}\\sum\\limits_{i=1}^{a}x_{i+}^2 - \\frac{x_{++}^2}{ab} \\\\ \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{a}\\sum\\limits_{j=1}^{b} x_{+ j}^2 - \\frac{x_{++}^2}{ab} \\end{align*}\\] See Theorem 39.3.1 \\(SS_{Error}\\) does not simplify to any convenient form, but may be calculated from the other terms as \\(SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}\\) 2.3.3 RCBD Cross Products The cross products of the RCBD design \\[\\begin{align*} 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp;= 0 \\end{align*}\\] Proof: \\[ 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\\\ \\ \\ = 2\\bigg(\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\bigg)\\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ \\bar x_{i+}\\bar x_{+ j} - \\bar x_{i+}\\bar x_{++} - \\bar x_{+ j}\\bar x_{++} + \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}\\bar x_{+ j} - \\bar x_{+ j}^2 + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}^2 - \\bar x_{i+}\\bar x_{+ j} + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}( -\\bar x_{++}^2 - \\bar x_{i+}^2 - \\bar x_{+ j}^2 + x_{ij}\\bar x_{i+} + x_{ij}\\bar x_{+ j} - 2 x_{ij}\\bar x_{++} - \\bar x_{i+}\\bar x_{+ j} \\\\ \\ \\ \\ \\ + 2\\bar x_{i+}\\bar x_{++} + 2\\bar x_{+ j}\\bar x_{++} ) \\\\ \\ \\ = 2\\bigg(-\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{++}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j} \\\\ \\ \\ \\ \\ - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2 x_{ij}\\bar x_{++} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ = 2\\bigg( \\frac{ab\\bar x_{++}^2}{a^2b^2} - \\frac{b}{b^2}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{a}{a^2}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[1]} =2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[2]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[3]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ ^{[4]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\] \\[ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[5]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\frac{2\\bar x_{++}^2}{ab} \\bigg)\\\\ \\ \\ = 2\\bigg(\\frac{4\\bar x_{++}^2}{ab} - \\frac{4\\bar x_{++}^2}{ab} + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\bigg)\\\\ \\ \\ = 2(0 + 0 + 0) \\\\ = 2(0) \\\\ = 0 \\] See Summation Theorem 35.1.7 See Summation Theorem 35.1.8 See Summation Theorem 35.1.4 See Summation Theorem 35.1.5 See Summation Theorem 35.1.6 Using the theorems in Chapter @ref{summation-chapter} it is can be shown that each of the three cross products is equal to zero. However, the physical tedium of reducing each cross product is much greater than the approach taken above. "],
["bernoulli-distribution.html", "3 Bernoulli Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function 3.3 Expected Values 3.4 Moment Generating Function 3.5 Theorems for the Bernoulli Distribution", " 3 Bernoulli Distribution 3.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} \\pi^x(1-\\pi)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(\\pi\\) is the probability of a success. 3.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-\\pi &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] (#fig:Bernoulli_Distribution)The graphs on the left and right show a Bernoulli Probability Distribution and Cumulative Distribution Function, respectively, with \\(\\pi=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(\\pi=.4\\). 3.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot \\pi^{x} (1-\\pi)^{1-x}\\\\ &amp;= 0 \\cdot \\pi^{0} (1-\\pi)^{1-0} + 1 \\cdot \\pi^{1} (1-\\pi)^{1-1}\\\\ &amp;= 0 + \\pi (1-\\pi)^{0}\\\\ &amp;= \\pi\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot \\pi^x (1-\\pi)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot \\pi^0 (1-\\pi)^{1-0} + 1^2 \\cdot \\pi^1 (1-\\pi)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot \\pi \\cdot 1 \\\\ &amp;= 0 + \\pi\\\\ &amp;= \\pi\\\\ \\\\ \\\\ \\mu &amp;= E(X) = \\pi\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\pi-\\pi^2 \\\\ &amp;= \\pi(1-\\pi) \\end{align*} \\] 3.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} \\pi^{x} (1-\\pi)^{1-x}\\\\ &amp;= e^{t0} \\pi^0 (1-\\pi)^{1-0} + e^t \\pi^t (1-\\pi)^{1-1}\\\\ &amp;= (1-\\pi) + e^t \\pi\\\\ &amp;=\\pi e^t + (1-\\pi) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= \\pi e^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= \\pi e^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= \\pi e^0\\\\ &amp;= \\pi e^0\\\\ &amp;= \\pi\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= \\pi e^0\\\\ &amp;= \\pi\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= \\pi\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\pi - \\pi^2 \\\\ &amp;= \\pi (1-\\pi) \\end{align*} \\] 3.5 Theorems for the Bernoulli Distribution 3.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}\\pi^x(1-\\pi)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} \\pi^x (1-\\pi)^{1-x} &amp;= \\pi^0 (1-\\pi)^1 + \\pi^1 (1-\\pi)^0 \\\\ &amp;= (1-\\pi) + \\pi \\\\ &amp;= 1 \\end{align*}\\] 3.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,\\pi)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (\\pi e^t+(1-\\pi)) (\\pi e^t+(1-\\pi)) \\cdots (\\pi e^t+(1-\\pi)) \\\\ &amp;= (\\pi e^t+(1-\\pi))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(\\pi\\). Thus, \\(Y\\sim\\) Binomial\\((n,\\pi)\\). "],
["binomial-distribution.html", "4 Binomial Distribution 4.1 Probability Mass Function 4.2 Cumulative Mass Function 4.3 Expected Values 4.4 Moment Generating Function 4.5 Maximum Likelihood Estimator 4.6 Theorems for the Binomial Distribution", " 4 Binomial Distribution 4.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(\\pi\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} \\pi^x (1-\\pi)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(\\pi\\) is the probability of a success on each individual trial. 4.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} \\pi^i (1-\\pi)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} \\pi^{x+1} (1-\\pi)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} \\pi^{x+1} (1-\\pi)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} \\pi^{x+1} (1-\\pi)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} \\pi \\cdot \\pi^x \\frac{(1-\\pi)^{n-x}}{(1-\\pi)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{\\pi}{1-\\pi} \\pi^x (1-\\pi)^{n-x} \\\\ &amp;= \\frac{\\pi}{1-\\pi} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} \\pi^x (1-\\pi)^{n-x} \\\\ &amp;= \\frac{\\pi}{1-\\pi} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} \\pi^x (1-\\pi)^{n-x} \\\\ &amp;= \\frac{\\pi}{1-\\pi} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] (#fig:Binomial_Distribution)The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(\\pi=.4\\). 4.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\ &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} \\pi^x (1-\\pi)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} \\pi^x q^{n-x} \\\\ &amp;= 0 \\cdot {n\\choose 0}\\pi^0q^n+1 \\cdot {n\\choose 1}\\pi^1q^{n-1} + \\cdots + n{n\\choose n}\\pi^nq^{n-n}\\\\ &amp;= 0 + 1{n\\choose 1}\\pi^1q^{n-1} + 2{n\\choose 2}\\pi^2q^{n-2} + \\cdots + n{n\\choose n}\\pi^nq^{n-n}\\\\ &amp;= n\\pi^1 q^{n-1} + n(n-1)\\pi^2q^{n-2} + \\cdots + n(n-1)\\pi^{n-1}q^{n-(n-1)} + n \\pi^n\\\\ &amp;= n\\pi [q^{n-1} + (n-1)\\pi q^{n-2} + \\cdots + \\pi^{n-1}]\\\\ &amp;= n\\pi \\Big[{n-1\\choose 0}\\pi^0q^{n-1} + {n-1\\choose 1}\\pi^1q^{(n-1)-1} + \\cdots + {n-1\\choose n-1}\\pi^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\ &amp;= n\\pi (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}\\pi^xq^{(n-1)-x}) \\\\ ^{[2]} &amp;= n\\pi(\\pi+q)^{n-1} \\\\ ^{[1]} &amp;= n\\pi(\\pi+(1-\\pi))^{n-1} \\\\ &amp;= n\\pi(\\pi+1-\\pi)^{n-1} \\\\ &amp;= n\\pi(1)^{n-1} \\\\ &amp;= n\\pi(1) \\\\ &amp;= n\\pi \\end{align*}\\] Let \\(q = (1 - \\pi)\\) By the Binomial Theorem (6.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} \\pi^x (1-\\pi)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} \\pi^x q^{n-x} \\\\ &amp;= 0^2 \\frac{n!}{0!(n-0)!} \\pi^0q^n + 1^2 \\frac{n!}{1!(n-1)!} \\pi^1q^{n-1} + \\cdots + n^2 \\frac{n!}{n!(n-n)!} \\pi^nq^{n-n} \\\\ &amp;= 0 + 1 \\frac{n!}{(n-1)!} \\pi q^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} \\pi^2q^{n-2} + \\cdots + n \\frac{n!}{(n-1)!(n-n)!} \\pi^n \\\\ &amp;= n\\pi \\Big[1 \\frac{(n-1)!}{(n-1)!} \\pi^0q^{n-1} + 2 \\frac{(n-1)!}{1(n-2)!} \\pi^2q^{n-2} + \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} \\pi^{n-1}\\Big] \\\\ &amp;= n\\pi \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} \\pi^{1-1} q^{n-1} + \\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} \\pi^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\ &amp;= n\\pi \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} \\pi^{x-1}1^{(n-1)-(x-1)} \\\\ ^{[2]} &amp;= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} \\pi^yq^{m-y} \\\\ &amp;= n\\pi \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} \\pi^yq^{m-y} + {m \\choose y} \\pi^yq^{m-y}\\Big] \\\\ &amp;= n\\pi \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} \\pi^yq^{m-y} + \\sum\\limits_{y=0}^{m} {m \\choose y} \\pi^yq^{m-y}\\Big] \\\\ ^{[3]} &amp;= n\\pi(m\\pi+1) \\\\ &amp;= n\\pi[(n-1)\\pi+1] \\\\ &amp;=n\\pi(n\\pi-\\pi+1) \\\\ &amp;=n^2\\pi^2 - n\\pi^2 + n\\pi \\end{align*}\\] \\(q = (1 - \\pi)\\) Let \\(y = x - 1\\) and \\(n = m + 1\\) \\(\\Rightarrow\\) \\(x = y + 1\\) and \\(m = n - 1\\) \\(\\sum\\limits_{y=0}^{m}y{m \\choose y}\\pi^yq^{m-y}\\) is of the form of the expected value of \\(Y\\), and \\(E(Y)=m\\pi=(n-1)\\pi\\). \\(\\sum\\limits_{y=0}^{m}{m \\choose y}\\pi^yq^{m-y}\\) is the sum of all probabilities over the domain of \\(Y\\) which is 1. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= n\\pi \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n^2\\pi^2 - n\\pi^2 + n\\pi - n^2\\pi^2 \\\\ &amp;= -n\\pi^2 + n\\pi \\\\ &amp;= n\\pi(-\\pi-1) \\\\ &amp;= n\\pi(1-\\pi) \\end{align*}\\] 4.4 Moment Generating Function \\[ \\begin{align*} M_X(t) &amp;= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}(\\pi e^{tx})^x(1-\\pi)^{n-x} \\\\ ^{[1]} &amp;= [(1-\\pi)+\\pi e^t]^n \\end{align*} \\] By the Binomial Theorem (6.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[ \\begin{align*} M_X^{(1)}(t) &amp;= n[(1 - \\pi) + \\pi e^t] ^ {n - 1} \\pi e^t\\\\ \\\\ M_X^{(2)}(t) &amp;= n[(1-\\pi) + \\pi e^t] ^ {n-1} \\pi e^t + n(n-1)[(1-\\pi) + \\pi e^t] ^ {n-2}(\\pi e^t)^2\\\\ &amp;= n\\pi e^t[(1-\\pi) + \\pi e^t] ^ {n-1} + n(n-1)\\pi e^{2t}[(1-\\pi) + \\pi e^t] ^ {n-2}\\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= n[(1-\\pi)+\\pi e^0]^{n-1}\\pi e^0 \\\\ &amp;= n[1-\\pi+\\pi^{n-1}\\pi\\\\ &amp;= n(1)^{n-1}\\pi &amp;= n\\pi\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= n\\pi e^0 [(1-\\pi) + \\pi e^0]^{n-1} + n(n-2) \\pi e^{2\\cdot0}[(1-\\pi) + \\pi e^0]^{n-2} \\\\ &amp;= n\\pi(1-\\pi+\\pi)^{n-2}+n(n-1)\\pi^2(1-\\pi+\\pi^{n-2} \\\\ &amp;= n\\pi (1)^{n-1} + n(n-1) \\pi^2 (1)^{n-2} \\\\ &amp;= n\\pi+n(n-1)\\pi^2 \\\\ &amp;= n\\pi+(n^2-n)\\pi^2 \\\\ &amp;= n\\pi + n^2 + n^2\\pi^2 - n\\pi^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= n\\pi \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n\\pi + n^2\\pi^2 - n\\pi^2 - n^2\\pi^2 \\\\ &amp;= n\\pi - n\\pi^2\\\\ &amp;= n\\pi(1-\\pi) \\end{align*}\\] 4.5 Maximum Likelihood Estimator Since \\(n\\) is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for \\(n\\). The mean and variance can both be estimated from the single parameter \\(\\pi\\). Let \\(X\\) be a Binomial random variable with parameter \\(\\pi\\) and \\(n\\) outcomes \\((x_1,x_2,\\ldots,x_n)\\). Let \\(x_i=0\\) for a failure and \\(x_i=1\\) for a success. In other words, \\(X\\) is the sum of \\(n\\) Bernoulli trials with equal probability of success and \\(X=\\sum\\limits_{i=1}^{n}x_i\\). 4.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\ &amp;= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\ &amp;= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\ &amp;= \\theta^X(1-\\theta)^{n-X} \\end{align*}\\] 4.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln L(\\theta) \\\\ &amp;= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\ &amp;= X\\ln(\\theta)+(n-X)\\ln(1-\\theta) \\end{align*}\\] 4.5.3 MLE for p \\[\\begin{align*} \\frac{d\\ell(p)}{d \\pi} &amp;= \\frac{X}{\\pi}-\\frac{n-X}{1-\\pi} \\\\ \\\\ \\\\ 0 &amp;= \\frac{X}{\\pi}-\\frac{n-X}{1-\\pi} \\\\ \\Rightarrow \\frac{X}{\\pi} &amp;= \\frac{n-X}{1-\\pi} \\\\ \\Rightarrow (1-\\pi)X &amp;= \\pi(n-X) \\\\ \\Rightarrow X-\\pi X &amp;= n\\pi-\\pi X \\\\ \\Rightarrow X &amp;= n\\pi \\\\ \\Rightarrow \\frac{X}{n} &amp;= \\pi \\\\ \\end{align*}\\] So \\(\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(\\pi\\). 4.6 Theorems for the Binomial Distribution 4.6.1 Validity of the Distribution \\[\\begin{align*} \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x} = 1 \\end{align*}\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^n {n\\choose x} \\pi^x (1-\\pi)^{n-x} \\\\ ^{[1]} &amp;= \\big(\\pi + (1-\\pi)\\big)^n \\\\ &amp;= (1)^n \\\\ &amp;= 1 \\end{align*}\\] By the Binomial Theorem (6.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) 4.6.2 Sum of Binomial Random Variables Let \\(X_1,X_2,\\ldots,X_k\\) be independent random variables where \\(X_i\\) comes from a Binomial distribution with parameters \\(n_i\\) and \\(\\pi\\). That is \\(X_i\\sim(n_i,\\pi)\\). Let \\(Y = \\sum\\limits_{i=1}{k} X_i\\). Then \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,\\pi)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\ &amp;= \\prod\\limits_{i=1}^{k} [(1-\\pi)+\\pi e^t]^{n_i} \\\\ &amp;= [(1-\\pi)+\\pi e^t]^{\\sum\\limits_{i=1}^{k}n_i} \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(\\sum\\limits_{i=1}^{k}n_i\\) and \\(\\pi\\). Thus \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,\\pi)\\). 4.6.3 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(\\pi\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Binomial\\((n,\\pi)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (\\pi e^t+(1-\\pi))(\\pi e^t+(1-\\pi))\\cdots (\\pi e^t+(1-\\pi)) \\\\ &amp;= (\\pi e^t+(1-\\pi))^n \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-test.html", "5 Binomial Test 5.1 Binomial Test", " 5 Binomial Test 5.1 Binomial Test The binomial test is used to look for evidence that the proportion of a Binomial distributed random variable may differ from a hypothesized (or previously observed) value. 5.1.1 Test Statistic The test statistic for a binomial test is the observed frequency of experimental subjects that exhibit the trait of interest. 5.1.2 Definitions Let \\(X\\) be a random variable following a binomial distribution with parameters \\(n\\) and \\(\\pi\\). Let \\(x\\) be the observed frequency of experimental subjects exhibiting the trait of interest. 5.1.3 Hypotheses The hypotheses for the Binomial test may take the following forms: For a two-sided test: \\[\\begin{align} H_0: \\pi = \\pi_0 \\\\ H_a: \\pi \\neq \\pi_0 \\end{align}\\] For a one-sided test: \\[\\begin{align} H_0: \\pi &lt; \\pi_0 \\\\ H_a: \\pi \\geq \\pi_0 \\end{align}\\] or \\[\\begin{align} H_0: \\pi &gt; \\pi_0 \\\\ H_a: \\pi \\leq \\pi_0 \\end{align}\\] 5.1.4 Decision Rule The decision to reject the null hypothesis is made when the observed value of \\(x\\) lies in the critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. In a two-sided test, the upper bound is shared equally in both tails. Due to the discrete nature of the distribution, the total probability in the tails may not equal \\(\\alpha\\). The figures below depict examples of rejection regions for selected values of the Binomial distribution parameters. The decision rule is: Reject \\(H_0\\) if \\(x &lt; Binomial(\\alpha/2, n, \\pi_0)\\) or \\(x &gt; Binomial(1 - \\alpha/2, n, \\pi_0)\\) Figure 5.1: The examples displayed use \\(n = 20\\). For the top, middle, and bottom examples, \\(\\pi\\) is set at 0.3, 0.5, and 0.75, respectively. Notice that in some cases, the rejection regions for \\(\\alpha = 0.10\\) and \\(\\alpha = 0.05\\) are identical. In the one-sided test, \\(\\alpha\\) is placed in only one tail. The figures below depict examples of rejection regions for selected values of the Binomial distribution parameters. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(x \\leq Binomial(\\alpha, n, \\pi_0)\\) For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(x \\geq Binomial(1 - \\alpha, n, \\pi_0)\\) Figure 5.2: The examples displayed use \\(n = 20\\). For the top, middle, and bottom examples, \\(\\pi\\) is set at 0.3, 0.5, and 0.75, respectively. 5.1.5 Power The derivations below make use of the following symbols: \\(x\\): The observed frequency of experimental units exhibiting the trait of interest. \\(n\\): The total number of experimental units. \\(\\pi_0\\): The proportion of the population that exhibits the trait of interest under the null hypothesis. \\(\\pi_a\\): The proportion of the population that exhibits the trait of interest under the alternative hypothesis. \\(\\alpha\\): The significance level. \\(\\gamma(\\pi)\\): The power of the test for the parameter \\(\\pi\\). \\(Binomial(\\alpha, n, \\pi)\\): A quantile of the Binomial distribution with a probability \\(\\alpha\\), and parameters \\(n\\) and \\(\\pi\\). \\(C\\): The critical region. Two Sided Test \\[\\begin{align} \\gamma(\\pi_a) &amp;= P_{\\pi_a}(x \\in C) \\\\ &amp;= P_{\\pi_a}(Binomial(\\alpha/2, n, \\pi_0) \\leq Binomial(\\alpha / 2, n, \\pi_a)) + \\\\ &amp; \\ \\ \\ \\ \\ \\ \\ P_{\\pi_a}(Binomial(1 - \\alpha / 2, n, \\pi_0) \\geq Binomial(1 - \\alpha / 2, n, \\pi_a)) \\end{align}\\] Left Sided Test \\[\\begin{align} \\gamma(\\pi_a) &amp;= P_{\\pi_a}(x \\in C) \\\\ &amp;= P_{\\pi_a}(Binomial(\\alpha, n, \\pi_0) \\leq Binomial(\\alpha, n, \\pi_a)) \\end{align}\\] Right Sided Test \\[\\begin{align} \\gamma(\\pi_a) &amp;= P_{\\pi_a}(x \\in C) \\\\ &amp;= P_{\\pi_a}(Binomial(1 - \\alpha, n, \\pi_0) \\geq Binomial(1 - \\alpha, n, \\pi_a)) \\end{align}\\] Since the Binomial distribution is discrete, the power curve has the interesting characteristic of not being monotonic. It is sometimes described as having a “sawtooth” appearance. This behavior means that a larger sample size is not always preferred. For example, in the following figure, a sample size of 10 has better power than a sample size of 12. Figure 5.3: Power for a Binomial test with \\(\\pi_0 = .15\\) and \\(\\pi_a = 0.25\\) "],
["binomial-theorem.html", "6 Binomial Theorem 6.1 Traditional Proof 6.2 General Approach 6.3 Other Theorems", " 6 Binomial Theorem The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions. Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach. Other useful theorems are provided at the end of this chapter. 6.1 Traditional Proof 6.1.1 Lemma: Pascal’s rule Let \\(n\\) and \\(x\\) be non-negative integers such that \\(x\\leq n\\). Then \\({n-1\\choose x} + {n-1\\choose x-1} = {n\\choose x}\\). Proof: \\[\\begin{align*} {n-1\\choose x} + {n-1\\choose x-1} &amp;= \\frac{(n-1)!}{x!(n-1-x)!} + \\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)!}\\\\ &amp;= \\frac{(n-1)!}{x(x-1)!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!} +\\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{n!}{x!(n-x)!} \\\\ &amp;= {n\\choose x} \\end{align*}\\] 6.1.2 The Binomial Theorem Let \\(a\\) and \\(b\\) be constants and let \\(n\\) be any positive integer. Then \\[(a+b)^n = \\sum\\limits_{x=0}^{n} {n\\choose x} a^{n-x} b^x\\] Proof: This proof is completed by mathematical induction. Base Step: \\(n=1\\) \\[\\begin{align*} (a+b)^1 &amp;= \\sum\\limits_{x=0}^{1} {1\\choose x} a^{1-x} b^x \\\\ &amp;= {1\\choose 0} a^{1-0} b^0 + {1\\choose 1} a^{1-1} b^1 \\\\ &amp;= 1\\cdot a\\cdot 1 + 1\\cdot 1\\cdot b \\\\ &amp;= a+b \\end{align*}\\] Inductive Step: Assume that the Theorem holds for \\(n\\), and show it is true for \\(n+1\\). \\[\\begin{align*} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= a(a+b)^n + b(a+b)^n \\\\ &amp;= a(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x + b^n) + b(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^n) \\\\ &amp;= (a^{n+1}+a\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^{n+1}) \\\\ &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x+1}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[1]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}a^{n-x+1}b^x) + (\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[2]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}{n\\choose x}a^{n-x+1}b^x) + \\sum\\limits_{x-1}^{n-1}{n\\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\\\ ^{[3]} &amp;= a^{n+1} + \\sum\\limits_{x+1}^{n}{n+1\\choose x}a^{n-x+1}b^x + b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{x=1}^{n}{n+1\\choose x}a^{(n+1)-x}b^x+b^{n+1} \\\\ ^{[4]} &amp;= \\sum\\limits_{x=0}^{n+1}{n+1\\choose x}a^{(n+1)-x}b^x \\end{align*}\\] This completes both the inductive step and the proof. \\(ab^n={n\\choose n}a^{n-n+1}b^n\\) which is the term for \\(x=n\\) in the first summation. \\(a^nb={n\\choose 0}a^{n-0}b^1\\) which is the term for \\(x=0\\) in the second summation. \\(\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-x+1}b^x\\) This step is made using Pascal’s Rule with \\(n=n-1\\). \\(a^{n+1}={n+1\\choose 0}a^{(n+1)-0}b^0\\) which is the term for \\(x=0\\) in the summation. \\(\\ \\ b^{n+1}={n+1\\choose n+1}a^{(n+1)-(n+1)}b^{n+1}\\) which is the term for \\(x=n+1\\) in the summation 6.2 General Approach 6.2.1 A Binomial Expansion Theorem This theorem and its corrolary are provided by Brunette (Brunette 2003–2007a). For any positive integer \\(n\\), let \\(B_n = (x_1+y_1) (x_2+y_2) \\cdots (x_n+y_n)\\). In the expansion \\(B_n\\), before combining possible like terms, the following are true: There will be \\(2^n\\) terms. Each of these terms will be a product of \\(n\\) factors. In each such product there will be one factor from each binomial (in \\(B_n\\)). Every such product of \\(n\\) factors, one from each binomial, is represented in the expansion. Proof: Proof is done by induction. For the case \\(n=1\\), the result is clear. Now assume that the theorem is true for a particular \\(n\\) and consider \\(B_{n+1}\\). \\[ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} \\] By the inductive assumption, \\(B_n = T_1 + T_2 + \\cdots + T_{2^n}\\) where each \\(T_i\\) is a product of \\(n\\) factors, one factor from each binomial. It follows that every term in the expansion of \\(B_n+1\\) is either of the type \\(T_ix_{n+1}\\) or \\(T_iy_{n+1}\\), for some \\(1\\leq i \\leq 2^n\\). But each term of either of the above types is clearly a product of \\(n+1\\) factors with one factor coming from each binomial. thus, if (ii) and (iii) are true for \\(B_n\\), then they are true for \\(B_n+1\\). Next, by the inductive assumption, the expansion of \\(B_n\\) is a sum of \\(2^n+2^n\\) terms, i.e., \\(2^{n+1}\\) terms. This completes the inductive step for (i). Lastly, it remains for us to consider a product of the type \\(p_1 p_2 \\cdots p_n p_{n+1}\\) where, for each \\(1\\leq i\\leq n+1\\), \\(p_i = x_i\\) or \\(p_i = y_i\\). By the inductive hypothesis, \\(p_1 p_2 \\cdots p_n\\) is a term in the expansion of \\(B_n\\). If \\(p_{n+1} = x_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_nx_{n+1}\\), and so of \\(B_{n+1}\\). Likewise, if \\(p_{n+1}=y_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_n y_{n+1}\\), and so of \\(B_{n+1}\\). This completes the inductive step and the proof. 6.2.2 Corollary: Binomial Theorem Let \\(x\\) and \\(y\\) be constants and let \\(n\\) be any positive integer. Then \\(\\displaystyle (x+y)^n = \\sum\\limits_{i=0}^{n} {n\\choose i} x^{n-i} y^i\\\\\\) Proof: Since each term in the expansion will have \\(n\\) terms, each term must follow the form \\(x^{n-i} y^i\\) for \\(0 \\leq i \\leq n\\), and in all, there are \\(2^n\\) such terms. For any given value of \\(i\\), the number of terms of the form \\(x^{n-i}y^i\\) is clearly the number of ways one can choose the \\(i\\) factors of \\(y\\) from the \\(n\\) available binomials, i.e., \\({n\\choose i}\\), which gives \\[(x+y)^n = \\sum\\limits_{i=0}^{n}{n\\choose i} x^{n-i} y^i\\] 6.3 Other Theorems 6.3.1 Theorem \\[{N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n}\\] where \\(0 \\leq n \\leq N_1 + N_2\\). Proof: Using the Binomial Theorem we establish \\[ (1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\\\ \\Rightarrow [{N_1\\choose 0}a^0+\\cdots+{N_1\\choose N_1}a^{N_1}]\\cdot [{N_2\\choose 0}a^0+\\cdots+{N_2\\choose N_2}a^{N_2}] \\\\ \\ \\ \\ \\ ={N_1+N_2\\choose 0}+{N_1+N_2\\choose 1}a+\\cdots +{N_1+N_2\\choose N_1+N_2}a^{N_1+N_2} \\] Expanding the left side of the equation gives \\[ {N_1\\choose 0}{N_2\\choose 0} + {N_1\\choose 0}{N_2\\choose 1}a + \\cdots + {N_1\\choose 0}{N_2\\choose N_2}a^{N_2} + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose 1}{N_2\\choose N_2}a^{N_2+1} + \\cdots + {N_1\\choose N_1}{N_2\\choose 0}a^{N_1} + {N_1\\choose N_1}{N_2\\choose 1}a^{N_1+1} \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\\\ = {N_1\\choose 0}{N_2\\choose 0}+{N_1\\choose 0}{N_2\\choose 1}a + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + {N_1\\choose 0}{N_2\\choose 2}a^2+{N_1\\choose 1}{N_2\\choose 1}a^2 + {N_1\\choose 2}{N_2\\choose 0}a^2 \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\] Notice that for any \\(n\\) where \\(0 \\leq n \\leq N_1 + N_2\\), the coefficient for \\(a^n\\), found by combining like terms, is \\({N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 1}{N_2\\choose n-1} + \\cdots+{N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose 0}{N_2\\choose n}\\) and, by the equivalence of the first equation in the proof, is equal to the coefficient \\({N_1 + N_2\\choose n}\\). 6.3.2 Theorem \\[\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}} = 1\\] for \\(0 \\leq n \\leq N_1 + N_2\\).\\ Proof: Theorem 6.3.1 establishes the equality \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\frac{\\sum\\limits_{i=1}^{n} {N_1\\choose i}{N_2\\choose n-i}} {{N_1+N_2\\choose n}} = 1 \\] References "],
["chebychevs-theorem.html", "7 Chebychev’s Theorem 7.1 Chebychev’s Theorem 7.2 Alternate Proof of Chebychev’s Theorem 7.3 Chebychev’s Theorem for Absolute Deviation", " 7 Chebychev’s Theorem 7.1 Chebychev’s Theorem In any finite set of numbers and for any real number \\(h &gt; 1\\), at least \\((1 - \\frac{1}{h^2}) \\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Under these conditions we know \\[|x_1-\\mu| &gt; h\\sigma,\\ |x_2-\\mu| &gt; h\\sigma, \\ldots,\\ |x_r-\\mu| &gt; h\\sigma\\] Squaring gives \\[(x_1-\\mu)^2 &gt; h^2\\sigma^2,\\ (x_2-\\mu)^2 &gt; h^2\\sigma^2,\\ldots,\\ (x_r-\\mu)^2 &gt; h^2\\sigma^2\\\\ \\ \\ \\ \\ \\Rightarrow\\sum\\limits_{i=1}^{r}(x_1-\\mu)^2 &gt; \\sum\\limits_{i=1}^{r}h^2\\sigma^2 = rh^2\\sigma^2 \\] Since all \\((x_i-\\mu)^2\\) must necessarily be positive, \\[\\begin{align*} \\sum\\limits_{i=1}^{r}(x_i-\\mu)^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2\\sigma^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ ^{[1]} \\Rightarrow rh^2\\sigma^2 &amp;&lt; n\\sigma^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2 &amp;&lt; n \\\\ \\ \\ \\ \\ \\Rightarrow\\frac{r}{n} &amp;&lt; \\frac{1}{h^2} \\end{align*}\\] \\(\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) \\(\\ \\ \\ \\ \\Rightarrow n\\sigma^2 = \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) and \\(\\frac{r}{n}\\) is the fraction of numbers outside \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). By the law of complements, the fraction of numbers inside the interval is \\(1 - \\frac{r}{n}\\), which implies \\(1 - \\frac{r}{n} &gt; 1 - \\frac{1}{h^2}\\). Thus, more than \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the points lie within \\(h\\) standard deviations of the mean, or within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). 7.2 Alternate Proof of Chebychev’s Theorem In any finite set of numbers and for any real number \\(h&gt;1\\), at least \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma,\\mu+h\\cdot\\sigma)\\).\\ Proof: The proof here is done for the discrete case, but is applicable also in the continuous case by replacing the summations with integrals (with integrals, the limits will be from \\(-\\infty\\) to \\(\\infty\\)). \\[\\begin{align*} \\sigma^2 &amp;= E(x-\\mu)^2 \\\\ &amp;= \\sum\\limits_{y=0}^{\\infty}(y-\\mu)^2p(y) \\\\ &amp;= \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu-h\\sigma+1}^{\\mu+h\\sigma-1}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y) \\\\ ^{[1]} \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y)\\\\ \\end{align*}\\] Since all the \\((y-\\mu)^2\\) must be positive, removing the middle term will surely result in this inequality. In both of these summations \\(y\\) is outside the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\), so \\[\\begin{align*} |y-\\mu| &amp;\\geq h\\sigma \\\\ \\Rightarrow (y-\\mu^2) &amp;\\geq h^2\\sigma^2 \\\\ \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}h^2\\sigma^2p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}h^2\\sigma^2p(y) \\\\ \\Rightarrow\\sigma^2 &amp;\\geq h^2\\sigma^2\\Big[\\sum\\limits_{y=0}^{\\mu-h\\sigma}p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}p(y)\\Big] \\end{align*}\\] The first summation is the sum of all probabilities that \\(y-\\mu &lt; h\\sigma\\), i.e. \\(P(y-\\mu &lt; h\\sigma)\\). Likewise, the second summation is \\(P(y-\\mu &gt; h\\sigma)\\). \\[\\begin{align*} \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(y-\\mu&lt;h\\sigma) + P(y-\\mu&gt;h\\sigma)] \\\\ \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(|y-\\mu|&gt;h\\sigma)] \\\\ \\Rightarrow \\frac{1}{h^2} &amp;\\geq P(|y-\\mu|&gt;h\\sigma) \\\\ \\Rightarrow 1-\\frac{1}{h^2} &amp;\\leq P(|y-\\mu|&gt;h\\sigma) \\end{align*}\\] 7.3 Chebychev’s Theorem for Absolute Deviation This theorem is provided by Brunette (Brunette 2003–2007b) In any finite set of numbers, and for any real number \\(h &gt; 1\\), at least \\(1 - \\frac{1}{h}\\) of the numbers lie within \\(h\\) absolute deviations of the mean, where the absolute deviation is defined \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}{n}|x_i-\\bar x|\\). In other words, \\(1-\\frac{1}{h}\\) of the numbers are in the interval \\((\\bar x-h\\cdot Ab , \\bar x+h\\cdot Ab)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Accordingly, \\[h \\cdot Ab \\leq |x_1-\\bar x| ,\\ h \\cdot Ab \\leq |x_1-\\bar x| ,\\ldots ,\\ h \\cdot Ab \\leq |x_1-\\bar x| \\] \\[\\begin{align*} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{r}|x_i-\\bar x| \\\\ \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{n}|x_i-\\bar x| \\\\ ^{[1]} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq n \\cdot Ab\\\\ \\Rightarrow \\frac{r}{n} &amp;\\leq \\frac{1}{h}\\\\ \\Rightarrow -\\frac{r}{n} &amp;\\geq -\\frac{1}{h}\\\\ \\Rightarrow 1-\\frac{r}{n} &amp;\\geq 1-\\frac{1}{h} \\end{align*}\\] \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) \\(\\Rightarrow n \\cdot Ab = \\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) Now \\(\\frac{r}{n}\\) is the fraction of numbers outside the interval. So \\(1-\\frac{r}{n}\\) is the fraction of numbers within \\(h\\) absolute deviations of the mean, or within the interval \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). References "],
["chi-square-distribution.html", "8 Chi-Square Distribution 8.1 Probability Distribution Function 8.2 Cumulative Distribution Function 8.3 Expected Values 8.4 Moment Generating Function 8.5 Maximum Likelihood Function 8.6 Theorems for the Chi-Square Distribution", " 8 Chi-Square Distribution 8.1 Probability Distribution Function A random variable \\(X\\) is said to have a Chi-Square Distribution with parameter \\(\\nu\\) if its probability distribution function is \\[f(x) = \\left\\{ \\begin{array}{ll} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} &amp; 0&lt;x,\\ 0&lt;\\nu\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\(\\nu\\) is commonly referred to as the degrees of freedom. 8.2 Cumulative Distribution Function The cumulative distribution function for the Chi-Square Distribution cannot be written in closed form. It’s integral form is expressed as \\[ F(x) = \\left\\{ \\begin{array}{ll} \\displaystyle\\int\\limits_{0}^{x} \\frac{t^{\\frac{\\nu}{2}-1}e^{-\\frac{t}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} dt &amp; 0&lt;x,\\ 0&lt;\\nu\\\\\\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:ChiSquare_Distribution)The graphs on the top and bottom depict the Chi-Square probability distribution and cumulative distribution functions, respectively, for \\(\\nu=4,7,10\\). As \\(\\nu\\) gets larger, the distribution becomes flatter with thicker tails. 8.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma\\Big(\\frac{\\nu}{2}+1\\Big)2^{\\frac{\\nu}{2}+1}\\Big] \\\\ &amp;= \\frac{\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{2\\nu}{2} \\\\ &amp;= \\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}+1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma(\\frac{\\nu}{2}+2)2^{\\frac{\\nu}{2}+2}\\Big] \\\\ &amp;= \\frac{\\Gamma\\Big(\\frac{\\nu}{2}+2\\Big)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{(\\frac{\\nu}{2}+1)\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\cdot 2^2=2\\Big(\\frac{\\nu}{2}+1\\Big)\\nu \\\\ &amp;= (\\nu+2)\\nu=\\nu^2+2\\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 8.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}e^{tx}\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{2tx}{2}-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-\\frac{2tx-x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{-2t+1}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{1-2t}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{-x}{\\frac{2}{1-2t}}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Big(\\frac{2}{1-2t}\\Big)^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big]\\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= (1-2t)^{-\\frac{\\nu}{2}} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\frac{\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1}(-2) \\\\ &amp;= \\frac{2\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= (-\\frac{\\nu}{2}-1)\\nu(1-2t)^{-\\frac{\\nu}{2}-2}(-2) \\\\ &amp;= (\\frac{2\\nu}{2}+2)\\nu(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu+2)\\nu)(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-2t)^{-\\frac{\\nu}{2}-2}\\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= \\nu(1-2\\cdot 0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu \\\\ M_X^{(2)}(0) &amp;= (\\nu^2+2\\nu)(1-2\\cdot 0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\nu\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 8.5 Maximum Likelihood Function Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from a Chi-square distribution with parameter \\(\\nu\\). 8.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\nu/2-1}e^{-x_1/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdot \\frac{x_2^{\\nu/2-1}e^{-x_2/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdots \\frac{x_n^{\\nu/2-1}e^{-x_n/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\nu/2-1}e^{-x_i/2}} {2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}e^{-x_i/2} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\sum\\limits_{i=1}^{n}\\frac{x_i}{2} \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\end{align*}\\] 8.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\big(L(\\theta)\\big) \\\\ &amp;= \\ln\\Bigg[ \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\Bigg] \\\\ &amp;= \\ln\\Bigg[ \\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) \\Bigg] + \\ln\\Bigg( \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\Bigg) + \\ln\\bigg(\\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}\\bigg) \\\\ &amp;= -n \\ln\\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i \\bigg) \\\\ &amp;= -n\\bigg( \\ln(2^{\\nu/2}) + \\Gamma\\Big(\\frac{\\nu}{2}\\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\bigg(\\frac{\\nu}{2} \\ln 2 + \\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n\\nu}{2} \\ln 2 - n\\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 8.5.3 MLE for \\(\\nu\\) \\[\\begin{align*} \\frac{d\\ell}{d\\nu} &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) \\cdot \\frac{1}{2} + 0 + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\Rightarrow \\frac{n}{2} \\ln 2 - \\frac{1}{2}\\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow n\\ln 2 - \\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln 2}{n} &amp;= \\frac{\\Gamma^\\prime\\big(\\frac{\\nu}{2}\\big)}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\end{align*}\\] Due to the complexity of the Gamma function in this equation, no solution can be developed for \\(\\nu\\) in closed form. Thus, we have to rely on numerical methods to obtain a solution to the equation and find the maximum likelihood estimator. 8.6 Theorems for the Chi-Square Distribution 8.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big] \\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}{2^{\\frac{\\nu}{2}} \\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 8.6.2 Sum of Chi-Square Random Variables Let \\(X_1 , X_2 , \\ldots , X_n\\) be independent Chi-Square random variables with parameter \\(\\nu_i\\), that is \\(X_i\\sim\\chi^2(\\nu_i),\\ i=1,2,\\ldots,n\\). Suppose \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\chi^2(\\sum\\limits_{i=1}^{n}\\nu_i)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}=E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-2t)^{-\\frac{\\nu_1}{2}}(1-2t)^{-\\frac{\\nu_2}{2}}\\cdots (1-2t)^{-\\frac{\\nu_n}{2}} \\\\ &amp;= (1-2t)^{\\sum\\limits_{i=1}^{n}\\nu_i} \\end{align*}\\] Which is the mgf of a Chi-Square random variable with parameter \\(\\sum\\limits_{i=1}^{n}\\nu_i\\). Thus \\(Y\\sim\\chi^2\\bigg(\\sum\\limits_{i=1}^{n}\\nu_i\\bigg)\\). 8.6.3 Multiple of a Chi-Square Random Variable Let \\(X\\) be a Chi-Square random variable with parameter \\(\\nu\\), that is \\(X\\sim\\chi^2(\\nu),\\ i=1,2,\\ldots,n\\). Suppose \\(Y = c \\cdot X\\). Then \\(Y\\sim Gamma(\\frac{\\nu}{2}, 2 \\cdot c)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY})=E(e^{tcX}) \\\\ &amp;= (1-2tc)^{-\\frac{\\nu}{2}} \\\\ &amp;= (1-2c \\cdot t)^{-\\frac{\\nu}{2}} \\end{align*}\\] Which is the mgf of a Gamma distributed variable with parameters \\(\\alpha = \\frac{\\nu}{2}\\) and \\(\\beta = 2c\\). Thus, \\(Y\\sim Gamma(\\frac{\\nu}{2}, 2 \\cdot c)\\). 8.6.4 Square of a Standard Normal Random Variable If \\(Z\\sim N(0,1)\\), then \\(Z^2\\sim\\chi^2(1)\\). Proof: \\[\\begin{align*} M_{Z^2}(t) &amp;= E(e^{tZ^2}) \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}e^{tz^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty}e^{tz^2} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(-2t+1)}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[1]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[2]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}e^{-u} \\frac{\\sqrt{2}u^{-\\frac{1}{2}}}{2(1-2t)^{\\frac{1}{2}}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}e^{-u}u^{-\\frac{1}{2}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-u}du \\\\ ^{[3]} &amp;= \\frac{1}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}}\\Gamma(\\frac{1}{2}) \\\\ &amp;= \\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{1}{2}}}=(1-2t)^{-\\frac{1}{2}} \\\\ \\end{align*}\\] \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\) when f(x) is an even function () Let \\(u=\\frac{z^2}{2}(1-2t) \\ \\ \\ \\ \\Rightarrow z=\\frac{\\sqrt{2}u^{\\frac{1}{2}}}{(1-2t)^{\\frac{1}{2}}}\\)     So \\(dz=\\frac{\\sqrt{2}u^{-\\frac{1}{2}}} {2(1-2t)^{\\frac{1}{2}}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) Which is the mgf of a Chi-Square random variable with 1 degree of freedom. Thus \\(Z^2\\sim\\chi^2(1)\\). "],
["combinations.html", "9 Combinations", " 9 Combinations 9.0.1 Lemma A set of \\(n\\) elements may be partitioned into \\(m\\) distinct groups containing \\(k_1 , k_2 , \\ldots , k_m\\) objects, respectively, where each object appears in exactly one group and \\(\\sum\\limits_{i=1}^{m}k_i=n\\), in \\(\\displaystyle N={n\\choose k_1k_2\\ldots k_m}=\\frac{n!}{k_1!k_2!\\ldots k_m!}\\) ways.\\ Proof: \\(N\\) is the number of ways all \\(n\\) of the elements of the set can be arranged in \\(m\\) groups where the order within each group is not important (i.e. rearrangements of elements in a group do not qualify as distinct groups). The number of distinct arrangements of the \\(n\\) elements in which the order of selection is important, \\(P_k^n\\), is equal to \\(N\\) multiplied by the number of ways each individual group of \\(k_i\\) can be selected in which the order is important, i.e. \\[\\begin{align*} P_n^n &amp;= N \\cdot P_{k_1}^{k_1} P_{k_2}^{k_2} \\cdots P_{k_m}^{k_m} \\\\ \\Rightarrow n! &amp;= N \\cdot k_1! k_2! \\cdots k_m! \\\\ \\Rightarrow N &amp;= \\frac{n!}{k_1! k_2! \\cdots k_m!} \\end{align*}\\] 9.0.2 Combinations Theorem Given a set of \\(n\\) elements, the number of possible ways to select a subset of size \\(k\\), without regard to the order of their selection, is \\(\\frac{n!}{k!(n-k)!}\\).\\ Proof: This theorem is a special case of the Lemma with \\(n=n\\), \\(m=2\\), \\(k_1=k\\) and \\(k_2=n-k\\). thus, \\[\\displaystyle N=\\frac{n!}{k!(n-k)!}\\] The formula \\(\\displaystyle \\frac{n!}{k!(n-k)!}\\) is denoted in a number of ways, depending on the author. Denotations may be \\(C_k^n\\), \\(_nC_k\\), \\(C_{n,k}\\), \\(C(n,k)\\), and \\({n\\choose k}\\). Throughout this book, the form \\({n\\choose k}\\) is used and may be read “\\(n\\) choose \\(k\\) objects.” 9.0.3 Theorem For any integer \\(a\\) such that \\(0\\leq a\\leq k\\), \\[ {n\\choose k} = \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)}{n-a\\choose k-a} \\] Proof: \\[\\begin{align*} {n\\choose k} &amp;= \\frac{n!}{k!(n-k)!} \\\\ &amp;= \\frac{n(n-1)!}{k(k-1)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)!}{k(k-1)(k-2)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)(n-a)!}{k(k-1)(k-2)\\cdots(k-a+1)(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-a+a-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)+(a-k)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)-(k-a)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot {n-a\\choose k-a} \\end{align*}\\] "],
["correlation-pearsons.html", "10 Correlation (Pearson’s) 10.1 Theorems on Pearson’s Correlation 10.2 Computational Formula for \\(\\rho\\)", " 10 Correlation (Pearson’s) Pearson’s correlation coefficient of the variables \\(X\\) and \\(Y\\) is a measure of the linear relationship between \\(X\\) and \\(Y\\). It is defined \\[\\rho = \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\cdot \\sigma_Y^2}}\\] Notice that if \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y,)=0\\) and \\(\\rho=0\\) and there is no linear relationship between the variables. 10.1 Theorems on Pearson’s Correlation 10.2 Computational Formula for \\(\\rho\\) \\[\\rho = \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\sum\\limits_{i=1}^{n}(x_i-\\mu_X)\\sum\\limits_{j=1}^{m}(y_i-\\mu_Y)}\\] Proof: \\[\\begin{align*} \\rho &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)\\frac{1}{N}} {\\sqrt{\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2}{N}\\frac{\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}{N}}} \\\\ &amp;= \\frac{\\frac{1}{N}\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\frac{1}{N}\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)} {\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)}} \\end{align*}\\] "],
["covariance.html", "11 Covariance 11.1 Definition of Covariance 11.2 Theorems on Covariance", " 11 Covariance 11.1 Definition of Covariance For any two random variables \\(X\\) and \\(Y\\), the covariance of \\(X\\) and \\(Y\\) is defined as \\[Cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)]\\] 11.2 Theorems on Covariance 11.2.1 Theorem Let \\(X\\) be a random variable. Then \\[Cov(X,X) = V(X)\\] Proof: \\[\\begin{align*} Cov(X,X) &amp;= E[(X-\\mu)(X-\\mu)] \\\\ &amp;= E[(X-\\mu)^2] \\\\ &amp;= V(X) \\end{align*}\\] 11.2.2 Theorem Let \\(X\\) and \\(Y\\) be random variables. Then \\[Cov(X,Y) = E(XY)-E(X)E(Y)\\] Proof: \\[\\begin{align*} Cov(X,Y) &amp;= E[(X-\\mu_x)(Y-\\mu_Y)] \\\\ &amp;= E[XY - X\\mu_y - Y\\mu_X + \\mu_X\\mu_Y] \\\\ &amp;= E(XY) - E(X)\\mu_Y - \\mu_XE(Y) + \\mu_X\\mu_Y \\\\ &amp;= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - 2E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - E(X)E(Y) \\end{align*}\\] 11.2.3 Covariance Let \\(X\\) and \\(Y\\) be random variables and let \\(a\\) and \\(b\\) be constants. Then \\[Cov(aX,bY) = abCov(X,Y)\\] Proof: \\[\\begin{align*} Cov(aX,bY) &amp;= E(aXbY) - E(aX)E(bY) \\\\ &amp;= abE(XY) - abE(X)E(Y) \\\\ &amp;= ab[E(XY) - E(X)E(Y)] \\\\ &amp;= abCov(X,Y) \\end{align*}\\] 11.2.4 Theorem Let \\(X_1 , X_2 , \\ldots , X_n\\) be random variables with \\(E(X_i) = \\mu_i\\) for \\(i = 1,2,\\ldots,n\\) and let \\(Y_1,Y_2,\\ldots,Y_m\\) be random variables with \\(E(Y_j) = \\phi_j\\) for \\(j=1,2,\\ldots,m\\). Also, let \\(a_1,a_2,\\ldots,a_n\\) and \\(b_1,b_2,\\ldots,b_m\\) be constants.\\ If \\(U_1 = \\sum\\limits_{i=1}^{n}a_iX_i\\) and \\(U_2 = \\sum\\limits_{i=1}^{m}b_iY_i\\), then \\[Cov(U_1,U_2) = \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jCov(X_i,Y_j)\\] Proof: \\[\\begin{align*} Cov(U_1,U_2) &amp;= E[(U_1-E(U_1))(U_2-E(U_2))] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-\\sum\\limits_{i=1}^{n}a_i\\mu_i\\big) \\big(\\sum\\limits_{j=1}^{m}b_jY_j-\\sum\\limits_{j=1}^{m}b_j\\phi_j\\big)\\Big] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_i(X_i-\\mu_i)\\big)\\big(\\sum\\limits_{j=1}^{m}b_j(Y_j-\\phi_j)\\big)\\Big] \\\\ &amp;= E\\Big[\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j(X_i-\\mu_i)(Y_j-\\phi_j)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jE[(X_i-\\mu_i)(Y_j-\\phi_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j\\ Cov(X_i,Y_j) \\end{align*}\\] 11.2.5 Theorem Let \\(X_1,X_2,\\ldots,X_n\\) be random variables with \\(E(X_i)=\\mu_i\\) for \\(i=1,2,\\ldots,n\\) and let \\(a_1,a_2,\\ldots,a_n\\) be constants. If \\(Y = \\sum\\limits_{i=1}^{n}a_iX_i\\) then \\[V(Y) = \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ }a_ia_jCov(X_i,X_j)\\] Proof: \\[\\begin{align*} V(Y) &amp;= E[(Y-\\mu_Y)^2] \\\\ &amp;= E[(Y-\\mu_Y)(Y-\\mu_Y)] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-a_i\\mu_i\\big) \\big(\\sum\\limits_{n=1}^{n}a_jX_j-a_j\\mu_j\\big)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2Cov(X_i,X_i)+ \\sum\\limits_{\\ \\ i\\neq}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+ 2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jCov(X_i,X_j) \\end{align*}\\] "],
["experimental-designs.html", "12 Experimental Designs 12.1 Designs in Categorical Data Analysis", " 12 Experimental Designs 12.1 Designs in Categorical Data Analysis Studies in Categorical Data Analysis can be classified into several designs. These designs fall into the following two categories: Retrospective Design: looks at and analyzes measurements that have already been taken. Prospective Design: specifies the measurements to be collected at a future time. 12.1.1 Case Control Study In case control studies, the marginal distribution of the response variable is fixed by the sampling design. In other words, researchers select particular numbers of each category of the response variable in order to ensure that enough of each case are included in the sample. The result is that the marginal distribution of the response is non-random. Unfortunately, in order to calculate conditional probabilities, the marginal distribution of interest must be random. The difference of proportions for the response and the relative risk are both based on the marginal distribution of the response, and are both invalid procedures in case-control studies. In taking the measurements, researchers idenitfy people who are already classified into the response variable, making the design retrospective. 12.1.2 Cross Sectional Study 12.1.3 Cohort Study In Cohort Studies, subjects make their own choice about which group in the explanatory variable to join and researchers monitor the subjects with respect to a response variable over a period of time. Both the explanatory and response variables are random and only the total sample size is fixed by the researcher. Thus, conditional probabilities may be computed for both the predictor and response variables; differences in proportions may be estimated; and the relative risk is defined for the response variable. Since subjects select the group in which they will be and a measurement of their response is taken later, cohort studies are prospective. 12.1.4 Randomized Study In randomized Studies, the researcher randomly assigns subjects to the explanatory variable and then observes their response (making this a prospective study). The marginal distribution of the explanatory variable is therefore fixed, and conditional probabilities may not be computed. The response variable, on the other hand, is random and conditional probabilites may be computed, as well as the difference of proportions and relative risk. 12.1.5 Summary of Designs conditional conditional probability probability difference of Relative Odds explanatory response proportions Risk Ratio Case Control xxx xxx Cross-Sectional xxx xxx xxx xxx xxx Cohort xxx xxx xxx xxx xxx Randomized xxx xxx xxx xxx "],
["exponential-distribution.html", "13 Exponential Distribution 13.1 Probability Distribution Function 13.2 Cumulative Distribution Function 13.3 Expected Values 13.4 Moment Generating Function 13.5 Maximum Likelihood Estimator 13.6 Theorems for the Exponential Distribution", " 13 Exponential Distribution 13.1 Probability Distribution Function A random variable is said to have an Exponential Distribution with parameter \\(\\beta\\) if its probability distribution function is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{\\beta}e^{\\frac{-x}{B}}, &amp; 0&lt;x,\\ \\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 13.2 Cumulative Distribution Function \\[\\begin{align*} F(x) &amp;= \\int\\limits_{0}^{x}\\frac{1}{\\beta}\\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}dt \\\\ &amp;= -\\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}\\Big|_0^x \\\\ &amp;= -\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\}-\\Big(-\\exp\\Big\\{{\\frac{-0}{\\beta}}\\Big\\}\\Big) \\\\ &amp;= \\exp\\Big\\{{\\frac{0}{\\beta}}\\Big\\}-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\\\ &amp;= 1-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\end{align*}\\] And so the cumulative distribution function is given by \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{\\frac{-x}{\\beta}}, &amp; 0&lt;x,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:Exponential_Distribution)The figures on the top and bottom display the Exponential probability and cumulative distirubtion functions, respectively, for \\(\\beta=1,3\\). 13.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}xf(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}xe^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{2-1}e^{\\frac{-x}{\\beta}}dx\\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^2\\Gamma(2)) \\\\ &amp;=\\frac{\\beta^2\\cdot 1!}{\\beta} \\\\ &amp;=\\beta \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2f(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^2e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{3-1}e^{\\frac{-x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^3\\Gamma(3)) \\\\ &amp;= \\frac{\\beta^3\\cdot 2!}{\\beta} \\\\ &amp;= 2\\beta^2 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= 2\\beta^2-\\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 13.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx}{\\beta}-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{-x(\\beta 1-\\beta t}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}(\\frac{-\\beta}{1-\\beta t})e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}\\cdot 0-\\frac{-1}{1-\\beta t}e^0 \\\\ &amp;= \\frac{1}{1-\\beta t}=(1-\\beta t)^{-1} \\\\ \\\\ \\\\ M_X^{(1)}(t) &amp;= -1(1-\\beta t)^{-2}(-\\beta) \\\\ &amp;= \\beta(1-\\beta t)^{-2} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= -2\\beta(1-\\beta t)^{-3}(-\\beta) \\\\ &amp;= 2\\beta^2(1-\\beta t)^{-3} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\beta(1-\\beta\\cdot 0)^{-2} \\\\ &amp;= \\beta(1-0)^{-2} \\\\ &amp;= \\beta(1)^{-2} \\\\ &amp;= \\beta \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= 2\\beta^2(1-\\beta\\cdot 0)^{-3} \\\\ &amp;= 2\\beta^2(1-0)^{-3} \\\\ &amp;= 2\\beta^2(1)^{-3} \\\\ &amp;= 2\\beta^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= 2\\beta^2 - \\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 13.5 Maximum Likelihood Estimator Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from an Exponential distribution with parameter \\(\\beta\\). 13.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= f(x_1|\\theta)f(x_2|\\theta)\\cdots f(x_n|\\theta)\\\\ &amp;= \\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_1}{\\theta}\\bigg\\} \\cdot\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\cdots\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\\\ &amp;= \\frac{1}{\\theta^n}\\exp\\bigg\\{-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\end{align*}\\] 13.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln(L(\\theta)) \\\\ &amp;= \\ln(1)-n\\ln(\\theta)-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= 0-n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= -n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] 13.5.3 MLE for \\(\\beta\\) \\[\\begin{align*} \\frac{d\\ell(\\beta)}{d\\beta} &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n}{\\beta} &amp;= \\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n\\beta^2}{\\beta} &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow n\\beta &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] So \\(\\hat\\beta=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(\\beta\\). 13.6 Theorems for the Exponential Distribution 13.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{B}}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx &amp;= -e^{\\frac{-x}{\\beta}}\\Big|_0^\\infty \\\\ &amp;= -e^{\\frac{-\\infty}{\\beta}}-(-e^{\\frac{-0}{\\beta}}) \\\\ &amp;= e^{\\frac{0}{\\beta}}-e^{\\frac{-\\infty}{\\beta}} \\\\ &amp;= 1-0 \\\\ &amp;= 1 \\end{align*}\\] 13.6.2 Sum of Exponential Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from an Exponential distribution with parameter \\(\\beta\\), i.e. \\(X_i\\sim\\)Exponential\\((\\beta)\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Gamma\\((n,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-1}(1-\\beta t)^{-1}\\cdots(1-\\beta t)^{-1} \\\\ &amp;= (1-\\beta t)^{-n} \\end{align*}\\] Which is the mgf for a Gamma random variable with parameters \\(n\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((n,\\beta)\\). "],
["functions.html", "14 Functions 14.1 Fundamental Concepts and Definitions 14.2 Identities and Inverses 14.3 Odd and Even Functions 14.4 Theorems", " 14 Functions 14.1 Fundamental Concepts and Definitions Much of this chapter is taken from the lectures of Dr. John Brunette, University of Southern Maine A function is a collection of ordered pairs in which no two pairs have the same first element. The set of all first members of the pairs is called the domain. The set of all second members of the pairs is called the range. Suppose now that for any function \\(f\\) we have two items \\(x\\) and \\(y\\) such that \\(x\\in dom(f)\\) and \\(y\\in ran(x)\\) where \\(dom(f)\\) and \\(ran(f)\\) denote the domain and range of \\(f\\), respectively. It is said that \\(f\\) maps \\(x\\) onto \\(y\\), written \\[f:\\ x\\mapsto y\\] It is common to write the \\(ran(f)\\) as some expression of \\(x\\). For example, \\(f: x\\mapsto x^2\\) takes each element in the domain, and pairs it with it’s square. The common shorthand for this is \\(f(x)=x^2\\), meaning that whatever appears between the parentheses following the \\(f\\) is to be squared. 14.1.1 Function Operations The three basic operations that can be performed on functions are addition, multipilication, and composition. For any two functions \\(f\\) and \\(g\\) these operations are defined as: Addition \\(\\lbrack f+g\\rbrack(x)=:\\big\\{\\big(x,f(x)+g(x)\\big)\\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Multiplication \\(\\lbrack f\\cdot g\\rbrack(x):=\\big\\{\\big(x,f(x)\\cdot g(x)\\big) \\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Composition \\(\\lbrack f\\circ g\\rbrack(x)=\\big\\{\\big(x,f\\big(g(x)\\big) \\mid x\\in dom(g)\\) and \\(g(x)\\in dom(f)\\big\\}\\) Notice that the composition \\(\\lbrack f\\circ g\\rbrack(x)=f \\circ g: g(x)\\mapsto f(x)\\). In other words, the result of \\(g\\) is then applied to \\(f\\) to produce the result of the composition. 14.2 Identities and Inverses Recall that addition and multiplication have identity properties. Specifically, for any real number \\(x\\), applying one of these identities returns the value \\(x\\), i.e. \\(x+0=x\\) and \\(x\\cdot 1\\)=x. Functions also have an identity, denoted \\(id(x)\\), that is defined as \\[id:\\ x\\mapsto x\\] Furthermore, the composition of \\(id\\) with \\(f\\) behaves in this way: \\[id\\circ f=f\\circ id=f\\] Functions may also exhibit the property of inverses that are exhibited by addition and multiplication. In the latter two, combining any real number \\(x\\) and its inverse returns the identity of that operation, i.e. \\(x+-x=0\\) and \\(x\\cdot x^{-1}=1,\\ x\\neq 0\\). Likewise, some functions have an inverse function. If a function \\(f\\) has an inverse \\(f^{-1}\\), then \\[f\\circ f^{-1}=f^{-1}\\circ f=id\\] On closer observation, we see \\[f^{-1}\\circ f\\big(dom(x)\\big)=f^{-1}\\Big(f\\big(dom(x)\\big)\\Big)=f^{-1}\\big(ran(x)\\big)=dom(x)\\] So \\(f^{-1}\\) must be the set of all ordered pairs \\((y,x)\\) where \\(x\\in dom(x)\\) and \\(y\\in ran(x)\\), i.e. \\(f^{-1}(x)=\\{(y,x) \\mid x\\in dom(x)\\) and \\(y\\in ran(x)\\}\\). By the definition of functions, no two first elements in \\(f^{-1}\\) can be the same. But the first elements in \\(f^{-1}\\) are the second elements in \\(f\\). So \\(f^{-1}\\) only exists if no two second elements in \\(f\\) are the same. We thus make the following definition: A function \\(f\\) is called a one-to-one function if it has no two ordered pairs with the same second element. For any one-to-one function \\(f\\), no two of the first elements are the same, and no two of the second elements are the same. Thus, \\(f^{-1}\\) is a function, because no two of its first elements are the same, and because the range of \\(f^{-1}\\) is the domain of \\(f\\), no two second elements in \\(f^{-1}\\) are the same, and \\(f^{-1}\\) is a one-to-one function. Thus, every one-to-one function has an inverse. If a function \\(f\\) is not one-to-one, however, then there exist two pairs in \\(f\\) that have the same second element. The inverse \\(f^{-1}\\) therefore has two pairs where the first element is the same. When such is the case, the definition of a function is violated, and \\(f^{-1}\\) cannot be a function. Thus, if a function is invertible, it must be one-to-one. 14.3 Odd and Even Functions A function is said to be even if for any real number \\(x,\\ f(-x)=f(x)\\). A function is said to be odd if for any real number \\(x,\\ f(-x)=-f(x)\\). If neither of these criteria are met, the function is simply said to be neither odd nor even. 14.4 Theorems 14.4.1 Operations on Even Functions Let \\(f\\) and \\(g\\) both be even functions. Then: \\([f+g](x)\\) is an even function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x)+g(-x) \\\\ &amp;= f(x)+g(x) \\\\ &amp;= [f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an even function. 14.4.2 Operations on Odd Functions Let \\(f\\) and \\(g\\) both be odd functions. Then: \\([f+g](x)\\) is an odd function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an odd function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\\\ &amp;= -[f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an odd function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= -f(x)\\cdot -g(x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= -f\\big(g(x)\\big) \\\\ &amp;= -[f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an odd function. 14.4.3 Operations on an Odd and Even Function Let \\(f\\) be an even function and let \\(g\\) both be an odd function. Then: \\([f+g](x)\\) is neither an odd nor an even function \\([f\\cdot g](x)\\) is an odd function \\([f\\circ g](x)\\) is an even function \\([g\\circ f](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\end{align*}\\] so \\([f+g](x)\\) is neither an odd nor an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot -g(x) \\\\ &amp;= -\\big(f(x)\\cdot g(x)\\big) \\\\ &amp;= -[f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an odd function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\[\\begin{align*} [g\\circ f](-x) &amp;= g\\big(f(-x)\\big) \\\\ &amp;= g\\big(f(x)\\big) \\\\ &amp;= [g\\circ f](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\end{itemize} 14.4.4 Derivatives and Anti-derivatives of Odd Functions Let \\(f\\) be an odd function and let \\(f^\\prime\\) and \\(F\\) denote the derivative and anti-derivative of \\(f\\), respectively. Then \\(f^\\prime\\) and \\(F\\) are both even functions. Proof: \\[\\begin{align*} f(-x) &amp;= -f(x)\\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack f(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack-f(x)\\big\\rbrack \\\\ \\Rightarrow f^\\prime(-x)\\cdot -1 &amp;= -f^\\prime(x) \\\\ \\Rightarrow -f^\\prime(-x) &amp;= -f^\\prime(x) \\\\ \\Rightarrow f^\\prime(-x) &amp;= f^\\prime(x) \\end{align*}\\] So \\(f^\\prime\\) is an even function. \\[\\begin{align*} f(-x) &amp;= -f(x) \\\\ \\Rightarrow \\int f(-x) &amp;= \\int-f(x)\\\\ \\Rightarrow F(-x)\\cdot-1 &amp;= -F(x)\\\\ \\Rightarrow -F(-x) &amp;= -F(x)\\\\ \\Rightarrow F(-x) &amp;= F(x) \\end{align*}\\] So \\(F\\) is also an even function.   14.4.5 Derivatives and Anti-derivatives of Even Functions Let \\(g\\) be an even function, and let \\(g^\\prime\\) and \\(G\\) denote the derivative and anti-derivative of \\(g\\), respectively. Then \\(g^\\prime\\) and \\(G\\) are both odd functions. Proof: \\[\\begin{align*} g(-x) &amp;= g(x) \\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack g(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack g(x)\\big\\rbrack \\\\ \\Rightarrow g^\\prime(-x)\\cdot -1 &amp;= g^\\prime(x) \\\\ \\Rightarrow -g^\\prime(-x) &amp;= g^\\prime(x) \\\\ \\Rightarrow g^\\prime(-x) &amp;= -g^\\prime(x) \\end{align*}\\] So \\(g^\\prime\\) is an odd function. \\[\\begin{align*} g(-x) &amp;= g(x)\\\\ \\Rightarrow \\int g(-x) &amp;= \\int g(x)\\\\ \\Rightarrow G(-x)\\cdot-1 &amp;= G(x)\\\\ \\Rightarrow -G(-x) &amp;= G(x)\\\\ \\Rightarrow G(-x) &amp;= -G(x) \\end{align*}\\] So \\(G\\) is also an odd function. "],
["gamma-distribution.html", "15 Gamma Distribution 15.1 Probability Distribution Function 15.2 Cumulative Distribution Function 15.3 Expected Values 15.4 Moment Generating Function 15.5 Maximum Likelihood Estimators 15.6 Theorems for the Gamma Distribution", " 15 Gamma Distribution 15.1 Probability Distribution Function A random variable \\(X\\) is said to have a Gamma Distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its probability distribution function is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}}{\\Gamma(\\alpha)\\beta^\\alpha}, &amp; 0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array} \\right. \\] Where \\(\\alpha\\) is a scale parameter and\\ \\(\\beta\\) is a shape parameter. 15.2 Cumulative Distribution Function The cumulative distribution function for the Gamma Distribution cannot be expressed in closed form. It’s interval form is expressed here. \\[F(x) = \\left\\{ \\begin{array}{ll} \\int\\limits_{0}^{x}\\frac{t^{\\alpha-1}e^{-\\frac{t}{\\beta}}}{\\Gamma(\\alpha)\\beta^\\alpha}, &amp; 0&lt;t,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ \\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:Gamma_Distribution)The figures on the left and right display the Gamma probability and cumulative distirubtion functions, respectively, for the combinations of \\(\\alpha=2,3\\) and \\(\\beta=1,3\\). 15.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} [\\Gamma(\\alpha+1)\\beta^{\\alpha+1}] \\\\ &amp;= \\frac{\\Gamma(\\alpha+1)\\beta^{\\alpha+1}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{\\alpha\\Gamma(\\alpha)\\beta^{\\alpha+1}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\alpha\\beta \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha+1}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} [\\Gamma(\\alpha+2)\\beta^{\\alpha+2}] \\\\ &amp;= \\frac{\\Gamma(\\alpha+2)\\beta^{\\alpha+2}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{(\\alpha+1)\\Gamma(\\alpha+1)\\beta^{\\alpha+2}} {\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{(\\alpha+1)\\alpha\\Gamma(\\alpha)\\beta^{\\alpha+2}} {\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\alpha(\\alpha+1)\\beta^2 \\\\ &amp;= (\\alpha^2+\\alpha)\\beta^2 \\\\ &amp;= \\alpha^2\\beta^2+\\alpha\\beta^2 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\alpha\\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^2\\beta^2 + \\alpha\\beta^2 - \\alpha^2\\beta^2 \\\\ &amp;= \\alpha\\beta^2 \\end{align*}\\] 15.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}e^{tx} x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{tx}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{tx-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{\\frac{\\beta tx}{\\beta}-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{\\frac{\\beta tx-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{-x\\frac{-\\beta t+1}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{-x\\frac{1-\\beta t}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\Big[\\Gamma(\\alpha)\\Big(\\frac{\\beta}{1-\\beta t}\\Big)\\alpha)\\Big] \\\\ &amp;= \\frac{\\Gamma(\\alpha)\\beta^\\alpha} {\\Gamma(\\alpha)\\beta^\\alpha(1-\\beta t)^\\alpha} \\\\ &amp;= \\frac{1}{(1-\\beta t)^\\alpha}=(1-\\beta t)^{-\\alpha} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\alpha(1-\\beta t)^{-\\alpha-1}(-\\beta) \\\\ &amp;= \\alpha\\beta(1-\\beta t)^{-\\alpha-1} \\\\ M_X^{(2)}(t) &amp;= (-\\alpha-1)\\alpha\\beta(1-\\beta t)^{-\\alpha-2}(-\\beta) \\\\ &amp;= (\\alpha+1)\\alpha\\beta^2(1-\\beta t)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1-\\beta t)^{-\\alpha-2} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0)=\\alpha\\beta(1-\\beta\\cdot 0)^{-\\alpha-1} \\\\ &amp;= \\alpha\\beta(1-0)^{\\alpha-1}=\\alpha\\beta(1)^{-\\alpha-1} \\\\ &amp;= \\alpha\\beta \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0)=(\\alpha^2\\beta^2+\\alpha\\beta^2)(1-\\beta 0)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1-0)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1)^{-\\alpha-2} \\\\ &amp;= \\alpha^2\\beta^2+\\alpha\\beta^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\alpha\\beta\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^2\\beta^2 + \\alpha\\beta^2 - \\alpha^2\\beta^2 \\\\ &amp;= \\alpha\\beta^2 \\end{align*}\\] 15.5 Maximum Likelihood Estimators Let \\(x_1,x_2,\\ldots,x_n\\) denote a random sample from a Gamma Distribution with parameters \\(\\alpha\\) and \\(\\beta\\). 15.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\alpha-1}e^{-x_1/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\frac{x_2^{\\alpha-1}e^{-x_2/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\cdots \\frac{x_n^{\\alpha-1}e^{-x_n/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\alpha-1}e^{-x_i/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\bigg(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}\\bigg)^n \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1}e^{-x_i/\\beta} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1}e^{-x_i/\\beta} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{\\sum\\limits_{i=1}^{n}-\\frac{x_i}{\\beta} \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{-\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\end{align*}\\] 15.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\bigg[ \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{-\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg] \\\\ &amp;= \\ln\\big( \\Gamma(\\alpha) \\beta^\\alpha \\big)^{-n} + \\ln\\bigg( \\exp \\bigg\\{ -\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\bigg) + \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg) \\\\ &amp;= -n\\ln\\big( \\Gamma(\\alpha) \\beta^\\alpha \\big) - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg) \\\\ &amp;= -n\\big[ \\ln\\big( \\Gamma(\\alpha)\\beta^\\alpha\\big) \\big] - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + \\sum\\limits_{i=1}^{n}(\\alpha-1)\\ln x_i \\\\ &amp;= -n\\ln\\Gamma(\\alpha) - n\\alpha\\ln\\beta - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + (\\alpha-1)\\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 15.5.3 MLE for \\(\\alpha\\) \\[\\begin{align*} \\frac{d\\ell}{d\\alpha} &amp;= -n\\frac{1}{\\Gamma(\\alpha)}\\Gamma^\\prime(\\alpha) - n\\ln\\beta - 0 + \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\\\ \\\\ 0 &amp;= -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\Rightarrow n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} &amp;= \\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln\\beta \\\\ \\Rightarrow \\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} &amp;= \\frac{1}{n}\\bigg( \\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln\\beta \\bigg) \\end{align*}\\] However, this must be solved numerically. Notice also that the MLE for \\(\\alpha\\) depends on \\(\\beta\\). 15.5.4 MLE for \\(\\beta\\) \\[\\begin{align*} \\frac{d\\ell}{d\\beta} &amp;= 0 - n\\alpha\\frac{1}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i + 0 \\\\ &amp;= -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\frac{n\\alpha}{\\beta} &amp;= \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow n\\alpha\\beta &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n\\alpha} \\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] This estimate, however, depends on \\(\\alpha\\). Since each estimator depends on the value of the other parameter, we must maximize the likelihood functions simulatneously. That is, we must simultaneously solve the system \\[ \\left\\{ \\begin{array}{rl} -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i &amp; = 0\\\\ -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i &amp; = 0\\\\ \\end{array} \\right. \\] Solving this system will require numerical methods. 15.5.5 Approximation of \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) Approximations of \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) can be obtained by noticing that\\ \\[\\begin{align*} \\frac{d\\ell}{d\\beta} &amp;= 0 \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n\\alpha}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha\\beta &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] So \\(\\widehat{\\alpha\\beta} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\). Recall that \\(\\alpha\\beta\\) and \\(\\alpha\\beta^2\\) are the mean and variance of the Gamma Distribution, respectively. We utilize \\[ \\frac{\\alpha\\beta^2}{\\alpha\\beta} = \\beta \\] If we assume that \\(\\widehat{\\alpha\\beta^2} = \\frac{1}{n-1}\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\), then \\[ \\frac{\\widehat{\\alpha\\beta^2}}{\\widehat{\\alpha\\beta}} = \\beta^* \\approx \\hat\\beta \\] Where \\(\\beta^*\\) denotes an approximation to \\(\\hat\\beta\\) We now substitute \\(\\beta^*\\) into \\[\\begin{align*} \\widehat{\\alpha\\beta} &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha^*\\beta^* &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha^* &amp;= \\frac{1}{n\\beta^*}\\sum\\limits_{i=1}^{n}x_i \\approx \\hat\\alpha \\end{align*}\\] Where \\(\\alpha^*\\) denotes an approximation to \\(\\hat\\alpha\\). This method of estimation is prone to error because \\(\\beta^*\\) is found through two levels of estimation and \\(\\alpha^*\\) is found through three levels of estimation. Surely, this process inflates the error of estimation. At this point, however, I have no information to indicate how badly the error of estimation is inflated, nor have I performed any investigation into this problem. 15.6 Theorems for the Gamma Distribution 15.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}x\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx = 1\\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}[\\Gamma(\\alpha)\\beta^\\alpha] \\\\ &amp;= \\frac{\\Gamma(\\alpha)\\beta^\\alpha}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 15.6.2 Sum of Gamma Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be Gamma distributed random variables with parameters \\(\\alpha_i\\) and \\(\\beta\\), that is \\(X_i\\sim\\)Gamma\\((\\alpha_i,\\beta)\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\)Gamma\\((\\sum\\limits_{i=1}^{n}\\alpha_i,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY})=E(e^{t(X_1+X_2+\\cdots+X_n)} \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-\\alpha_1}(1-\\beta t)^{-\\alpha_2}\\cdots (1-\\beta t)^{-\\alpha_n}=(1-\\beta t)^{-\\sum\\limits_{i=1}^{n}\\alpha_i} \\end{align*}\\] Which is the moment generating function of a Gamma random variable with parameters \\(\\sum\\limits_{i=1}^{n}\\alpha_i\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((\\sum\\limits_{i=1}^{n}\\alpha_i,\\beta)\\). 15.6.3 Sum of Exponential Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from an Exponential distribution with parameter \\(\\beta\\), i.e. \\(X_i\\sim\\)Exponential\\((\\beta)\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Gamma\\((n,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-1}(1-\\beta t)^{-1}\\cdots(1-\\beta t)^{-1} \\\\ &amp;= (1-\\beta t)^{-n} \\end{align*}\\] Which is the moment generating function for a Gamma random variable with parameters \\(n\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((n,\\beta)\\). "],
["gamma-function.html", "16 Gamma Function 16.1 Definition 16.2 Theorems for the Gamma Function 16.3 References", " 16 Gamma Function The Gamma Function is a function used frequently in statistical theory. It has properties that permit simplified calculations and is used in defining many probability distributions, particularly within the Exponential family of distributions. 16.1 Definition \\[\\Gamma(x) = \\int\\limits_0^{\\infty}t^{x-1}e^{-t}dt\\] Note that the definition defines a function of \\(x\\) that is integrated over \\(t\\). Thus, for each value of \\(x\\), a curve is defined, and the Gamma function calculates the area under the curve defined by \\(x\\). 16.2 Theorems for the Gamma Function 16.2.1 Lemma \\[\\left[ -t^{x-1} e^{-t} \\right]_{t = 0}^{t = \\infty} = 0\\] Proof: \\[\\begin{aligned} \\left[ -t^{x-1} e^{-t} \\right]_{t = 0}^{t = \\infty} &amp;= \\lim_{t\\to\\infty}\\big(-t^{x - 1} e^{-t}\\big) - 0^{x - 1} e^{-0} \\\\ &amp;= \\lim_{t\\to\\infty}\\big(-t^{x - 1} e^{-t}\\big) - 0 \\\\ &amp;= \\lim_{t\\to\\infty}\\big(-t^{x - 1} e^{-t}\\big) \\\\ &amp;= - \\lim_{t\\to\\infty}\\frac{t^{x - 1}}{e^t} \\\\ &amp;= - \\lim_{t\\to\\infty} \\left\\{exp\\left[ (x - 1) \\ln t - t\\right] \\right\\} \\\\ &amp;= - \\lim_{t\\to\\infty} \\left\\{ exp \\left[(x - 1) \\cdot t \\cdot \\Big( \\frac{\\ln t}{t} - 1\\Big) \\right]\\right\\} \\\\ ^{[1]} &amp;= \\lim_{t\\to\\infty} \\left\\{ exp \\left[(x - 1) \\cdot t \\cdot \\Big( 0 - 1\\Big) \\right]\\right\\} \\\\ &amp;= \\lim_{t\\to\\infty} \\left\\{ exp \\left[- (x - 1) \\cdot t \\right]\\right\\} \\\\ &amp;= \\lim_{t\\to\\infty} \\frac{1}{e^{(x - 1) \\cdot t}} \\\\ &amp;= 0 \\end{aligned}\\] L’H^{o}pital’s Rule: \\(\\lim_{x\\to u}\\frac{f(x)}{g(x)} = \\lim_{x\\to u} \\frac{f&#39;(x)}{g&#39;(x)}\\). This implies \\(\\lim_{x\\to \\infty} \\frac{\\ln x}{x} = \\lim_{x\\to \\infty} \\frac{\\frac{1}{x}}{1} = \\lim_{x \\to \\infty} \\frac{1}{x} = 0\\) 16.2.2 Theorem: The Reduction Relation \\(\\Gamma(x) = (x-1) \\cdot \\Gamma(x)\\) Proof: The proof relies on integration by parts. Let: \\[\\begin{aligned} u &amp;= t^{x-1} \\\\ du &amp;= (x - 1) \\cdot t^{(x-2)} \\\\\\\\ v &amp;= -e^{-t} \\\\ dv &amp;= e^{-t} dt \\end{aligned}\\] Integration by parts yields: \\[\\begin{aligned} \\Gamma(x) &amp;= \\int\\limits_0^{\\infty}t^{x-1}e^{-t}dt \\\\ &amp;= u \\cdot v - \\int\\limits_0^\\infty v \\cdot du \\\\ &amp;= \\Big[t^{x-1} \\cdot - e^{-t}\\Big]_{t=0}^{t=\\infty} - \\int\\limits_0^\\infty-e^{-t} \\cdot (x-1)\\cdot t^{(x-2)} dt \\\\ &amp;= -\\Big[t^{x-1} \\cdot e^{-t}\\Big]_{t=0}^{t=\\infty} - (- (x - 1)) \\int\\limits_0^\\infty e^{-t} \\cdot t^{(x-2)} dt \\\\ ^{[1]} &amp;= -0 + (x - 1) \\int\\limits_0^\\infty e^{-t} \\cdot t^{(x-2)} dt \\\\ &amp;= (x - 1) \\int\\limits_0^\\infty e^{-t} \\cdot t^{((x-1)-1)} dt \\\\ &amp;= (x - 1) \\int\\limits_0^\\infty t^{((x-1)-1)} \\cdot e^{-t} dt \\\\ &amp;= (x - 1) \\cdot \\Gamma(x - 1) \\end{aligned}\\] 16.2.1 16.2.3 Corollary \\[\\Gamma(x) = \\frac{1}{x} \\cdot \\Gamma(x + 1)\\] Proof: Theorem 16.2.2 establishes \\[ \\Gamma(x) = (x - 1) \\cdot \\Gamma(x - 1)\\] Let \\(y = x + 1\\). Then \\[\\begin{aligned} \\Gamma(y) &amp;= (y - 1) \\cdot \\Gamma(y - 1) \\\\ \\Rightarrow \\Gamma(x + 1) &amp;= (x + 1 - 1) \\cdot \\Gamma(x + 1 - 1) \\\\ &amp;= x \\cdot \\Gamma(x) \\\\ \\Rightarrow \\frac{1}{x} \\cdot \\Gamma(x + 1) &amp;= \\Gamma(x) \\\\ \\Rightarrow \\Gamma(x) &amp;= \\frac{1}{x} \\cdot \\Gamma(x + 1) \\end{aligned}\\] This allows the recurrence relation to move toward \\(\\Gamma(0)\\) for any value of \\(x\\). Note, however, that \\(\\Gamma(0)\\) is undefined. Thus, solutions for the Gamma Function may be determined for positive integers, since \\(\\Gamma(1)\\) can be solved. On the other hand, \\(\\Gamma(-1)\\) can not be solved, and the recurrence relation results in a zero denominator. Hence, the Gamma Function is defined for all \\(x \\in \\mathbb{R}\\) so long as \\(x \\not\\in \\mathbb{Z^-}\\) 16.2.4 Theorem: \\[\\Gamma\\Big(\\frac{1}{2}\\Big) = \\sqrt{\\pi}\\] Proof: \\[\\begin{aligned} \\Gamma\\Big(\\frac{1}{2}\\Big) &amp;= \\int\\limits_0^\\infty t^{-\\frac{1}{2}} e ^{-t} dt \\\\ &amp;= \\frac{2}{2} \\int\\limits_0^\\infty t^{-\\frac{1}{2}} e ^{-t} dt \\\\ &amp;= 2 \\int\\limits_0^\\infty \\frac{1}{2} \\cdot t ^{-\\frac{1}{2}} e^{-(\\sqrt{t})^2} dt \\\\ &amp;= 2 \\int\\limits_0^\\infty \\frac{1}{2} \\cdot t ^{-\\frac{1}{2}} e^{-(\\sqrt{t})^2} dt \\\\ ^{[1]} &amp;= 2 \\int\\limits_0^\\infty \\frac{1}{2} \\cdot x ^{-1} e^{-x^2} \\cdot 2\\cdot x \\ dx \\\\ &amp;= 2 \\int\\limits_0^\\infty \\frac{2x}{2x} e^{-x^2} dx \\\\ &amp;= 2 \\int\\limits_0^\\infty e^{-x^2} dx \\\\ ^{[2]} &amp;= 2 \\cdot \\frac{\\sqrt{\\pi}}{2} \\\\ &amp;= \\sqrt{\\pi} \\end{aligned}\\] \\(x = \\sqrt{t}\\); \\(t = x^2\\); and \\(dt = 2x dx\\) Theorem 17.1.2 16.3 References Pennsylvania State University, Elberly College of Science, STAT 414/415, https://onlinecourses.science.psu.edu/stat414/node/142 Theodore Hatch Whitfield, Lecture Notes, E156 Mathematical Foundations of Statistical Software. "],
["gaussian-integral.html", "17 Gaussian Integral 17.1 Theorems for the Gaussian Integral 17.2 References", " 17 Gaussian Integral The Gaussian Integral is defined by \\[ \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx \\] The Gaussian Integral may be generalized to the form \\[\\int\\limits_{-\\infty}^\\infty e^{-a(x + b)^2} \\cdot dx \\] 17.1 Theorems for the Gaussian Integral 17.1.1 Theorem The Gaussian Integral is an even function. Proof: Recall that an even function is a function \\(f(x)\\) such that \\(f(-x) = f(x)\\). Let \\(f(x)\\) be the Gaussian Integral \\[f(x) = \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx \\] \\[\\begin{aligned} f(-x) &amp;= \\int\\limits_{-\\infty}^\\infty e^{-(-x)^2} \\cdot dx \\\\ &amp;= \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx \\\\ &amp;= f(x) \\end{aligned}\\] 17.1.2 Theorem \\[ \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx = \\sqrt{\\pi}\\] Proof: Let \\(y = x\\), and let \\(I = \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx\\). This permits the equation \\[I = \\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx = \\int\\limits_{-\\infty}^\\infty e^{-y^2} \\cdot dy \\] We use this equality to define the double integral for \\(I^2\\). \\[\\begin{aligned} I^2 &amp;= I \\cdot I \\\\ &amp;= \\Big(\\int\\limits_{-\\infty}^\\infty e^{-x^2} \\cdot dx\\Big) \\cdot \\Big(\\int\\limits_{-\\infty}^\\infty e^{-y^2} \\cdot dx\\Big) \\\\ &amp;= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty e^{-x^2} e^{-y^2} \\cdot dx\\ dy\\\\ &amp;= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty e^{-(x^2 + y^2)} \\cdot dx\\ dy\\\\ ^{[1]} &amp;= \\int\\limits_0^{2\\pi} \\int\\limits_{0}^\\infty e^{-r^2} \\cdot r \\cdot dr\\ d\\theta \\\\ &amp;= \\int\\limits_0^{2\\pi} -\\frac{1}{2} e ^{-r^2} \\Big\\rvert_{r = 0}^{r = \\infty} \\cdot d\\theta \\\\ &amp;= \\int\\limits_0^{2\\pi} - 0 - \\big(-\\frac{1}{2}\\big) \\cdot d\\theta\\\\ &amp;= \\int\\limits_0^{2\\pi} \\frac{1}{2} \\cdot d\\theta\\\\ &amp;= \\frac{\\theta}{2} \\Big\\rvert_{\\theta = 0}^{\\theta = 2\\pi} \\\\ &amp;= \\frac{2\\pi}{2} - \\frac{0}{2} \\\\ &amp;= \\frac{2\\pi}{2} \\\\ &amp;= \\pi \\end{aligned}\\] Conversion to polar coordinates. Let the radius be \\(r = x^2 + y^2\\) on the domain of \\([0, \\infty]\\) and let the angle be \\(\\theta\\) on the domain of \\([0, 2\\pi]\\). \\(dx\\ dy = r dr\\ d\\theta\\). This establishes that \\(I^2 = \\pi\\). It follows: \\[\\begin{aligned} I^2 &amp;= \\pi \\\\ I &amp;= \\sqrt{\\pi} \\\\ \\int\\limits_{-\\infty}^\\infty e^{-x^2} &amp;= \\sqrt{\\pi} \\end{aligned}\\] 17.1.3 Theorem \\[\\int\\limits_{-\\infty}^\\infty e^{-a(x + b)^2} \\cdot dx = \\sqrt{\\frac{\\pi}{a}}\\] Proof: Let \\(y = x\\), and let \\(I = \\int\\limits_{-\\infty}^\\infty e^{-a(x+b)^2} \\cdot dx\\). This permits the equation \\[I = \\int\\limits_{-\\infty}^\\infty e^{-a(x+b)^2} \\cdot dx = \\int\\limits_{-\\infty}^\\infty e^{-a(y+b)^2} \\cdot dy \\] We use this equality to define the double integral for \\(I^2\\). \\[\\begin{aligned} I^2 &amp;= I \\cdot I \\\\ &amp;= \\Big(\\int\\limits_{-\\infty}^\\infty e^{-a(x+b)^2} \\cdot dx\\Big) \\cdot \\Big(\\int\\limits_{-\\infty}^\\infty e^{-a(y+b)^2} \\cdot dx\\Big) \\\\ &amp;= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty e^{-a(x+b)^2} e^{-a(y+b)^2} \\cdot dx\\ dy\\\\ &amp;= \\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty e^{-(a(x + b)^2 + a(y + b)^2)} \\cdot dx\\ dy\\\\ ^{[1]} &amp;= \\int\\limits_0^{2\\pi} \\int\\limits_{0}^\\infty e^{-2a(r+b)^2} \\cdot r \\cdot dr\\ d\\theta \\\\ &amp;= \\int\\limits_0^{2\\pi} -\\frac{1}{2a} e ^{-r^2} \\Big\\rvert_{r = 0}^{r = \\infty} \\cdot d\\theta \\\\ &amp;= \\int\\limits_0^{2\\pi} - 0 - \\big(-\\frac{1}{2a}\\big) \\cdot d\\theta\\\\ &amp;= \\int\\limits_0^{2\\pi} \\frac{1}{2a} \\cdot d\\theta\\\\ &amp;= \\frac{\\theta}{2a} \\Big\\rvert_{\\theta = 0}^{\\theta = 2\\pi} \\\\ &amp;= \\frac{2\\pi}{2a} - \\frac{0}{2} \\\\ &amp;= \\frac{2\\pi}{2a} \\\\ &amp;= \\frac{\\pi}{a} \\end{aligned}\\] Conversion to polar coordinates. Let the radius be \\(r = a(x+b)^2 + a(y+b)^2\\) on the domain of \\([0, \\infty]\\) and let the angle be \\(\\theta\\) on the domain of \\([0, 2\\pi]\\). \\(dx\\ dy = r dr\\ d\\theta\\). This establishes that \\(I^2 = \\frac{\\pi}{a}\\). It follows: \\[\\begin{aligned} I^2 &amp;= \\frac{\\pi}{a} \\\\ I &amp;= \\sqrt{\\frac{\\pi}{a}} \\\\ \\int\\limits_{-\\infty}^\\infty e^{-a(x+b)^2} &amp;= \\sqrt{\\frac{\\pi}{a}} \\end{aligned}\\] 17.2 References Theodore Hatch Whitfield, Lecture Notes, E156 Mathematical Foundations of Statistical Software. "],
["geometric-distribution.html", "18 Geometric Distribution 18.1 First Success as a Random Variable 18.2 Number of Failures as Random Variable", " 18 Geometric Distribution 18.1 First Success as a Random Variable The Geometric Distribution random variable may be considered as the number of trials required to achieve the first success. In this consideration, if \\(x\\) is the total number of trials and \\(k\\) is the number of failures, \\(x = k+1\\). 18.1.1 Probability Mass Function A random variable \\(X\\) is said to have a Geometric Distribution with parameter \\(p\\) if its probability mass function is \\[p(x) = \\left\\{ \\begin{array}{ll} p(1-p)^{x-1}, &amp; x=1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] where \\(p\\) is the probability of a success on any given trial and \\(x\\) is the number of trials until the first success. 18.1.2 Cumulative Distribution Function The cumulative probability of \\(x\\) is computed as the \\(x^{th}\\) partial sum of the Geometric Series See 19.1.1. \\[\\begin{align*} P(x) &amp;= \\sum\\limits_{i=1}^{x}p(1-p)^{x-1} \\\\ &amp;= \\frac{p-p(1-p)^{x-1}}{1-(1-p)} \\\\ &amp;= \\frac{p[1-(1-p)^{x-1}]}{p} \\\\ &amp;= 1-(1-p)^{x-1} \\end{align*}\\] So \\[P(x) = \\left\\{ \\begin{array}{ll} 1-(1-p)^{x-1},&amp; x=1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} P(X=x+1) &amp;= p(1-p)^x \\\\ &amp;= (1-p)p(1-p)^{x-1} \\\\ &amp;= (1-p)P(X=x) \\end{align*}\\] Figure 18.1: The figures on the left and right display the Geometric probability and cumulative distirubtion functions, respectively, for the combinations of \\(p=.3\\). 18.1.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=1}^{\\infty}x\\cdot p(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}x\\cdot (1-p)^{x-1} \\\\ ^{[1]} &amp;= p\\sum\\limits_{x=1}^{\\infty}x\\cdot q^{x-1} \\\\ ^{[2]} &amp;= p\\frac{d}{dq}\\Big(\\sum\\limits_{x=1}^{\\infty}q^x\\Big) \\\\ &amp;= p\\frac{d}{dq}\\Big(\\sum\\limits_{x=1}^{\\infty}q\\cdot q^{x-1}\\Big) \\\\ ^{[3]} &amp;= p\\frac{d}{dq}\\Big(\\frac{q}{1-q}\\Big) \\\\ ^{[4]} &amp;= p\\Big(\\frac{(1-q)\\cdot 1-q(-1)}{(1-q)^2}\\Big) \\\\ &amp;= p\\Big(\\frac{1-q+q}{(1-q)^2}\\Big) \\\\ &amp;= p\\Big(\\frac{1}{(1-q)^2}\\Big) \\\\ ^{[5]} &amp;= p\\cdot\\frac{1}{p^2} &amp;= \\frac{p}{p^2} &amp;= \\frac{1}{p} \\end{align*}\\] Let \\(1-p = 1\\) \\(a\\cdot x^{a-1}=\\frac{d}{dx}(x^a)\\) \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=\\frac{a}{1-r}\\) (Geometric Series 19.1.1). Product Rule for Differentiation: \\(\\frac{d}{dx}(\\frac{f(x)}{g(x)}) = \\frac{g\\prime(x)\\cdot f(x)-f\\prime(x)\\cdot g(x)}{g^2(x)}\\) \\(1-p=q \\ \\ \\Rightarrow p=1-q\\) \\[\\begin{align*} E[X(X-1)] ^{[1]} &amp;= \\sum\\limits_{x=2}^{\\infty}x(x-1)p(1-p)^{x-1} \\\\ &amp;= p(1-p)\\sum\\limits_{x=2}^{\\infty}x(x-1)(1-p)^{x-2} \\\\ ^{[2]} &amp;= pq\\sum\\limits_{x=2}^{\\infty}x(x-1)q^{x-2} \\\\ ^{[3]} &amp;= pq\\frac{d^2}{dq^2}\\Big(\\sum\\limits_{x=2}^{\\infty}q^x\\Big) \\\\ &amp;= pq\\frac{d^2}{dq^2}\\Big(\\sum\\limits_{x=2}^{\\infty}q\\cdot q^{x-1}\\Big) \\\\ ^{[4]} &amp;= pq\\frac{d^2}{dq^2}\\Big(\\frac{2q-1}{1-q}\\Big) \\\\ ^{[5]} &amp;= pq\\frac{d}{dq}(-(1-q)^{-2}) \\\\ ^{[6]} &amp;= pq\\frac{2}{(1-q)^3} \\\\ &amp;= \\frac{2pq}{(1-q)^3} \\\\ ^{[7]} &amp;= \\frac{2p(1-p)}{p^3} \\\\ &amp; =\\frac{2(1-p)}{p^2} \\end{align*}\\] We start the summand at \\(x=2\\) because the term for \\(x=1\\) is 0. Let \\(1-p = 1\\) \\(a(a-1)x^{a-2}=\\frac{d^2}{dx^2}x^a\\) \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1} = \\frac{a}{1-r}=1+a+ar+ar^2+\\cdots.\\) The current series leaves out the first term,\\   \\(\\sum\\limits_{k=2}^{\\infty}ar^{k-1}=(\\sum\\limits_{k=1}^{\\infty}ar^{k-1})-1 = \\frac{a}{1-r}-1=\\frac{1}{1-r}-\\frac{1-r}{1-r}=\\frac{a-(1-r)}{1-r}=\\frac{a+r-1}{r-1}\\) \\(\\frac{d}{dx}(\\frac{2x-1}{1-x})=\\frac{-(2x-1)-2(1-x)}{(1-x)^2} = \\frac{2x+1-2+2x}{(1-x)^2}=\\frac{-1}{(1-x)^2}=-(1-x)^{-2}\\) \\(\\frac{d}{dx}-(1-x)^{-2}=2(1-x)^{-3}=\\frac{2}{(1-x)^3}\\) See note number 5. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\frac{1}{p} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= [E(X^2)-E(X)]+E(X)-E(X)^2 \\\\ &amp;= E(X^2-X)+E(X)-E(X)^2 \\\\ &amp;= E[(X(X-1)]+E(X)-E(X)^2 \\\\ &amp;= \\frac{2(1-p)}{p^2} + \\frac{1}{p} - \\frac{1}{p^2} \\\\ &amp;= \\frac{2(1-p)}{p^2} + \\frac{p}{p^2} - \\frac{1}{p^2} \\\\ &amp;= \\frac{2(1-p)+p-1}{p^2} \\\\ &amp;= \\frac{2-2p+p-1}{p^2} \\\\ &amp;= \\frac{2-1+p-2p}{p^2} \\\\ &amp;= \\frac{1-p}{p^2} \\end{align*}\\] 18.1.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{x=1}^{\\infty}e^{tx}p(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}e^{tx}(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}e^{t^x}(1-p)^{x-1} \\\\ &amp;= pe^t\\sum\\limits_{x=1}^{\\infty}e^{t^{(x-1)}} \\\\ &amp;= pe^t\\sum\\limits_{x=1}^{\\infty}[(1-p)e^t]^{x-1} \\\\ ^{[1]} &amp;= pe^t\\frac{1}{1-(1-p)e^t} \\\\ &amp;= \\frac{pe^t}{1-(1-p)e^t} \\end{align*}\\] \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=\\frac{a}{1-r}\\), (Geometric Series 19.1.1) \\[\\begin{align*} M_X^{(1)}(t) &amp;= \\frac{[1-(1-p)e^t]pe^t-pe^t[-(1-p)e^t]}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t[1-(1-p)e^t+(1-pe^t)]}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t(1)}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t}{\\big(1-(1-p)e^t\\big)^2} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= \\frac{(1-(1-p)e^t)^2pe^t-pe^t[-2(1-(1-p)e^t)(1-p)e^t}{\\big(1-(1-p)e^t\\big)^4} \\\\ &amp;= \\frac{pe^t[(1-(1-p)e^t)^2+2(1-(1-p)e^t(1-p)e^t]}{\\big(1-(1-p)e^t\\big)^4} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\frac{pe^0}{\\big(1-(1-p)e^0\\big)^2} \\\\ &amp;= \\frac{p}{\\big(1-(1-p)\\big)^2} \\\\ &amp;= \\frac{p}{(1-1+p)^2} \\\\ &amp;= \\frac{p}{p^2} \\\\ &amp;= \\frac{1}{p}\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= \\frac{pe^0[(1-(1-p)e^0)^2+2(1-(1-p)e^0)(1-p)e^0]}{(1-(1-p)e^0)^4} \\\\ &amp;= \\frac{p[(1-(1-p))^2+2(1-(1-p))(1-p)]}{(1-(1-p))^4} \\\\ &amp;= \\frac{p[(1-1+p)^2+2(1-1+p)(1-p)]}{(1-1+p)^4} \\\\ &amp;= \\frac{p[p^2=2p(1-p)]}{p^4} \\\\ &amp;= \\frac{p(p^2+2p-2p^2)}{p^4} \\\\ &amp;= \\frac{p(2p-p^2)}{p^4} \\\\ &amp;= \\frac{p^2(2-p)}{p^4} \\\\ &amp;= \\frac{2-p}{p^2} \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\frac{1}{p}\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\frac{2-p}{p^2} - \\frac{1}{p^2} \\\\ &amp;= \\frac{1-p}{p^2} \\end{align*}\\] 18.1.5 Maximum Likelihood Estimator Let \\(x_1, x_2 , \\ldots , x_n\\) be a random sample from a Geometric distribution with parameter \\(p\\). 18.1.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= P(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= p(1-p)^{x-1} \\end{align*}\\] 18.1.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln p(1-p)^{x-1} \\\\ &amp;= \\ln p+\\ln(1-p)^{x-1} \\\\ &amp;= \\ln p+(x-1)\\ln(1-p) \\end{align*}\\] 18.1.5.3 MLE for \\(p\\) \\[\\begin{align*} \\frac{d\\ell}{dp} &amp;= \\frac{1}{p} + \\frac{-(x-1)}{1-p} \\\\ &amp;= \\frac{1}{p} + \\frac{1-x}{1-p}\\\\ \\\\ \\\\ 0 &amp;= \\frac{1}{p}+\\frac{1-x}{1-p}\\\\ \\Rightarrow \\frac{1-x}{1-p} &amp;= -\\frac{1}{p}\\\\ \\Rightarrow 1-x &amp;= \\frac{-1+p}{p}\\\\ \\Rightarrow -x &amp;= \\frac{-1+p}{p}-1\\\\ \\Rightarrow x=1-\\frac{-1+p}{p} &amp;= \\frac{p}{p}-\\frac{-1+p}{p} \\\\ &amp;= \\frac{p+1-p}{p} \\\\ &amp;= \\frac{1}{p}\\\\ \\Rightarrow p &amp;= \\frac{1}{x} \\end{align*}\\] So \\[\\hat p=\\frac{1}{x}\\] is the Maximum Likelihood Estimator for \\(p\\). 18.1.6 Theorems for the Geometric Distribution 18.1.6.1 Validity of the Distribution \\[\\sum\\limits_{i=1}^{\\infty}p(1-p)^{x-1} = 1\\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{\\infty}p(1-p)^{x-1} &amp;= \\frac{p}{1-(1-p)} \\\\ &amp;= \\frac{p}{p} \\\\ &amp;= 1 \\end{align*}\\] \\(S=\\lim\\limits_{k\\rightarrow\\infty}S_k =\\lim\\limits_{k\\rightarrow\\infty}\\frac{a-ar^k}{1-r}\\) (Geometric Series 19.1.1) 18.1.6.2 Sum of Geometric Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from a Geometric Distribution with parameter \\(p\\), that is, \\(X_i\\sim\\)Geometric\\((p)\\), \\(i=1,2,3,\\ldots\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Negative Binomial\\((n,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n)}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;=\\frac{pe^t}{1-(1-p)e^t}\\cdot\\frac{pe^t}{1-(1-p)e^t}\\cdot\\ \\cdots \\ \\cdot\\frac{pe^t}{1-(1-p)e^t} \\\\ &amp;= \\Big(\\frac{pe^t}{1-(1-p)e^t}\\Big)^n \\end{align*}\\] Which is the mgf of a Negative Binomial random variable with parameters \\(n\\) and \\(p\\). Thus \\(Y\\sim\\)Negative Binomial\\((n,p)\\). 18.1.6.3 Lemma Let \\(X\\) be a Geometric random variable with parameter \\(p\\). Then \\(P(X&gt;a)=(1-p)^a\\). Proof: \\[\\begin{align*} P(X&gt;a) &amp;= 1-P(X\\leq a)=1-\\sum\\limits_{i=1}^{a}p(1-p)^i-1 \\\\ ^{[1]} &amp;= 1-\\frac{p-p(1-p)^a}{1-(1-p)} \\\\ &amp;= 1-\\frac{p\\big(1-(1-p)^a\\big)}{1-1+p} \\\\ &amp;= 1-\\frac{p\\big(1-(1-p)^a\\big)}{p} \\\\ &amp;= 1-\\big(1-(1-p)^a\\big) \\\\ &amp;= 1-1+(1-p)^a \\\\ &amp;= (1-p)^a \\end{align*}\\] \\(S_k=\\lim\\limits_{k\\rightarrow\\infty}\\frac{a-ar^k}{1-r}\\) (Geometric Series 19.1.1) 18.1.6.4 Memoryless Property \\[P(X\\geq k+j|X\\geq k)=P(X\\geq k)\\] Proof: \\[\\begin{align*} P(X &gt; a+b) ^{[1]} &amp;= (1-p)^{a+b}=(1-p)^a(1-p)^b \\\\ \\\\ \\\\ P(X &gt; k+j|X &gt; k) &amp;=\\frac{P(X &gt; k+j\\cap X &gt; k)}{P(X &gt; k)} \\\\ ^{[2]} &amp;= \\frac{P(X &gt; k+j)}{P(X &gt; k)} \\\\ &amp;= \\frac{(1-p)^k(1-p)^j}{(1-p)^k} \\\\ &amp;= (1-p)^j \\\\ &amp;= P(X &gt; j) \\end{align*}\\] Geometric Distribution 18.1.6.3 Since \\(j\\) must be a positive integer in the Geometric Distribution, it is certain that \\\\((k+j)\\cap k=k+j\\). 18.2 Number of Failures as Random Variable The Geometric Distribution random variable may be considered the number of failures observed before a success is observed. In this consideration, if \\(x\\) is the number of failures, then the total number of trials is \\(x + 1\\). 18.2.1 Probability Mass Function A random variable \\(X\\) is said to have a Geometric Distribution with parameter \\(p\\) if its probability mass function is \\[p(x) = \\left\\{ \\begin{array}{ll} p(1-p)^{x}, &amp; x=0,1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] 18.2.2 Cumulative Distribution Function The cumulative probability of \\(x\\) is computed as the \\(x^{th}\\) partial sum of the Geometric Series See 19.1.1. \\[\\begin{align*} P(x) &amp;= \\sum\\limits_{i=0}^{x}p(1-p)^{x} \\\\ &amp;= \\frac{p-p(1-p)^{x+1}}{1-(1-p)} \\\\ &amp;= \\frac{p[1-(1-p)^{x+1}]}{p} \\\\ &amp;= 1-(1-p)^{x+1} \\end{align*}\\] So \\[P(x) = \\left\\{ \\begin{array}{ll} 1-(1-p)^{x+1},&amp; x=0,1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] "],
["geometric-series.html", "19 Geometric Series 19.1 Series Starts at \\(k = 1\\) 19.2 Series Starts at \\(k = 0\\)", " 19 Geometric Series 19.1 Series Starts at \\(k = 1\\) A Geometric Series is a series of the form \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) where \\(a\\neq 0,\\ r\\neq 0,1\\). Expanding the series gives \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=a+ar+ar^2+ar^3+\\cdots\\). 19.1.1 Partial and Infinite Summations Let \\(S_k\\) denote the sum of a series over \\(k\\) terms (or the \\(k^{th}\\) partial sum). For the Geometric Series\\ \\[\\begin{align*} S_k &amp;= \\sum\\limits_{k=1}^{k}ar^{k-1} \\\\ &amp;= a + ar + ar^2 + ar^3 + \\cdots + ar^{k-1} \\\\ &amp;= ar^0 + ar^2 + ar^2 + \\cdots + ar^{k-1} \\\\ &amp;= a + ar + ar^2 + \\cdots + ar^{k-1} \\end{align*}\\] Notice that \\(rS_k = ar + ar^2+ a r^3 + \\cdots + ar^k\\). So \\[\\begin{align*} S_k - rS_k &amp;= (a + ar + \\cdots + ar^{k-1}) - (ar + ar^2 + \\cdots + ar^k) \\\\ &amp;= a + ar - ar + ar^2 - ar^2 + \\cdots + ar^{k-2} - ar^{k-2} + ar^{k-1} - ar^{k-1} - ar^k \\\\ &amp;= a - ar^k \\end{align*}\\] Observing that \\(S_k - rS_k = S_k(1 - r)\\), we may conclude \\[\\begin{align*} S_k(1-r) &amp;= S_k - rS_k \\\\ &amp;= a - ar^k \\\\ \\Rightarrow S_k &amp;= \\frac{a-ar^k}{1-r} \\end{align*}\\] 19.1.2 Proofs of Convergence \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) converges when \\(|r|&lt;1\\) and diverges when \\(|r|&gt;1\\). Proof: Recall that the \\(k^{th}\\) partial sum of the Geometric Series is \\[ S_k = \\frac{a-ar^k}{1-r} \\] And let \\(S\\) denote the sum of the infinite series, i.e. the sum as\\(k\\rightarrow\\infty\\). Case 1: \\(|r|&lt;1\\) \\[\\begin{align*} S &amp;= \\lim\\limits_{k\\rightarrow\\infty}S_k \\\\ &amp;= \\lim\\limits_{k\\rightarrow\\infty} \\frac{a-ar^k}{1-r} \\\\ &amp;= \\frac{a-\\lim\\limits_{k\\rightarrow\\infty}ar^k}{1-r} \\\\ &amp;= \\frac{a}{1-r} \\end{align*}\\] So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) converges when \\(|r|&lt;1\\) and \\(S=\\frac{a}{1-r}\\) Case 2: \\(|r|&gt;1\\) \\[\\begin{align*} S &amp;= \\lim\\limits_{k\\rightarrow\\infty}S_k \\\\ &amp;= \\lim\\limits_{k\\rightarrow\\infty} \\frac{a-ar^k}{1-r} \\\\ &amp;= \\frac{a-\\lim\\limits_{k\\rightarrow\\infty}ar^k}{1-r} \\\\ &amp;= \\frac{a-\\infty}{1-r} \\end{align*}\\] So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) diverges whern \\(|r|&gt;1\\). 19.2 Series Starts at \\(k = 0\\) A Geometric Series is a series of the form \\(\\sum\\limits_{k=0}^{\\infty}ar^{k}\\) where \\(a\\neq 0,\\ r\\neq 0,1\\). Expanding the series gives \\(\\sum\\limits_{k=0}^{\\infty}ar^{k}=a+ar+ar^2+ar^3+\\cdots\\). 19.2.1 Partial and Infinite Summations Let \\(S_k\\) denote the sum of a series over \\(k\\) terms (or the \\(k^{th}\\) partial sum). For the Geometric Series\\ \\[\\begin{align*} S_k &amp;= \\sum\\limits_{k=0}^{n}ar^{k} \\\\ &amp;= a + ar + ar^2 + ar^3 + \\cdots + ar^{k} \\\\ &amp;= ar^0 + ar^2 + ar^2 + \\cdots + ar^{k} \\\\ &amp;= a + ar + ar^2 + \\cdots + ar^{k} \\end{align*}\\] Notice that \\(rS_k = ar + ar^2+ a r^3 + \\cdots + ar^k\\). So \\[\\begin{align*} S_k - rS_k &amp;= (a + ar + \\cdots + ar^{k}) - (ar + ar^2 + \\cdots + ar^{k+1}) \\\\ &amp;= a + ar - ar + ar^2 - ar^2 + \\cdots + ar^{k-1} - ar^{k-1} + ar^k + ar^k - ar^{k + 1}\\\\ &amp;= a - ar^{k+1} \\end{align*}\\] Observing that \\(S_k - rS_k = S_k(1 - r)\\), we may conclude \\[\\begin{align*} S_k(1-r) &amp;= S_k - rS_k \\\\ &amp;= a - ar^{k+1} \\\\ \\Rightarrow S_k &amp;= \\frac{a-ar^{k+1}}{1-r} \\end{align*}\\] 19.2.2 Proofs of Convergence \\(\\sum\\limits_{k=0}^{\\infty}ar^{k}\\) converges when \\(|r|&lt;1\\) and diverges when \\(|r|&gt;1\\). Proof: Recall that the \\(k^{th}\\) partial sum of the Geometric Series is \\[ S_k = \\frac{a-ar^{k+1}}{1-r} \\] And let \\(S\\) denote the sum of the infinite series, i.e. the sum as\\(k\\rightarrow\\infty\\). Case 1: \\(|r|&lt;1\\) \\[\\begin{align*} S &amp;= \\lim\\limits_{k\\rightarrow\\infty}S_k \\\\ &amp;= \\lim\\limits_{k\\rightarrow\\infty} \\frac{a-ar^{k+1}}{1-r} \\\\ &amp;= \\frac{a-\\lim\\limits_{k\\rightarrow\\infty}ar^{k+1}}{1-r} \\\\ &amp;= \\frac{a}{1-r} \\end{align*}\\] So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k}\\) converges when \\(|r|&lt;1\\) and \\(S=\\frac{a}{1-r}\\) Case 2: \\(|r|&gt;1\\) \\[\\begin{align*} S &amp;= \\lim\\limits_{k\\rightarrow\\infty}S_k \\\\ &amp;= \\lim\\limits_{k\\rightarrow\\infty} \\frac{a-ar^{k+1}}{1-r} \\\\ &amp;= \\frac{a-\\lim\\limits_{k\\rightarrow\\infty}ar^{k+1}}{1-r} \\\\ &amp;= \\frac{a-\\infty}{1-r} \\end{align*}\\] So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) diverges whern \\(|r|&gt;1\\). "],
["hypergeometric-distribution.html", "20 Hypergeometric Distribution 20.1 Probability Mass Function 20.2 Cumulative Mass Function 20.3 Expected Values 20.4 Moment Generating Function 20.5 Theorems for the Hypergeometric Distribution", " 20 Hypergeometric Distribution 20.1 Probability Mass Function A random variable \\(X\\) is said to follow a Hypergeometric Distribution if its probability mass function is \\[p(x) = \\left\\{ \\begin{array}{ll} \\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}}, &amp; x=0,1,2,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] where \\(N\\) is the number of objects available to choose from \\(n\\) is the number of objects chosen from \\(N\\) \\(r\\) is the number of objects in \\(N\\) that posses a desired characteristic (successes) \\(x\\) is the number of objects in \\(n\\) that possess the desired characterstic 20.2 Cumulative Mass Function \\[P(x) = \\left\\{ \\begin{array}{ll} \\sum\\limits_{i=0}^{x}\\frac{{r\\choose i}{N-r\\choose n-i}}{{N\\choose n}}, &amp; x=0,1,2,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] 20.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^{n}x\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ &amp;= \\sum\\limits_{x=0}^{n}x{r\\choose x}\\frac{{N-r\\choose n-x}}{{N\\choose n}} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n}x\\frac{r}{x}{r-1\\choose x-1}\\frac{{N-r\\choose n-x}} {\\frac{N}{n}{N-1\\choose n-1}} \\\\ &amp;= \\frac{rn}{N}\\sum\\limits_{x=0}^{n} \\frac{{r-1\\choose x-1}{N-r\\choose n-x}}{{N-1\\choose n-1}} \\\\ ^{[2]} &amp;= \\frac{rn}{N}\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\frac{rn}{N}\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\\\ &amp;= \\frac{\\frac{rn}{N}\\cdot n}{N}r\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\\\ ^{[3]} &amp;= \\frac{rn}{N}\\cdot 1 \\\\ &amp;= \\frac{rn}{N} \\end{align*}\\] For any integer \\(a\\) such that \\(0\\leq a\\leq k,\\ {n\\choose k}=\\frac{n(n-1)\\cdots(n-a+1)}{k(k-1)\\cdots(k-a+1)}{n-a\\choose k-a}\\) (Theorem 9.0.3). Let \\(y=x-1\\)  \\(\\Rightarrow x=y+1\\). \\(\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}}=1\\)\\ with \\(N_1=r,\\ N_2=N-r,\\ i=x\\). (Theorem 6.3.2 \\[\\begin{align*} E[X(X-1)] &amp;= \\sum\\limits_{x=0}^{n}x(x-1)\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n}\\frac{x(x-1)r(r-1)}{x(x-1)}\\frac{{r-2\\choose x-2} {N-r\\choose n-x}}{\\frac{N(N-1)}{n(n-1)}{N-2\\choose n-2}} \\\\ &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\sum\\limits_{x=0}^{n} \\frac{{r-2\\choose x-2}{N-r\\choose n-x}}{{N-2\\choose n-2}} \\\\ ^{[2]} &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\sum\\limits_{y=0}^{n-2} \\frac{{r-2\\choose y}{N-r\\choose n-y-2}}{{N-2\\choose n-2}} \\\\ ^{[3]} &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\cdot 1 \\\\ &amp;=\\frac{r(r-1)n(n-1)}{N(N-1)} \\end{align*}\\] For any integer \\(a\\) such that \\(0\\leq a\\leq k,\\ {n\\choose k}=\\frac{n(n-1)\\cdots(n-a+1)}{k(k-1)\\cdots(k-a+1)}{n-a\\choose k-a}\\) (Theorem 9.0.3). Let \\(y=x-1\\)  \\(\\Rightarrow x=y+1\\). \\(\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}}=1\\)\\ with \\(N_1=r,\\ N_2=N-r,\\ i=x\\). (Theorem 6.3.2 \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\frac{rn}{N} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= (E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= E(X^2-X) + E(X) - E(X)^2 \\\\ &amp;= E[X(X-1)] + E(X) - E(X)^2\\\\ &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)} + \\frac{rn}{N} - \\frac{r^2n^2}{N^2} \\\\ &amp;= \\frac{r(r-1)n(n-1)N}{N^2(N-1)} + \\frac{rnN(N-1)}{N^2(N-1)} - \\frac{r^2n^2(N-1)}{N^2(N-1)} \\\\ &amp;= \\frac{(r^2-r)(n^2-n)N rn(N^2-N)-r^2n^2(N-1)}{N^2(N-1)} \\\\ &amp;= \\frac{(r^2n^2N-r^2n^2N-rn^2N+rnN+rnN^2-rnN-r^2n^2N+r^2n^2}{N^2(N-1)} \\\\ &amp;= \\frac{-r^2nN-rn^2N+rnN^2+r^2n^2}{N^2(N-1)} \\\\ &amp;= \\frac{nr(-rN-nN+N^2+rn}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N^2-nN-rN+rn}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N-r)(N-n)}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N-r)(N-n)}{N\\cdot N(N-1)} \\\\ &amp;= \\frac{nr}{N}\\cdot\\frac{N-r}{N}\\cdot\\frac{N-n}{N-1} \\end{align*}\\] 20.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ &amp;= \\frac{1}{{N\\choose n}}\\sum\\limits_{x=0}^{n}e^{tx}{r\\choose x}{N-r\\choose n-x} \\\\ &amp;= \\frac{1}{{N\\choose n}}[e^{0t}{r\\choose 0}{N-r\\choose n-0} + e^{1t}{r\\choose 1}{N-r\\choose n-1} + e^{2t}{r\\choose 2}{N-r\\choose n-2} + \\cdots + e^{nt}{r\\choose n}{N-r\\choose n-n}] \\\\ &amp;= \\frac{1}{{N\\choose n}}[{N-r\\choose n-0}+e^{t}{r\\choose 1}{N-r\\choose n-1} + e^{2t}{r\\choose 2}{N-r\\choose n-2}+\\cdots+e^{nt}{r\\choose n}{N-r\\choose n-n}] \\end{align*}\\] This mgf does not reduce to any form which can be differentiated, and we cannot use it to generate moments for the distribution. 20.5 Theorems for the Hypergeometric Distribution 20.5.1 Validity of the Distribution \\[ \\sum\\limits_{x=0}^{n}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} = 1 \\] Proof: Theorem 6.3.1 states \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1}+\\cdots +{N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} \\\\ = \\sum\\limits_{x=0}^{n}{N_1\\choose x}{N_2\\choose n-x} \\\\ = {N_1+N_2\\choose n} \\] Using \\(N_1 = r\\) and \\(N_2 = N-r\\) we have \\[\\begin{align*} \\sum\\limits_{x=0}^{n}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} &amp;= \\frac{{r+N-r\\choose n}}{{N\\choose n}} \\\\ &amp;= \\frac{{N\\choose n}}{{N\\choose n}} \\\\ &amp;= 1 \\end{align*}\\] "],
["integration-techniques-and-theorems.html", "21 Integration: Techniques and Theorems 21.1 Elementary Theorems", " 21 Integration: Techniques and Theorems 21.1 Elementary Theorems 21.1.1 Integration of Even Functions about Zero Suppose \\(f\\) is an integratable function, and let \\(F(x)=\\int\\limits_{0}^{x_0}f(x)dx\\). Then \\(\\int\\limits_{-x_0}^{0}f(x)dx=\\int\\limits_{0}^{x_0}f(x)dx\\) if and only if \\(f\\) is an even function.\\ Proof: First, let \\(f\\) be an even function. Then, by Theorem 14.4.5, the anti-derivative \\(F\\) is an odd function. \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= F(0) + F(x_0) \\\\ ^{[1]} &amp;= F(x_0) \\\\ \\\\ \\\\ \\int\\limits_{0}^{x_0}f(x)dx &amp;= F(x_0) - F(0) \\\\ &amp;= F(x_0) \\end{align*}\\] \\(F(0)=\\int\\limits_{0}^{0}f(x)dx=0\\). So \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(x_0) \\\\ &amp;= \\int\\limits_{0}^{x_0}f(x)dx \\end{align*}\\] Now suppose \\[ \\int\\limits_{-x_0}^{0}f(x)dx = \\int\\limits_{0}^{x_0}f(x)dx \\] Then \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= -F(-x_0) \\end{align*}\\] and \\[\\begin{align*} \\int\\limits_{0}^{x_0}f(x)dx = F(x_0) - F(0) \\\\ = F(x_0)$ \\end{align*}\\] So \\[\\begin{align*} -F(-x_0) &amp;= F(x_0) \\\\ \\Rightarrow F(-x_0) &amp;= -F(x_0) \\end{align*}\\] This satisfies the definition of an odd function. So by Theorem 14.4.4, \\(f\\) must be an even function. 21.1.2 Corollary If \\(f\\) is a continuous and even function and \\(t\\in\\Re\\), then \\[ \\int\\limits_{-t}^{t}f(x)dx = 2\\int\\limits_{0}^{t}f(x)dx \\] Furthermore, \\[\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\]. Proof: Since \\(f(x)\\) is even and by Theorem 21.1.1 \\[\\begin{align*} \\int\\limits_{-t}^{0}f(-x)dx &amp;= \\int\\limits_{-t}^{0}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] It follows that \\[\\begin{align*} \\int\\limits_{-t}^{t}f(x)dx &amp;= \\int\\limits_{-t}^{0}f(-x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= 2\\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] The second statement is proven by taking the limits as \\(t\\rightarrow\\infty\\). 21.1.3 Integrals of Horizontal Translations Let \\(x\\) be any real number and \\(a,b,\\) and \\(c\\) be constants. Also, let \\(f(x)\\) be continuous on the interval \\((a,b)\\). Then \\[\\int\\limits_{a}^{b}f(x)dx = \\int\\limits_{a+c}^{b+c} f(x+c)dx\\] Proof: The proof of this theorem is completed by applying a change of variable to \\[\\int\\limits_{a}^{b}f(x)dx\\] We let \\[\\begin{align*} y &amp;= x+c \\\\ \\Rightarrow x &amp;= y-c \\end{align*}\\] So \\(dx=dy\\). \\[\\begin{align*} x &amp;= a &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= a+c\\\\ x &amp;= b &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= b+c \\end{align*}\\]. Thus \\[\\begin{align*} \\int\\limits_{a}^{b}f(x)dx &amp;= \\int\\limits_{a+c}^{b+c}f(y)dy \\\\ &amp;= \\int\\limits_{a+c}^{b+c}f(x+c)dx \\end{align*}\\] "],
["logistic-regression.html", "22 Logistic Regression 22.1 The Logit Transformation 22.2 Retrieving the Modelled Probability", " 22 Logistic Regression 22.1 The Logit Transformation Logistic regression intends to model the probability that a given response will occur based on the characteristics in the predictor variables. The response, therefore, is a probability; a value between zero and one. This is in contrast to typical linear regression where the response lies range of real numbers. This is further complicated by the fact that in the observed data, a subject does not have a probabilistic response, but the response is the dichotomous occurrence of an event. The nature of the response variable in logistic regression, therefore, necessitates that a transformation be applied. 22.1.1 Obtaining the Logit Transformation Let us call the response for our logistic model the probability \\(p\\) that an event will occur. Since \\(p\\) is a probability, by definition, it’s domain is from 0 to 1. Ideally, we would like to have a response whose domain is \\(\\Re\\). First, let us consider the transformation \\(\\frac{p}{1-p}\\) (also called the odds of \\(p\\)) and it’s limits as \\(p\\) approaches 0 and 1. \\[\\begin{align*} \\lim\\limits_{p\\rightarrow 0} \\frac{p}{1-p} = \\frac{0}{1-0} = 0 \\end{align*}\\] and \\[\\begin{align*} \\lim\\limits_{p\\rightarrow 1} \\frac{p}{1-p} = \\infty \\end{align*}\\] So the domain of \\(\\frac{p}{1-p}\\) is \\((0,\\infty)\\). This is handy, as we do know that the \\(\\ln\\) function takes a variable on the domain \\((0,\\infty)\\) and maps it onto the range \\((-\\infty,\\infty)\\). Thus, the equation for our logistic model the transformation (called the logit, or log-odds): \\[\\ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n\\] 22.2 Retrieving the Modelled Probability While the logit transformation allows us to perform the logistic regression, the resulting measure tells us about the risk of an event associated with a predictor, but does not tell us directly about the probability of the event occuring. If we need to know the probability of the event ocurring, we must back transform the results of our regression equation. Essentially, we extract \\(\\hat p\\) from the modelled log-odds. This is done as by: \\[\\begin{align*} \\ln\\bigg(\\frac{\\hat{p}}{1-\\hat{p}}\\bigg) &amp;= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n \\\\ ^{[1]} &amp;= B \\\\ \\Rightarrow \\frac{\\hat{p}}{1-\\hat{p}} &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= (1-\\hat{p})\\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= \\exp(B) - \\hat{p} \\cdot \\exp(B) \\\\ \\Rightarrow \\hat{p} + \\hat{p} \\cdot \\exp(B) &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p}(1+\\exp(B)) &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= \\frac{\\exp(B)}{1 + \\exp(B)} \\\\ \\Rightarrow \\hat{p} &amp;= \\frac{\\exp(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n)} {1 + \\exp(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n)} \\end{align*}\\] Let \\(B = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n\\) "],
["mantel-haenszel-test-of-linear-trend.html", "23 Mantel-Haenszel Test of Linear Trend", " 23 Mantel-Haenszel Test of Linear Trend The Mantel-Haenszel Test is a method for testing independence of categorical variables on an ordinal scale. See Agresti (1996) for more discussion. Let \\(X\\) be a categorical variable of ordinal type with \\(R\\) levels. Let \\(Y\\) be a categorical variable of ordinal type with \\(C\\) levels. Suppose we take a sample of size \\(n\\) and take a measurement on each item in the sample with respect to \\(X\\) and \\(Y\\). The presence of a progresive between \\(X\\) and \\(Y\\) can be tested using the correlation coefficient \\(\\rho\\) (Mantel 1963). We may begin by taking the estimate of \\(\\rho\\) \\[\\begin{align*} r &amp;= \\frac{\\widehat{Cov}(X,Y)}{\\sqrt{s_X^2 s_Y^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}(x_i-\\bar x)(y_j-\\bar y)p(x_i,y_j)} {\\sqrt{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2p(x_i) \\sum\\limits_{j=1}^{n}(y_j-\\bar y)^2p(y_j)}} \\end{align*}\\] But since \\(X\\) and \\(Y\\) are categorical, we cannot sensibly perform any of the operations. Instead, we define the variables \\(U\\) and \\(V\\) to be the ordinal scoring of \\(X\\) and \\(Y\\) respecitively. In other words, \\(U_i\\) is the score for the category of \\(X_i\\) and \\(V_i\\) is the score for the category of \\(Y_i\\). Using this replacement we get \\[ r = \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}(u_i-\\bar u)(v_j-\\bar v)p(u_i,v_j)} {\\sqrt{\\sum\\limits_{i=1}^{n}(u_i-\\bar u)^2p(u_i) \\sum\\limits_{j=1}^{n}(v_j-\\bar v)^2p(v_j)}} \\] To obtain the values of \\(\\bar u\\) and \\(\\bar v\\), we consider the following table. Recall that there are \\(R\\) levels of the variable \\(X\\) and \\(C\\) levels of the variable \\(Y\\). u1 Category of V Category of U 1 2 … C Total U 1 n11 n12 … n1c n1+ U1 2 n21 n22 … n2c n2+ U2 … … … … … … … R nr1 nr2 … nrc nr+ Ur Total n+1 n+2 … n+c n++ V V1 V2 … Vc In the table, \\(n_{rc},\\ r=1,2,\\ldots,R,\\ c=1,2,\\ldots,C\\) is the number of observations in the sample with scores \\(r\\) and \\(c\\). From the table we can understand the marginal distributions of \\(U\\) and \\(V\\), and we see that for \\(r=1,2,\\ldots,R,\\ c=1,2,\\ldots,C\\) \\[\\begin{align*} p(u_r) &amp;= \\frac{n_{r+}}{n} \\\\ \\\\ p(v_c) &amp;= \\frac{n_{+ c}}{n} \\\\ \\\\ p(u_r,v_c) &amp;= \\frac{n_{rc}}{n} \\\\ \\\\ \\bar u &amp;= \\sum\\limits_{r=1}^{R}u_i\\frac{n_{r+}}{n} \\\\ \\\\ \\bar v &amp;= \\sum\\limits_{c=1}^{C}v_i\\frac{n_{+ c}}{n} \\end{align*}\\] With these observations, we can derive the value of \\(r\\) as \\[\\begin{align*} r &amp;= \\frac{\\widehat{Cov}(U,V)}{\\sqrt{s_U^2s_V^2}} \\\\ &amp;= \\frac{\\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_r-\\bar u)(v_c-\\bar v)n_{rc}}{n-1}} {\\sqrt{\\frac{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2}{n-1} \\frac{\\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}{n-1}}} \\\\ &amp;= \\frac{\\frac{1}{n-1}\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_r-\\bar u)(v_c-\\bar v)n_{rc}} {\\frac{1}{n-1}\\sqrt{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2 \\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}(u_r-\\bar u)(v_c-\\bar v)n_{rc}} {\\sqrt{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2 \\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_rv_c-u_r\\bar v-\\bar uv_c+\\bar u\\bar v)n_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_rv_cn_{rc}-u_r\\bar vn_{rc}-\\bar uv_cn_{rc}+\\bar u\\bar vn_{rc})} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_r\\bar vn_{rc} - \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}\\bar uv_cn_{rc} + \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}\\bar u\\bar vn_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\bar v\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rn_{rc} - \\bar u\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}v_cn_{rc} + \\bar u\\bar v\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}n_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\bar v\\sum\\limits_{r=1}^{R}u_rn_{r+} - \\bar u\\sum\\limits_{c=1}^{C}v_cn_{+ c} + \\bar u\\bar vn} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\sum\\limits_{r=1}^{R} u_rn_{r+}}{n} - \\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n} + n\\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n^2}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{2\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n} + \\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{1}{n}(\\sum\\limits_{r=1}^{R}u_rn_{r+}) (\\sum\\limits_{c=1}^{C}v_cn_{+ c})} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\end{align*}\\] References "],
["mcnemar-test.html", "24 McNemar Test 24.1 Sample Size Calculations for Paired Design 24.2 Power Calculation for Paired Design", " 24 McNemar Test This chapter only represents work that needed to be done for a specific application. Some of the formulas and equations provided are not necessarily coherent with the articles originally published on the topic. The majority of the work was derived from Connor’s 1987 paper. This chapter could benefit from a great deal of improvement and additional explanation. The McNemar Test compares proportions of related samples in which the outcome for each sample is a binary response. Th response is the same in each sample. Related samples may mean subjects from one sample are matched with subjects with similiar qualities (subjects are related, but outcomes are not); or it may mean that subjects are paired with themselves, as in a pre-post design (outcomes are related because they are taken on the same subject).\\ Trial 2 1 0 Trial 1 1 p11 p10 p1 0 p01 p00 1 - p1 p2 1 - p2 The table demonstrates the possible outcomes of such a experiment. Suppose \\(Y_i\\) deontes the outcome of Trial 1 and \\(Y_j\\) denotes the outcome of Trial 2. \\(Y_{ij}\\) denotes the outcome of the first and second trials, that is \\(Y_{ij} = Y_i \\cap Y_j\\). Then: \\[\\begin{align*} p_{11} &amp;= P(Y_i = 1 \\cap Y_j = 1) \\\\ p_{10} &amp;= P(Y_i = 1 \\cap Y_j = 0) \\\\ p_{01} &amp;= P(Y_i = 0 \\cap Y_j = 1) \\\\ p_{00} &amp;= P(Y_i = 0 \\cap Y_j = 0) \\\\ \\end{align*}\\] Furthermore \\[\\begin{align*} p_1 &amp;= p_{11} + p_{10} &amp;= P(Y_i = 1) \\\\ p_2 &amp;= p_{11} + p_{01} &amp;= P(Y_j = 1) \\end{align*}\\] In upcoming sections, the values of the difference and sum of \\(p_1\\) and \\(p_2\\) will be important, so we define \\[\\begin{align*} \\delta &amp;= p_1 - p_2 &amp;= (p_{11} + p_{10}) - (p_{11} + p_{01}) &amp;= p_{10} - p_{01} \\\\ \\\\ \\\\ \\psi &amp;= p_1 + p_2 &amp;= (p_{11} + p_{10}) + (p_{11} + p_{01}) &amp;= 2p_{11} + p_{10} + p_{01} \\end{align*}\\] 24.1 Sample Size Calculations for Paired Design Three methods of calculating power for McNemar’s Test have been presented. Miettenen proposed a method of estimating the power in 1968. Duffy provided the exact power in 1984. Connor provided an additional method of estimating the power in 1987. 24.1.1 Miettenen’s Sample Size Calculation Miettenen was the first to provide a popular power calculation for McNemar’s test with a paired-design. Duffy would later show that this calculation tends to under-estimate the power. Subsequently, sample sizes derived from this calculation are generally lower than is needed to obtain the designed power. Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). Now suppose we wish to determine the sample size \\(n_m\\) (for Miettenen method) required to find a change in proportion from \\(p_1\\) to \\(p_2\\) with significance \\(\\alpha\\) and power \\(1-\\beta\\). The required sample size is calculated by: \\[ n_m = \\frac{\\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2}{\\delta^2} \\] 24.1.2 Connor’s Sample Size Calculation Connor proposed a method for sample size calculation in addition to Miettenen’s. Connor’s method tends to over-estimate the power. Subsequently, sample sizes derived from this calculation are generally higher than is actually needed to obtain the designed power. Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). Now suppose we wish to determine the sample size \\(n_c\\) (for Miettenen method) required to find a change in proportion from \\(p_1\\) to \\(p_2\\) with significance \\(\\alpha\\) and power \\(1-\\beta\\). The required sample size is calculated by: \\[ n_c = \\frac{\\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2}{\\delta^2} \\] 24.2 Power Calculation for Paired Design The following power calculations are derived in a backward fashion. In the application I had at the time, I needed to calculate sample sizes, and also wanted to allow functionality in my R function to obtain power with a supplied sample size. Since I had the sample size equations, I solved for the power. Normally this would be done the other way around, ie, take the power function and solve for \\(n\\). In the future, this should be revised appropriately. 24.2.1 Power Calculation for Miettenen Method Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). The power function can be found from the sample size equation by: \\[\\begin{align*} n_m &amp;= \\frac{\\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2} {\\delta^2} \\\\ \\Rightarrow n_m\\delta^2 &amp;= \\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2 \\\\ \\Rightarrow \\sqrt{n_m}\\delta &amp;= Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\\\ \\Rightarrow \\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2} &amp;= Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\\\ \\Rightarrow \\frac{\\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2}}{\\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2}} &amp;= Z_\\beta \\\\ \\Rightarrow \\Phi^{-1}\\bigg(\\frac{\\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2}} {\\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} }\\bigg) &amp;= 1 - \\beta \\end{align*}\\] 24.2.2 Power Calculation for Connor Method Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). The power function can be found from the sample size equation by: \\[\\begin{align*} n_c &amp;= \\frac{\\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2}{\\delta^2} \\\\ \\Rightarrow n_c\\delta^2 &amp;= \\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2 \\\\ \\Rightarrow \\sqrt{n_c\\delta} &amp;= Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\\\ \\Rightarrow \\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2} &amp;= Z_\\beta (\\psi - \\delta^2)^{1/2} \\\\ \\Rightarrow \\frac{\\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2}}{(\\psi - \\delta^2)^{1/2}} &amp;= Z_\\beta \\\\ \\Rightarrow \\Phi^{-1}\\bigg(\\frac{\\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2}}{(\\psi - \\delta^2)^{1/2}}\\bigg) &amp;= 1 - \\beta \\end{align*}\\] "],
["moments-and-moment-generating-functions.html", "25 Moments and Moment Generating Functions 25.1 Definitions of Moments 25.2 Moment Generating Functions", " 25 Moments and Moment Generating Functions 25.1 Definitions of Moments 25.1.1 Definition: General Definition of Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about some point \\(c\\) is defined to be \\(E[(X-c)^k]\\). There are two moments that are of particular use in statistics. First, the moment of \\(X\\) about the origin; second, the moment of \\(X\\) about the mean. 25.1.2 Definition: Ordinary Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about the origin is defined to be \\(E[(X-0)^k] = E(X^k)\\). 25.1.3 Definition: Central Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about the mean \\(\\mu\\) is defined to be \\(E[(X-\\mu)^k]\\). Using these definitions we can derive the first three central moments as follows: \\[\\begin{align*} E[(X-\\mu)^1] &amp;= E(X - \\mu) \\\\ &amp;= E(X) - \\mu \\\\ &amp;= E(X) - E(X) \\\\ \\\\ E[(X-\\mu)^2] &amp;= E[(X-\\mu)(X-\\mu)] \\\\ &amp;= E(X^2-\\mu X-\\mu X+\\mu^2) \\\\ &amp;= E(X^2-2\\mu X+\\mu^2) \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\\\ \\\\ \\\\ E[(X-\\mu)^3] &amp;= E[(X-\\mu)(X-\\mu)(X-\\mu)] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)(X-\\mu)] \\\\ &amp;= E(X^3-\\mu X^2-2\\mu X^2+2\\mu^2X+\\mu^2X+\\mu^3) \\\\ &amp;= E(X^3-3\\mu X^2+3\\mu^2X-\\mu^3) \\\\ &amp;= E(X^3) - E(3\\mu X^2) + E(3\\mu^2X) - E(\\mu^3) \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 3\\mu^2E(X) - \\mu^3 \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 3\\mu^3 - \\mu^3 \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 2\\mu^3 \\end{align*}\\] It should be noticed that with all of these results, the moment about the mean can be evaluated by finding the ordinary moments. Thus, if we can find a consistent way to generate ordinary moments , we may use these results to find various parameters of a distribution. 25.2 Moment Generating Functions 25.2.1 Definition: Moment Generating Function The moment generating function of a random variable, denoted \\(M_X(t)\\), is defined to be: \\[ M_X(t) = E(e^{tX}) \\] The moment generating function of \\(X\\) is said to exist if for any positive constant \\(c,\\ M_X(t)\\) is finite for \\(|t|&lt;c\\). The definition can be expanded to \\[\\begin{align*} M_X(t) &amp;= E(e^{tX} \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}e^{tx_i}p(x_i) \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{\\infty}[\\frac{(tx_i)^0}{0!}+\\frac{(tx_i)^1}{1!} + \\frac{(tx_i)^2}{2!}+\\frac{(tx_i)^3}{3!}+\\cdots]p(x_i) \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}[1+tx_i+\\frac{(tx_i)^2}{2!} + \\frac{(tx_i)^3}{3!}+\\cdots]p(x_i) \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}[p(x_i)+tx_ip(x_i) + \\frac{(tx_i)^2}{2!}p(x_i)+\\frac{(tx_i)^3}{3!}p(x_i) + \\cdots] \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}p(x_i)+\\sum\\limits_{i=1}^{\\infty}tx_ip(x_i) + \\sum\\limits_{i=1}^{\\infty}\\frac{(tx_i)^2}{2!}p(x_i) + \\sum\\limits_{i=1}^{\\infty}\\frac{(tx_i)^3}{3!}p(x_i) + \\cdots \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}p(x_i)+t\\sum\\limits_{i=1}^{\\infty}x_ip(x_i) + \\frac{t^2}{2!}\\sum\\limits_{i=1}^{\\infty}x_i^2p(x_i) + \\frac{t^3}{3!}\\sum\\limits_{i=1}^{\\infty}x_i^3p(x_i)+\\cdots \\\\ &amp;= 1 + tE(X) \\\\ &amp;= \\frac{t^2}{2!}E(X^2) + \\frac{t^3}{3!} + \\cdots \\end{align*}\\] Taylor Series Expansion: \\(e^x=\\frac{x^0}{0!}+\\frac{x^1}{1!}+\\frac{x^2}{2!}\\cdots = 1+x+\\frac{x^2}{2!}+\\cdots\\) 25.2.2 Theorem: Extraction of Moments from Moment Generating Functions Let \\(M_X^{(k)}(t)\\) denote the \\(k^{th}\\) derivative of \\(M_X(t)\\) with respect to \\(t\\). Then \\(M_X^{(k)}(0)=E(X^k)\\). Proof: \\[\\begin{align*} M_X(t) &amp;= 1 + tE(X) \\\\ &amp;= \\frac{t^2}{2!}E(X^2) + \\frac{t^3}{3!} + \\cdots \\\\ \\\\ \\\\ M_X^{(1)}(t) &amp;= 0 + E(X) + \\frac{2t}{2!}E(X^2) + \\frac{3t^2}{3!}E(X^3) + \\cdots \\\\ &amp;= E(X) + tE(X^2) + \\frac{t^2}{2!}E(X^3) + \\cdots \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= 0 + E(X^2) + \\frac{2t}{2!}E(X^3) + \\frac{3t^2}{3!}E(X^4) + \\cdots \\\\ &amp;= E(X^2) + tE(X^3) + \\frac{t^2}{2!}E(X^4) + \\cdots \\\\ \\vdots \\\\ \\\\ \\\\ M_X^{(k)}(t) &amp;= 0 + E(X^k) + \\frac{2t}{2!}E(X^{k+1}) + \\frac{3t^2}{3!}E(X^{k+2}) + \\cdots \\\\ &amp;= E(X^k) + tE(X^{k+1}) + \\frac{t^2}{2!}E(X^{k+2}) + \\cdots \\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= 0 + E(X) + \\frac{2\\cdot 0}{2!}E(X^2) + \\frac{3\\cdot 0t^2}{3!}E(X^3) + \\cdots \\\\ &amp;= E(X)\\\\ \\\\ \\\\ M_X^{(2)}(0) &amp;= 0 + E(X^2) + \\frac{2\\cdot 0}{2!}E(X^3) + \\frac{3\\cdot 0^2}{3!}E(X^4) + \\cdots \\\\ &amp;= E(X^2) \\\\ \\\\ \\\\ \\vdots \\\\ \\\\ \\\\ M_X^{(0)}(t) &amp;= 0 + E(X^k) + \\frac{2\\cdot 0}{2!}E(X^{k+1}) + \\frac{3\\cdot 0^2}{3!}E(X^{k+2}) + \\cdots \\\\ &amp;= E(X^k) \\end{align*}\\] "],
["multinomial-distribution.html", "26 Multinomial Distribution 26.1 Cumulative Distribution Function 26.2 Expected Values", " 26 Multinomial Distribution Let \\(E_1,E_2,\\ldots,E_k\\) be mutually exclusive and exhaustive events and define a multinomial experiment to have the following characteristics: The experiment consists of \\(N\\) indepedendent trials. The outcome of each trial belongs to exactly one \\(E_j,\\ j=1,2,\\ldots,k\\). The probability that an outcome belongs to event \\(E_j\\) is \\(p_j\\). Let \\(X_{ij}=\\left\\{ \\begin{array}{ll} 1&amp; \\rm if\\ the\\ outcome\\ of\\ the\\ \\it i^{th}\\ \\rm trial\\ belongs\\ to\\ \\it E_j.\\\\ 0&amp; otherwise \\end{array} \\right.\\) and let \\(n_j=\\sum\\limits_{i=1}^{N}X_{ij}\\). Under these conditions, \\(N=\\sum\\limits_{j=1}^{k}n_j\\). By Lemma 9.0.1 the number of ways to partition \\(N\\) into the \\(k\\) events, without respect to order, is \\(\\frac{N!}{n_1!n_2!\\cdots n_k!}\\). So the probabilitiy of any particular outcome of the experiment is \\[p(n_1,n_2,\\ldots,n_{k-1}) = \\frac{N!}{n_1!n_2!\\cdots n_{k-1}!n^\\prime!} p_1^{n_1}p_2^{n_2}\\cdots p_{(k-1)}p^{\\prime n^\\prime} \\] where \\(n^\\prime = N - n_1 - n_2 - \\cdots - n_{k-1}\\) and \\(p^\\prime = 1 - p_1 - p_2 - \\cdots - p_{k-1}\\). In other words, the entire distribution is defined by the first \\(k-1\\) terms. 26.1 Cumulative Distribution Function \\[P(n_1,n_2,\\ldots,n_{k-1}) = \\sum\\limits_{n_1=0}^{N} \\sum\\limits_{n_2=0}^{N-n_1} \\cdots \\sum\\limits_{n_{k-1}=0}^{N^\\prime} \\frac{N!}{n_1!n_2!\\cdots n_{k-1}!n^\\prime!} p_1^{n_1}p_2^{n_2}\\cdots p_{k-1}^{n_{k-1}}p^{\\prime n^\\prime}\\] where \\(N^\\prime=N-n_1-n_2-\\cdots n_{k-1}\\). 26.2 Expected Values Since this is a multivariate distribution, we discuss finding the expected values for each variate \\(n_j\\) as opposed to an overall mean.\\ \\(n_j\\) is a random variable from a multinomial distribution that specifies how many of the \\(N\\) observations were of type \\(j\\). Each of the \\(N\\) observations willf all into exactly one type, so we can conclude that an observation is either of type \\(j\\) or it isn’t. Also, it is of type \\(j\\) with probability \\(p_j\\), and each trial is independent. Thus, we may consider \\(n_j\\) a binomial random variable and \\(E(n_j)=Np_j\\) and \\(V(n_j)=Np_j(1-p_j)\\). Now we must derive the Covariance of \\(n_j\\). We begin by defnining the random variables for \\(j\\neq m\\): \\[X_i=\\left\\{ \\begin{array}{ll} 1 &amp; \\rm if\\ trial \\it\\ i\\ \\rm results\\ in\\ type\\ \\it j.\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\[Y_i=\\left\\{ \\begin{array}{ll} 1 &amp; \\rm if\\ trial \\it\\ i\\ \\rm results\\ in\\ type\\ \\it m.\\\\ 0 &amp; otherwise \\end{array} \\right. \\] and let \\(n_j=\\sum\\limits_{i=1}^{n}X_i\\) and \\(n_m=\\sum\\limits_{i=1}^{n}Y_i\\). Since \\(X_i\\) and \\(Y_i\\) cannot simultaneously equal 1, \\(X_i\\cdot Y_i=0\\) for all \\(i\\). We thus have the following results so far: \\[E(X_i\\cdot Y_i) = 0\\] \\[E(X_i) = p_j\\] \\[E(Y_i) = p_m\\] \\(Cov(X_i,Y_i) = 0\\) if \\(i\\neq j\\) because the trials are independent \\(Cov(X_i,Y_i) = E(X_i\\cdot Y_i) - E(X_i)E(Y_i) = 0 - p_j p_m = -p_j p_m\\). (\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\) (Theorem 11.2.2) Using these results we find the Covariance of \\(n_j\\) and \\(n_m\\). \\[\\begin{align*} Cov(n_j,n_m) &amp;= \\sum\\limits_{j=1}^{N}\\sum\\limits_{m=1}^{N}Cov(X_i,Y_i) \\\\ &amp;= \\sum\\limits_{i=1}^{N}Cov(X_iY_i) + \\sum\\sum\\limits_{i\\neq j}Cov(X_i,Y_i) \\\\ &amp;= \\sum\\limits_{i=1}^{n}-p_jp_m + \\sum\\sum\\limits_{i\\neq j}0=-np_jp_m \\end{align*}\\] The Expected Values of the \\(p_j\\)’s can be found by \\[\\begin{align*} E(\\hat p_j) &amp;= E(\\frac{n_j}{N}) \\\\ &amp;= \\frac{1}{N}E(n_j) \\\\ &amp;= \\frac{1}{N}Np_j \\\\ &amp;= p_j \\\\ \\\\ \\\\ V(\\hat p_j) &amp;= V(\\frac{n_j}{N}) \\\\ &amp;= \\frac{1}{N^2}V(n_j) \\\\ &amp;= \\frac{1}{N^2}Np_j(1-p_j) \\\\ &amp;= \\frac{p_j(1-p_j)}{N} \\\\ \\\\ \\\\ Cov(\\hat p_j,\\hat p_m) &amp;= Cov(\\frac{n_j}{N},\\frac{n_m}{N}) \\\\ &amp;= \\frac{1}{N^2}Cov(n_j,n_m) \\\\ &amp;= \\frac{1}{N^2}(-Np_jp_m) \\\\ &amp;= \\frac{-p_jp_m}{N} \\end{align*}\\] "],
["normal-distribution.html", "27 Normal Distribution 27.1 Probability Distribution Function 27.2 Cumulative Distribution Function 27.3 Expected Values 27.4 Moment Generating Function 27.5 Maximum Likelihood Estimators 27.6 Theorems for the Normal Distribution", " 27 Normal Distribution 27.1 Probability Distribution Function A random variable \\(X\\) is said to have a Normal Distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) if its probability density function is \\[f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2},\\ \\ x\\in\\Re,\\ \\mu\\in\\Re,\\ 0&lt;\\sigma^2 \\] 27.2 Cumulative Distribution Function The cdf of the Normal Distribution cannot be written in closed form. It is not uncommon in statistical practice to denote the cdf of the Standard Normal Distribution, that is, the Normal distribution with \\(\\mu=0\\) and \\(\\sigma^2=1\\) to be denoted by \\(\\Phi(\\cdot)\\). Although it is rare, the \\(\\Phi(\\cdot)\\) notation is sometimes used to denote any cumulative density function. Because such citations are rare they are usually accompanied by a statement about the distribution \\(\\Phi(\\cdot)\\) represents. If no such statement is given, it is reasonably safe to assume that the function refers to the Standard Normal Distribution. The importance of the Standard Normal Distribution is made evident by the fact that the area under any Normal curve is proportional to the distribution’s variance. A proof of this is provided in Theorem Figure 27.1: The graphs on the left show probability distribution functions, and the graphs on the right show cumulative distribution functions. The effect of changing the variance, a vertical rescaling of the distribution, is evident Figure 27.2: The graphs on the left show probability distribution functions, and the graphs on the right show cumulative distribution functions. The effect of changing the mean, a horizontal shift in location, is illustrated 27.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{-\\infty}^{\\infty}x\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\Big\\{-\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\Big\\} \\\\ ^{[1]} &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\int\\limits_{-\\infty}^{\\infty}x\\exp\\Big\\{-\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\Big\\} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\int\\limits_{-\\infty}^{\\infty}(z\\sigma+\\mu)\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}\\sigma dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\int\\limits_{-\\infty}^{\\infty}z\\sigma^2\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\} +\\mu\\sigma\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack\\int\\limits_{-\\infty}^{\\infty}z\\sigma^2\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz +\\int\\limits_{-\\infty}^{\\infty}\\mu\\sigma\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack-\\sigma^2\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}\\Bigg|_{-\\infty}^{\\infty} +\\int\\limits_{-\\infty}^{\\infty}\\mu\\sigma\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack-0+0+\\int\\limits_{-\\infty}^{\\infty}\\mu\\sigma\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack\\mu\\sigma\\int\\limits_{-\\infty}^{\\infty}\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack \\\\ ^{[2]} &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack2\\mu\\sigma\\int\\limits_{0}^{\\infty}\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack \\\\ &amp;= \\frac{2\\mu\\sigma}{\\sqrt{2\\pi}\\sigma}\\Bigg\\lbrack\\int\\limits_{0}^{\\infty}\\exp\\Big\\{-\\frac{1}{2}z^2\\Big\\}dz\\Bigg\\rbrack \\\\ ^{[3]} &amp;= \\frac{2\\mu}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}\\frac{1}{2}u^{-\\frac{1}{2}}e^{-\\frac{u}{2}}du\\\\ &amp;= \\frac{2\\mu}{2\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}u^{-\\frac{1}{2}}e^{-\\frac{u}{2}}du \\\\ &amp;= \\frac{\\mu}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-\\frac{u}{2}}du\\\\ ^{[4]} &amp;= \\frac{\\mu}{\\sqrt{2\\pi}}2^{\\frac{1}{2}}\\Gamma\\Big(\\frac{1}{2}\\Big) \\\\ ^{[5]} &amp;= \\frac{\\mu\\sqrt{2\\pi}}{\\sqrt{2\\pi}} \\\\ &amp;= \\mu \\end{align*}\\] Let \\(z=\\frac{x-\\mu}{\\sigma}\\\\ \\Rightarrow x=z\\sigma+\\mu \\Rightarrow dx=\\sigma dz\\) This change can be made because the function being integrated is an even function. See Theorem Let \\(u=z^2 \\Rightarrow z=u^{\\frac{1}{2}} \\Rightarrow dz=\\frac{1}{2}u^{-\\frac{1}{2}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\(\\Gamma(\\frac{1}{2})=\\sqrt{\\pi}\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\Big\\}dx \\\\ ^{[1]} &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{(z\\sigma+\\mu)^2}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{\\frac{z^2}{2}\\Big\\}\\sigma dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{(z\\sigma+\\mu)^2}{\\sqrt{2\\pi}} \\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{z^2\\sigma^2+2z\\sigma\\mu+\\mu^2}{\\sqrt{2\\pi}} \\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\Bigg\\lbrack \\frac{z^2\\sigma^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\} +\\frac{2z\\sigma\\mu}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\} +\\frac{\\mu^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\} \\Bigg\\rbrack dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{z^2\\sigma^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +\\int\\limits_{-\\infty}^{\\infty}\\frac{2z\\sigma\\mu}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +\\int\\limits_{-\\infty}^{\\infty}\\frac{\\mu^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz\\\\ &amp;= \\sigma^2\\int\\limits_{-\\infty}^{\\infty}\\frac{z^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +2\\sigma\\mu\\int\\limits_{-\\infty}^{\\infty}\\frac{z}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +\\mu^2\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz \\\\ ^{[2]} &amp;= \\sigma^2\\int\\limits_{-\\infty}^{\\infty}\\frac{z^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +2\\sigma\\mu\\int\\limits_{-\\infty}^{\\infty}\\frac{z}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +\\mu^2\\cdot 1 \\\\ ^{[3]} &amp;= \\sigma^2\\int\\limits_{-\\infty}^{\\infty}\\frac{z^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +2\\sigma\\mu E(Z)+\\mu^2 \\\\ &amp;= \\sigma^2\\int\\limits_{-\\infty}^{\\infty}\\frac{z^2}{\\sqrt{2\\pi}}\\exp\\Big\\{\\frac{z^2}{2}\\Big\\}dz +2\\sigma\\mu\\cdot0+\\mu^2 \\\\ ^{[4]} &amp;= \\sigma^2\\Bigg(\\frac{z}{\\sqrt{2\\pi}\\cdot-\\exp\\Big\\{-\\frac{z^2}{2}\\Big\\}} -\\int\\limits_{-\\infty}^{\\infty}-\\exp\\Big\\{-\\frac{z^2}{2}\\frac{1}{\\sqrt{2\\pi}}dz\\Bigg)+\\mu^2 \\\\ &amp;= \\sigma^2\\Bigg(-0+0+\\int\\limits_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp\\Big\\{-\\frac{z^2}{2}\\Big\\}dz\\Bigg)+\\mu^2 \\\\ ^{[5]} &amp;= \\sigma^2(1)+\\mu^2 \\\\ &amp;=\\sigma^2+\\mu^2 \\end{align*}\\] \\(z=\\frac{x-\\mu}{\\sigma} \\\\ \\Rightarrow x=z\\sigma+\\mu\\Rightarrow dx=\\sigma dz\\) Theorems and with \\(\\mu=0\\) and \\(\\sigma^2=1\\) Using the previous result with \\(\\mu=0\\) and \\(\\sigma^2=1\\), \\(E(Z)=0\\). Integration by parts: \\(\\int\\limits_{a}^{b}u\\ dv =\\lbrack u\\cdot v\\rbrack_a^b-\\int\\limits_{a}^{b}v\\ du\\) with \\(u=\\frac{z}{\\sqrt{2\\pi}}\\Rightarrow du=\\frac{1}{\\sqrt{2\\pi}}dz\\)\\ and \\(dv=z\\exp\\{-\\frac{z^2}{2}\\}\\Rightarrow v=-\\exp\\{-\\frac{z^2}{2}\\}\\) See footnote 2. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\mu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\sigma^2+\\mu^2-\\mu^2 \\\\ &amp;= \\sigma^2 \\end{align*}\\] 27.4 Moment Generating Function \\[\\begin{align*} E(e^{tX}) &amp;= \\int\\limits_{-\\infty}^{\\infty}\\exp\\{tx\\}\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{\\frac{1}{2\\sigma^2}(x-\\mu)^2\\Big\\}dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{tx-\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\Big\\}dx \\\\ ^{[1]} &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{t(z\\cdot\\sigma+\\mu-\\frac{1}{2}z^2\\Big\\}\\sigma dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{\\sigma}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{t(z\\cdot\\sigma+\\mu)-\\frac{1}{2}z^2\\Big\\}dz \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\{\\mu t\\}\\exp\\Big\\{z\\sigma t-\\frac{1}{2}z^2\\Big\\}dz \\\\ &amp;= \\exp\\{\\mu t\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{\\sigma}{\\sqrt{2\\pi}\\sigma} \\exp\\Big\\{z\\sigma t-\\frac{1}{2}z^2\\Big\\}dz \\\\ &amp;= \\exp\\{\\mu t\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} \\exp\\Big\\{z\\sigma t-\\frac{1}{2}z^2-\\frac{1}{2}t^2\\sigma^2+\\frac{1}{2}t^2\\sigma^2\\Big\\}dz \\\\ &amp;= \\exp\\{\\mu t\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} \\exp\\Big\\{-\\frac{1}{2}z^2+z\\sigma t-\\frac{1}{2}t^2\\sigma^2\\Big\\} \\exp\\Big\\{\\frac{1}{2}t^2\\sigma^2\\Big\\}dz \\\\ &amp;= \\exp\\{\\mu t\\}\\exp\\Big\\{\\frac{1}{2}t^2\\sigma^2\\Big\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} \\exp\\Big\\{z^2-2z\\sigma t+t^2\\sigma^2\\Big\\}dz \\\\ &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} \\exp\\Big\\{\\frac{1}{2}(z-\\sigma t)^2\\Big\\}dz \\\\ ^{[2]} &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} \\exp\\Big\\{\\frac{1}{2}(u)^2\\Big\\}du \\\\ &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\cdot 1 \\\\ &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\} \\end{align*}\\] \\(z=\\frac{x-\\mu}{\\sigma}\\\\ \\Rightarrow x=z\\cdot\\sigma+\\mu\\Rightarrow dx=\\sigma dz\\) \\(u=z-\\sigma t\\\\ \\Rightarrow z=u+\\sigma t \\Rightarrow dz=du\\) See Theorems and \\[\\begin{align*} M_X^{(1)}(t) &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\cdot(\\mu+t\\sigma^2) \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\cdot\\sigma^2 +(\\mu+t\\sigma^2)\\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}(\\mu+t\\sigma^2) \\\\ &amp;= M_X^{(2)}(t)=\\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\}\\cdot\\sigma^2 +(\\mu+t\\sigma^2)^2\\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0)=\\exp\\Big\\{\\mu\\cdot0+\\frac{0^2\\sigma^2}{2}\\Big\\}\\cdot(\\mu+0\\cdot\\sigma) \\\\ &amp;= e^0\\cdot(\\mu+0) \\\\ &amp;= 1\\cdot\\mu \\\\ &amp;= \\mu \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= \\exp\\Big\\{\\mu\\cdot0+\\frac{0^2\\sigma^2}{2}\\Big\\}\\cdot\\sigma^2 +(\\mu+0\\cdot \\sigma^2)^2\\exp\\Big\\{\\mu\\cdot0+\\frac{0^2\\sigma^2}{2}\\Big\\} \\\\ &amp;= e^0\\sigma^2+(\\mu+0)^2e^0 \\\\ &amp;= 1\\cdot\\sigma^2+\\mu^2\\cdot1 \\\\ &amp;= \\sigma^2+\\mu^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\mu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X) \\\\ &amp;= \\sigma^2+\\mu^2-\\mu^2 \\\\ &amp;= \\sigma^2 \\end{align*}\\] 27.5 Maximum Likelihood Estimators Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). 27.5.1 Likelihood Function \\[\\begin{align*} L(x_1,x_2,\\ldots,x_n|\\theta) &amp;= f(x_1|\\theta)f(x_2|\\theta)\\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg\\{\\frac{-(x_1-\\mu)^2}{2\\sigma^2}\\bigg\\} \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg\\{\\frac{-(x_2-\\mu)^2}{2\\sigma^2}\\bigg\\} \\cdots \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg\\{\\frac{-(x_n-\\mu)^2}{2\\sigma^2}\\bigg\\} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg\\{\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\bigg\\} \\\\ &amp;= \\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^n \\prod\\limits_{i=1}^{n}\\exp\\bigg\\{\\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\bigg\\} \\\\ &amp;= \\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^n \\exp\\bigg\\{ \\sum\\limits_{i=1}^{n}\\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\bigg\\} \\\\ &amp;= \\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^n \\exp\\bigg\\{ \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\bigg\\} \\end{align*}\\] 27.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\big(L(\\theta)\\big) \\\\ &amp;= \\ln\\Bigg(\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^n \\exp\\bigg\\{ \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\bigg\\}\\Bigg) \\\\ &amp;= \\ln\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\bigg)^n + \\ln\\bigg(\\exp\\bigg\\{ \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\bigg\\}\\bigg) \\\\ &amp;= n\\ln\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ &amp;= n\\ln\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}(x_i^2 - 2\\mu x_i + \\mu^2) \\\\ &amp;= n\\ln\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2} \\bigg\\lbrack \\sum\\limits_{i=1}^{n}x_i^2 - 2\\mu\\sum\\limits_{i=1}^{n}x_i + \\sum\\limits_{i=1}^{n}\\mu^2\\bigg\\rbrack \\\\ &amp;= n\\ln\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^{n}x_i^2 + \\frac{2\\mu}{2\\sigma^2}\\sum\\limits_{i=1}^{n}x_i - \\frac{1}{2\\sigma^2}n\\mu^2 \\\\ &amp;= n\\ln\\bigg(\\frac{1}{\\sqrt{2\\pi}}\\sigma^{-1}\\bigg) - \\frac{\\sigma^{-2}}{2}\\sum\\limits_{i=1}^{n}x_i^2 + \\mu\\sigma^{-2}\\sum\\limits_{i=1}^{n}x_i - \\frac{n\\mu^2\\sigma^{-2}}{2} \\end{align*}\\] 27.5.3 MLE for \\(\\mu\\) \\[\\begin{align*} \\frac{d\\ell}{d\\mu} &amp;= 0 - 0 + \\sigma^{-2}\\sum\\limits_{i=1}^{n} - \\frac{2n\\mu\\sigma^{-2}}{2} \\\\ &amp;= \\sigma^{-2}\\sum\\limits_{i=1}^{n} - n\\mu\\sigma^{-2}\\\\ \\\\ \\\\ 0 &amp;= \\sigma^{-2}\\sum\\limits_{i=1}^{n} - n\\mu\\sigma^{-2}\\\\ \\Rightarrow n\\mu\\sigma^{-2} &amp;= \\sigma^{-2}\\sum\\limits_{i=1}^{n}\\\\ \\Rightarrow n\\mu\\ &amp;= \\sum\\limits_{i=1}^{n}\\\\ \\Rightarrow \\mu &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\end{align*}\\] So \\(\\hat\\mu = \\frac{1}{n}\\sum\\limits_{i=1}^{n}\\) is the maximum likelihood estimator for \\(\\mu\\). 27.5.4 MLE for \\(\\sigma^2\\) The work in deriving the MLE for \\(\\sigma^2\\) is greatly reduced if we use \\[\\begin{align*} \\ell(\\theta) &amp;= n\\ln\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2\\\\ &amp;= n\\ln\\bigg(\\frac{1}{\\sqrt{2\\pi}}\\sigma^{-1}\\bigg) - \\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ \\\\ \\\\ \\frac{d\\ell}{d\\sigma} &amp;= n\\bigg(\\frac{1}{\\frac{1}{\\sqrt{2\\pi}}\\sigma^{-1}}\\bigg) \\bigg(\\frac{-1}{\\sqrt{2\\pi}\\sigma^{-2}}\\bigg) - \\frac{-2}{2}\\sigma^{-3} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ &amp;= \\frac{-n\\sqrt{2\\pi}\\sigma}{\\sqrt{2\\pi}\\sigma^2} + \\frac{1}{\\sigma^3} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2\\\\ &amp;= \\frac{-n}{\\sigma} + \\frac{1}{\\sigma} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ \\\\ \\\\ 0 &amp;= \\frac{-n}{\\sigma} + \\frac{1}{\\sigma} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ \\Rightarrow \\frac{n}{\\sigma} &amp;= \\frac{1}{\\sigma} \\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ \\Rightarrow \\frac{\\sigma^3}{\\sigma} &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\\\ \\Rightarrow \\sigma^2 &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2 \\end{align*}\\] So \\(\\hat\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n}-(x_i-\\mu)^2\\) is the maximum likelihood estimator for \\(\\sigma^2\\). Notice however that this \\(MLE\\) is a biased estimator. 27.6 Theorems for the Normal Distribution 27.6.1 Validity of the Distribution (Polar Coordinates) \\[\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx=1\\] Proof: Let \\(A = \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx\\). By using the identity transformation \\(y=x\\), we may also write \\[\\begin{align*} A &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(y-\\mu)^2}{2\\sigma^2}dy \\\\ \\Rightarrow A^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma} e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx\\cdot \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma} e^\\frac{-(y-\\mu)^2}{2\\sigma^2}dy \\\\ &amp;= \\frac{1}{2\\pi\\sigma^2}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty} e^\\frac{-(x-\\mu)^2}{2\\sigma^2}e^\\frac{-(y-\\mu)^2}{2\\sigma^2}dx\\ dy \\\\ &amp;= \\frac{1}{2\\pi\\sigma^2}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}-\\frac{(y-\\mu)^2}{2\\sigma^2}} \\\\ &amp;= \\frac{1}{2\\pi\\sigma^2}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{1}{2\\sigma^2}[(x-\\mu)^2+(y-\\mu)^2]}dx\\ dy \\\\ ^{[1]} &amp;= \\frac{1}{2\\pi\\sigma^2}\\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{1}{2\\sigma^2}t^2+u^2}dt\\ du \\\\ ^{[2]} &amp;= \\frac{1}{2\\pi\\sigma^2}\\int\\limits_{0}^{2\\pi}\\int\\limits_{0}^{\\infty} e^{-\\frac{r^2}{2\\sigma^2}}r\\ dr\\ d\\theta \\\\ &amp;=\\frac{1}{2\\pi\\sigma^2}\\int\\limits_{0}^{2\\pi} -\\sigma^2e^{-\\frac{r^2}{2\\sigma^2}}\\Big|_0^\\infty d\\theta \\\\ &amp;=\\frac{1}{2\\pi\\sigma^2}\\int\\limits_{0}^{2\\pi}0+\\sigma^2\\ d\\theta \\\\ &amp;=\\frac{1}{2\\pi\\sigma^2}\\int\\limits_{0}^{2\\pi}\\sigma^2\\ d\\theta \\\\ &amp;=\\frac{1}{2\\pi\\sigma^2}\\cdot \\Big(\\sigma^2\\theta\\Big|_0^{2\\pi}\\Big) \\\\ &amp;=\\frac{1}{2\\pi\\sigma^2}[2\\pi\\sigma^2-0] \\\\ &amp;=\\frac{2\\pi\\sigma^2}{2\\pi\\sigma^2} \\\\ &amp;=1\\\\ \\Rightarrow A^2 &amp;= 1 \\\\ \\Rightarrow A &amp;= 1 \\end{align*}\\] Substitute \\(t=x-\\mu\\) and \\(u=y-\\mu\\) \\(t=r\\cos\\theta\\) and \\(u=r\\sin\\theta\\).\\ Thus \\(t^2+u^2=r^2\\cos^2\\theta+r^2\\sin^2\\theta=r^2(\\cos^2\\theta+\\sin^2\\theta)=r^2\\) Thus \\(\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx=1\\) 27.6.2 Validity of the Distribution (Gamma Function) \\[\\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx=1\\] Proof: \\[\\begin{align*} \\int\\limits_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}\\sigma}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx \\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\int\\limits_{-\\infty}^{\\infty}e^\\frac{-(x-\\mu)^2}{2\\sigma^2}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int\\limits_{-\\infty}^{\\infty}e^\\frac{-(y)^2}{2\\sigma^2}dy\\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma} \\int\\limits_{-\\infty}^{\\infty}e^{-\\frac{1}{2}\\frac{y^2}{\\sigma^2}}dy \\\\ ^{[2]} &amp;= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{u}{2}}\\frac{1}{2}\\sigma u^{-\\frac{1}{2}}du \\\\ &amp;=\\frac{\\sigma}{2\\sqrt{2\\pi}\\sigma} \\int\\limits_{-\\infty}^{\\infty}u^{-\\frac{1}{2}}e^{-\\frac{u}{2}}du \\\\ ^{[3]} &amp;= \\frac{2\\sigma}{2\\sqrt{2\\pi}\\sigma} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-\\frac{u}{2}}du\\\\ ^{[4]} &amp;= \\frac{1}{\\sqrt{2\\pi}}2^\\frac{1}{2}\\Gamma\\big(\\frac{1}{2}\\big) \\\\ ^{[5]} &amp;= \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi}} \\\\ &amp;= 1 \\end{align*}\\] \\(y=x-\\mu\\ \\Rightarrow x=y+\\mu\\ \\Rightarrow dx=dy\\) \\(u=(\\frac{y}{\\sigma})^2\\ \\Rightarrow y=\\sigma u^\\frac{1}{2}\\ \\Rightarrow dy=\\frac{1}{2}\\sigma u^{-\\frac{1}{2}}\\) If \\(f(x)\\) is an even function then \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx =2\\int\\limits_{0}^{\\infty}f(x)dx\\) (Theorem ). \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\(\\Gamma(\\frac{1}{2})=\\sqrt{\\pi}\\) 27.6.3 Multiple of a Normal Random Variable Let \\(X\\) be a Normal random variable with parameters \\(\\mu\\) and \\(\\sigma^2\\), and let \\(c\\) be a constant. If \\(Y=cX\\), then \\(Y\\sim Normal(c\\mu,\\ c^2\\sigma^2)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tcX}) \\\\ &amp;= \\exp\\Big\\{\\mu tc+\\frac{t^2c^2\\sigma^2}{2}\\Big\\} \\\\ &amp;= \\exp\\Big\\{c\\mu t+\\frac{t^2c^2\\sigma^2}{2}\\Big\\} \\end{align*}\\] Which is the Moment Generating Function of a Normal random variable with mean \\(c\\mu\\) and variance \\(c^2\\sigma^2\\). Thus \\(Y\\sim Normal(c\\mu,\\ c^2\\sigma^2)\\). 27.6.4 Weighted Sum of Normal Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from Normal distributions with parameters \\(\\mu_i\\) and \\(\\sigma_i^2\\), i.e. \\(X_i\\sim Normal(\\mu_i,\\sigma_i^2),\\ i=1,2,\\ldots,n\\), and let \\(a_1,a_2,\\ldots,a_n\\) be constants. If \\(Y=\\sum\\limits_{i=1}^{n}a_iX_i\\), then \\(Y\\sim Normal\\big(\\sum\\limits_{i=1}^{n}a_i\\mu_i,\\ \\sum\\limits_{i=1}^{n}a_i^2\\sigma_i^2\\big)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;=E(e^{tY}) \\\\ &amp;=E(e^{t(a_1X_1+a_2X_2+\\cdots+a_nX_n}) \\\\ &amp;=E(e^{ta_1X_1}e^{ta_2X_2}\\cdots e^{ta_nX_n}) \\\\ &amp;=E(e^{ta_1X_1})E(e^{ta_2X_2})\\cdots E(e^{ta_nX_n}) \\\\ &amp;=\\exp\\Big\\{ta_1\\mu_1+\\frac{ta_1^2\\sigma_1^2}{2}\\Big\\}\\cdot \\exp\\Big\\{ta_2\\mu_2+\\frac{ta_2^2\\sigma_2^2}{2}\\Big\\} \\cdot \\cdots \\cdot \\exp\\Big\\{ta_n\\mu_n+\\frac{ta_n^2\\sigma_n^2}{2}\\Big\\} \\\\ &amp;=\\exp\\Big\\{ta_1\\mu_1+\\frac{ta_1^2\\sigma_1^2}{2}+ta_2\\mu_2+\\frac{ta_2^2\\sigma_2^2}{2} +\\cdots+ta_n\\mu_n+\\frac{ta_n^2\\sigma_n^2}{2}\\Big\\} \\\\ &amp;=\\exp\\Big\\{t\\sum\\limits_{i=1}^{n}a_i\\mu_i + \\frac{t\\sum\\limits_{i=1}^{n}a_i^2\\sigma_i^2}{2}\\Big\\} \\end{align*}\\] Which is the mgf of a Normal random variable with parameters \\(\\sum\\limits_{i=1}^{n}a_i\\mu_i\\) and \\(\\sum\\limits_{i=1}^{n}a_i^2\\sigma_i^2\\). Thus \\(Y\\sim Normal\\big(\\sum\\limits_{i=1}^{n}a_i\\mu_i,\\ \\sum\\limits_{i=1}^{n}a_i^2\\sigma_i^2\\big)\\). 27.6.5 Sum of Normal Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from Normal distributions with parameters \\(\\mu_i\\) and \\(\\sigma_i^2\\), i.e. \\(X_i\\sim Normal(\\mu_i, \\sigma_i^2),\\ i=1,2,\\ldots,n\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim Normal\\big(\\sum\\limits_{i=1}^{n}\\mu_i,\\ \\sum\\limits_{i=1}^{n}\\sigma_i^2\\big)\\). Proof: \\(Y=\\sum\\limits_{i=1}^{n}X_i\\) is a special case of the Weighted Sum of Normal Random Variables where each of the weights is equal to 1. It follows then that \\(Y\\sim Normal\\big(\\sum\\limits_{i=1}^{n}1\\mu_i,\\ \\sum\\limits_{i=1}^{n}1^2\\sigma_i^2\\big)\\) which is equivalent to saying \\(Y\\sim Normal\\big(\\sum\\limits_{i=1}^{n}\\mu_i,\\ \\sum\\limits_{i=1}^{n}\\sigma_i^2\\big)\\). 27.6.6 Standard Normal Transformation Let \\(X\\) be a Normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). That is, \\(X\\sim\\)Normal(\\(\\mu,\\sigma^2\\)). If \\(Z=\\frac{X-\\mu}{\\sigma}\\), then \\(Z\\sim\\)Normal(0,1), and we say \\(Z\\) has a Standard Normal distribution. Furthermore, if \\(Z\\sim\\)Normal(0,1), and \\(X=Z\\cdot\\sigma+\\mu\\), then \\(X\\sim Normal(\\mu,\\sigma^2\\)). Proof: First, since \\(X\\sim Normal(\\mu,\\sigma^2\\)) and \\(Z=\\frac{X-\\mu}{\\sigma}\\) \\[\\begin{align*} E(e^{tZ}) &amp;=E\\Big(\\exp\\Big\\{t\\frac{x-\\mu}{\\sigma}\\Big\\}\\Big) \\\\ &amp;=E\\Big(\\exp\\Big\\{\\frac{tx-t\\mu}{\\sigma}\\Big\\}\\Big) \\\\ &amp;=E\\Big(\\exp\\Big\\{\\frac{tx}{\\sigma}-\\frac{t\\mu}{\\sigma}\\Big\\}\\Big) \\\\ &amp;=E\\Big(\\exp\\Big\\{\\frac{tx}{\\sigma}\\Big\\}\\exp\\Big\\{-\\frac{t\\mu}{\\sigma}\\Big\\}\\Big) \\\\ &amp;=\\exp\\Big\\{\\frac{-t\\mu}{\\sigma}\\Big\\}E\\Big(\\exp\\Big\\{\\frac{t}{\\sigma}x\\Big\\} \\\\ &amp;=\\exp\\Big\\{\\frac{-t\\mu}{\\sigma}\\Big\\}\\exp\\Big\\{\\frac{t\\mu}{\\sigma}+\\frac{t^2\\sigma^2}{2\\sigma^2}\\Big\\} \\\\ &amp;=\\exp\\Big\\{\\frac{-t\\mu}{\\sigma}-\\frac{t\\mu}{\\sigma}+\\frac{t^2\\sigma^2}{2\\sigma^2}\\Big\\} \\\\ &amp;=\\exp\\Big\\{\\frac{t^2\\sigma^2}{2\\sigma^2}\\Big\\} \\end{align*}\\] Which is the Moment Generating Function of a Normal random variable with \\(\\mu=0\\) and \\(\\sigma^2=1\\). Thus \\(Z\\sim Normal(0,1)\\). To complete the proof, we start with \\(Z\\sim\\)Normal(0,1) and let \\(X=Z\\cdot\\sigma+\\mu\\). \\[\\begin{align*} E(e^{tX}) = E(e^{t(z\\sigma+\\mu)}) \\\\ = E(e^{tz\\sigma}e^{t\\mu}) \\\\ = e^{t\\mu}E(e^{t\\sigma z}) \\\\ = \\exp\\{t\\mu\\}\\exp\\Big\\{\\frac{t^2\\sigma^2}{2}\\Big\\} \\\\ = \\exp\\Big\\{\\mu t+\\frac{t^2\\sigma^2}{2}\\Big\\} \\end{align*}\\] Which is the Moment Generating Funciton of a Normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus \\(X\\sim Normal(\\mu,\\sigma^2)\\). 27.6.7 Distribution of \\(\\bar X\\) Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), i.e. \\(X_i\\sim Normal(\\mu,\\ \\sigma^2)\\). Let \\(\\bar X=\\frac{1}{n}\\sum\\limits_{i=1}^{n}X_i\\). Then \\(\\bar X\\sim Normal\\big(\\mu,\\ \\frac{\\sigma^2}{n}\\big)\\). Proof: \\(\\bar X=\\frac{1}{n}\\sum\\limits_{i=1}^{n}X_i\\) is a special case of the Weighted Sum of Normal Random Variables where \\(a_i=\\frac{1}{n},\\ i=1,2,\\ldots,n\\). It follows then that \\[\\begin{align*} M_{\\bar X}(t) &amp;=exp\\Big(t\\sum\\limits_{i=1}^{n}\\frac{1}{n}\\mu +t\\frac{\\sum\\limits_{i=1}^{n}\\frac{1}{n}\\sigma^2}{2}\\Big) \\\\ &amp;=exp\\big(t\\frac{n\\mu}{n}+\\frac{t\\frac{n\\sigma^2}{n}}{2}\\big) \\\\ &amp;=exp\\big(t\\mu+\\frac{t\\frac{\\sigma^2}{n}}{2}\\big) \\end{align*}\\] Which is the mgf of a Normal random variable with parameters \\(\\mu\\) and \\(\\frac{\\sigma^2}{n}\\). Thus \\(\\bar X\\sim Normal\\big(\\mu,\\ \\frac{\\sigma^2}{n}\\big)\\). 27.6.8 Square of a Standard Normal Random Variable If \\(Z\\sim N(0,1)\\), then \\(Z^2\\sim\\chi^2(1)\\). Proof: \\[\\begin{align*} M_{Z^2}(t) &amp;= E(e^{tZ^2}) \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}e^{tz^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty}e^{tz^2} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(-2t+1)}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[1]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[2]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}e^{-u} \\frac{\\sqrt{2}u^{-\\frac{1}{2}}}{2(1-2t)^{\\frac{1}{2}}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}e^{-u}u^{-\\frac{1}{2}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-u}du \\\\ ^{[3]} &amp;= \\frac{1}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}}\\Gamma(\\frac{1}{2}) \\\\ &amp;= \\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{1}{2}}}=(1-2t)^{-\\frac{1}{2}} \\\\ \\end{align*}\\] \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\) when f(x) is an even function () Let \\(u=\\frac{z^2}{2}(1-2t) \\ \\ \\ \\ \\Rightarrow z=\\frac{\\sqrt{2}u^{\\frac{1}{2}}}{(1-2t)^{\\frac{1}{2}}}\\)     So \\(dz=\\frac{\\sqrt{2}u^{-\\frac{1}{2}}} {2(1-2t)^{\\frac{1}{2}}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) Which is the mgf of a Chi-Square random variable with 1 degree of freedom. Thus \\(Z^2\\sim\\chi^2(1)\\). "],
["poisson-distribution.html", "28 Poisson Distribution 28.1 Probability Mass Function 28.2 Cumulative Mass Function 28.3 Expected Values 28.4 Moment Generating Function 28.5 Maximum Likelihood Estimator 28.6 Theorems for the Poisson Distribution", " 28 Poisson Distribution 28.1 Probability Mass Function A random variable is said to have a Poisson distribution with parameter \\(\\lambda\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} \\frac{\\lambda^xe^{-\\lambda}}{x!}, &amp; x=0,1,2,...\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 28.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} e^{-\\lambda}\\sum\\limits_{i=0}^{x}\\frac{\\lambda^i}{i!}, &amp; x=1,2,3,...\\\\ 0 &amp; otherwise \\end{array}\\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} P(X=x+1) &amp;= \\frac{e^{-\\lambda}\\lambda^{x+1}}{(x+1)!} \\\\ &amp;= \\frac{\\lambda}{x+1}\\frac{e^{-\\lambda}\\lambda^x}{x!} \\\\ &amp;= \\frac{\\lambda}{x+1}P(X=x) \\end{align*}\\] Figure 28.1: The graphs on the left and right show a Poisson probability cumulative distribution function, respectively, for \\(\\lambda=3\\). 28.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^xe^{-\\lambda}}{x!} \\\\ &amp;= e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^x}{x!} \\\\ &amp;= e^{-\\lambda}\\Big(0\\frac{\\lambda^0}{0!}+1\\frac{\\lambda^1}{1!} +2\\frac{\\lambda^2}{2!}+3\\frac{\\lambda^3}{3!}+\\cdots\\Big) \\\\ &amp;= e^{-\\lambda}\\Big(0+\\lambda^1+\\frac{\\lambda^2}{1!}+\\frac{\\lambda^3}{2!}+\\cdots\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}\\Big(\\lambda^0+\\frac{\\lambda^1}{1!}+\\frac{\\lambda^2}{2!}+\\cdots\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}\\Big(\\frac{\\lambda^0}{0!}+\\frac{\\lambda^1}{1!} +\\frac{\\lambda^2}{2!}+\\cdots\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}e^\\lambda \\\\ &amp;= \\lambda e^(-\\lambda+\\lambda) \\\\ &amp;= \\lambda \\end{align*}\\] Taylor Series Expansion: \\(e^x=\\frac{x^0}{0!}+\\frac{x^1}{1!} +\\frac{x^2}{2!}+\\cdots =1+\\frac{x^1}{1!}+\\frac{x^2}{2!}+\\cdots\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^xe^{-\\lambda}}{x!} \\\\ &amp;= e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^x}{x!} \\\\ &amp;= e^{-\\lambda}\\Big(0^2\\frac{\\lambda^0}{0!}+1^2\\frac{\\lambda^1}{1!} +2^2\\frac{\\lambda^2}{2!}+3^2\\frac{\\lambda^3}{3!}+\\cdots\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}\\Big(\\frac{\\lambda^0}{1} +2\\frac{\\lambda^1}{1!}+3\\frac{\\lambda^2}{2!}+\\cdots\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}(x+1)\\frac{\\lambda^x}{x!} \\\\ &amp;= \\lambda e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty} \\Big(x\\frac{\\lambda^x}{x!}+\\frac{\\lambda^x}{x!}\\Big) \\\\ &amp;= \\lambda e^{-\\lambda}\\Big(\\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^x}{x!} +\\sum\\limits_{x=0}^{\\infty}\\frac{\\lambda^x}{x!}\\Big) \\\\ &amp;= \\lambda\\sum\\limits_{x=0}^{\\infty}x\\frac{\\lambda^xe^{-\\lambda}}{x!} +\\lambda\\sum\\limits_{x=0}^{\\infty}\\frac{\\lambda^xe^{-\\lambda}}{x!} \\\\ &amp;= \\lambda E(X)+\\lambda \\\\ &amp;= \\lambda^2+\\lambda\\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\lambda \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X) \\\\ &amp;= \\lambda^2 + \\lambda - \\lambda^2 \\\\ &amp;= \\lambda \\end{align*}\\] 28.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}e^{tx}\\frac{\\lambda^xe^{-\\lambda}}{x!} \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}\\frac{(\\lambda e^{tx})^xe^{-\\lambda}}{x!} \\\\ &amp;= e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}\\frac{(\\lambda e^{tx})^x}{x!} \\\\ &amp;= e^{-\\lambda}\\Big[\\frac{(\\lambda e^t)^0}{0!}+\\frac{(\\lambda e^t)^1}{1!}+ \\frac{(\\lambda e^t)^2}{2!}+\\cdots\\Big] \\\\ &amp;= e^{-\\lambda}e^{\\lambda e^t} \\\\ &amp;= e^{(\\lambda e^t-\\lambda)} \\\\ &amp;= e^{\\lambda(e^t-1)} \\end{align*}\\] Taylor Series Expansion: \\(e^x=\\frac{x^0}{0!}+\\frac{x^1}{1!} +\\frac{x^2}{2!}+\\cdots =1+\\frac{x^1}{1!}+\\frac{x^2}{2!}+\\cdots\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= e^{\\lambda(e^t-1)}\\lambda e^t=\\lambda e^te^{\\lambda(e^t-1)} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= (e^{\\lambda(e^t-1)}(\\lambda e^t)+(e^{\\lambda(e^t-1)}\\lambda e^t)(\\lambda e^t) \\\\ &amp;= \\lambda e^t[e^{\\lambda e^t-1)}+e^{\\lambda(e^t-1)}\\lambda e^t] \\\\ &amp;= \\lambda e^t[e^{\\lambda(e^t-1)}(1+\\lambda e^t)] \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\lambda e^0e^{\\lambda(e^0-1)}\\lambda e^t=\\lambda \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= \\lambda e^0[e^{\\lambda(e^0-1)}(1+\\lambda e^0)] \\\\ &amp;= \\lambda e^0[e^{\\lambda(e^0-1)}(1+\\lambda e^0] \\\\ &amp;= \\lambda(1+\\lambda) \\\\ &amp;= \\lambda+\\lambda^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\lambda \\\\ \\sigma^2 &amp;= E(X^2)-E(X) \\\\ &amp;= \\lambda+\\lambda^2-\\lambda^2 \\\\ &amp;= \\lambda \\end{align*}\\] 28.5 Maximum Likelihood Estimator Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample drawn from a Poisson Distribution with parameter \\(\\lambda\\). 28.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= p(x_1|\\theta)p(x_2|\\theta)\\cdots p(x_n|\\theta) \\\\ &amp;= \\frac{e^{-\\lambda}\\lambda^{x_1}}{x_1!}\\cdot\\frac{e^{-\\lambda}\\lambda^{x_2}}{x_2!} \\cdot \\ \\cdots\\ \\cdot \\frac{e^{-\\lambda}\\lambda^{x_n}}{x_n!} \\\\ &amp;= \\frac{e^{-n\\lambda}\\lambda^{\\sum\\limits_{i=1}^{n}x_i}}{\\prod\\limits_{i=1}^{n}x_1!} \\end{align*}\\] 28.5.2 Log-likelihood \\[\\begin{align*} \\ell(\\lambda) &amp;= \\ln(L(\\lambda)) \\\\ &amp;= \\ln\\Big[\\frac{e^{-n\\lambda}\\lambda^{\\sum\\limits_{i=1}^{n}x_i}}{\\prod\\limits_{i=1}^{n}x_1!}\\Big] \\\\ &amp;= \\ln(e^{-n\\lambda})+\\ln\\Big(\\lambda^{\\sum\\limits_{i=1}^{n}x_i}\\Big) - \\ln\\Big(\\prod\\limits_{i=1}^{n}x_1!\\Big) \\\\ &amp;= -n\\lambda + \\sum\\limits_{i=1}^{n}x_i\\ln\\Big(\\lambda)-\\ln(\\prod\\limits_{i=1}^{n}x_1!\\Big) \\end{align*}\\] 28.5.3 MLE for \\(\\lambda\\) \\[\\begin{align*} \\frac{d\\ell}{d\\lambda} &amp;= -n - \\frac{\\sum\\limits_{i=1}^{n}x_i}{\\lambda} - 0 \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i}{\\lambda} - n \\\\ \\\\ \\\\ 0 &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i}{\\lambda}-n\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}x_i}{\\lambda} &amp;= n\\\\ \\Rightarrow \\sum\\limits_{i=1}^{n}x_i &amp;= n\\lambda\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}x_i}{n} &amp;= \\lambda \\end{align*}\\] so \\(\\displaystyle \\hat\\lambda=\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\) is the Maximum Likelihood Estimator for \\(\\lambda\\). 28.6 Theorems for the Poisson Distribution 28.6.1 Derivation of the Poisson Distribution Suppose \\(X\\) is a Binomial random variable in all respects but has an infinite (non-fixed) number of trials, each with probability of success \\(p\\). Then the pdf of \\(X\\) is \\(P(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) Proof: For an infinite number of trials we take \\(\\lim\\limits_{n\\rightarrow\\infty}{n\\choose x}p^x(1-p)^{n-x}\\). By rewriting \\(p\\) in terms of \\(\\mu\\ (\\mu=np\\ \\Rightarrow p=\\frac{\\mu}{n})\\) we get \\[\\begin{align*} \\lim\\limits_{n\\rightarrow\\infty}{n\\choose x}\\frac{\\mu}{n}^x(1-\\frac{\\mu}{n})^{n-x} &amp;= \\lim\\limits_{n\\rightarrow\\infty}{n\\choose x} \\frac{\\mu}{n}^x(1-\\frac{\\mu}{n})^n(1-\\frac{\\mu}{n})^{-x} \\\\ &amp;= \\lim\\limits_{n\\rightarrow\\infty}\\bigg(\\frac{n(n-1)\\cdots(n-x+1)(n-x)!}{x!(n-x)!}\\bigg) \\mu^x\\frac{1}{n^x}(1-\\frac{\\mu}{n})^n(1-\\frac{\\mu}{n})^{-x} \\\\ &amp;= \\lim\\limits_{n\\rightarrow\\infty}\\bigg(\\frac{n(n-1)\\cdots(n-x+1)}{x!}\\bigg) \\mu^x\\frac{1}{n^x}(1-\\frac{\\mu}{n})^n(1-\\frac{\\mu}{n})^{-x} \\\\ &amp;= \\frac{\\mu^x}{x!}\\lim\\limits_{n\\rightarrow\\infty}\\bigg(\\frac{n(n-1)\\cdots(n-x+1)}{n^x}\\bigg) \\frac{1}{n^x}(1-\\frac{\\mu}{n})^n(1-\\frac{\\mu}{n})^{-x} \\\\ &amp;= \\frac{\\mu^x}{x!}\\lim\\limits_{n\\rightarrow\\infty}\\bigg(\\frac{n(n-1)\\cdots(n-x+1)}{n^x}\\bigg) \\lim\\limits_{n\\rightarrow\\infty}\\frac{1}{n^x}(1-\\frac{\\mu}{n})^n \\lim\\limits_{n\\rightarrow\\infty}(1-\\frac{\\mu}{n})^{-x} \\\\ &amp;= \\frac{\\mu^x}{x!}\\cdot 1\\cdot e^{-\\mu}\\cdot 1 \\\\ &amp;= \\frac{e^{-\\mu}\\mu^x}{x!} \\end{align*}\\] \\(\\lim\\limits_{n\\rightarrow\\infty}(1-\\frac{x}{n})^n=e^-x\\) Traditionally, we use \\(\\lambda\\) in place of \\(\\mu\\) for the Poisson distribution, giving us the desired result. 28.6.2 Validity of the Distribution \\[\\sum\\limits_{x=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^x}{x!} = 1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{\\infty}\\frac{e^{-\\lambda}\\lambda^x}{x!} &amp;= e^{-\\lambda}\\sum\\limits_{x=0}^{\\infty}\\frac{\\lambda^x}{x!} \\\\ &amp;= e^{-\\lambda}\\bigg(\\frac{\\lambda^0}{0!}+ \\frac{\\lambda^1}{1!}+\\frac{\\lambda^2}{2!}+\\cdots\\bigg) \\\\ &amp;= e^{-\\lambda}\\cdot e^\\lambda \\\\ &amp;= e^0 \\\\ &amp;= 1 \\end{align*}\\] Taylor Series Expansion: \\(e^x=\\frac{x^0}{0!}+\\frac{x^1}{1!}+\\frac{x^2}{2!}+\\cdots\\) 28.6.3 Sum of Poisson Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from a Poisson distribution with parameter \\(\\lambda_i,\\ i=1,2,\\ldots,n\\); that is, \\(X_i\\sim\\)Poisson\\((\\lambda_i)\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Poisson\\((\\sum\\limits_{i=1}^{n}\\lambda_i)\\).\\ Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n)} \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= e^{\\lambda_1(e^t-1)}e^{\\lambda_2(e^t-1)}\\cdots e^{\\lambda_n(e^t-1)} \\\\ &amp;= e^{(\\lambda_1+\\lambda_2+\\cdots+\\lambda_n)(e^t-1)} \\\\ &amp;= e^{(e^{t-1})\\sum\\limits_{i=1}^{n}\\lambda_i} \\end{align*}\\] Which is the mgf of a Poisson random variable with parameter \\(\\sum\\limits_{i=1}^{n}\\lambda_i\\). Thus \\(Y\\sim\\)Poisson\\((\\sum\\limits_{i=1}^{n}\\lambda_i)\\). "],
["probability.html", "29 Probability 29.1 Elementary Probability Concepts", " 29 Probability 29.1 Elementary Probability Concepts 29.1.1 Definition of Probability Let \\(S\\) be a sample space associated with an experiment. For every event \\(A \\in S\\) (ie, \\(A\\) is a subset of \\(S\\)), we assign a number, \\(P(A)\\) - called the of \\(A\\) - such that the following three axioms hold: Axiom 1: \\(P(A) \\geq 0\\). Axiom 2: \\(P(S) = 1\\). Axiom 3: If \\(A_1, A_2, A_3, ...\\) form a sequence of pairwise mutually exclusive events in \\(S\\) (that is, \\(A_i \\cap A_j = \\emptyset\\) if \\(i \\neq j\\)), then \\(P(A_1 \\cup A_2 \\cup A_3 \\cup ...) = \\sum\\limits_{i=1}^\\infty P(A_i)\\). 29.1.2 Definition: Conditional Probability The conditional probability of an event \\(A\\), given that an event \\(B\\) has occured and \\(P(B) &gt; 0\\) is equal to \\[ P(A|B) = \\frac{P(A\\cap B)}{P(B)} \\] 29.1.3 Definition: Independence Events \\(A\\) and \\(B\\) are said to be independent if any of the following holds \\[P(A|B) = P(A)\\] \\[P(B|A) = P(B)\\] \\[P(A\\cap B) = P(A)\\cdot P(B)\\] 29.1.4 Theorem: Multiplicative Law of Probability The probability of the intersection of two events \\(A\\) and \\(B\\) is \\[P(A\\cap B) = P(A)\\cdot P(B|A) = P(B)\\cdot P(A|B)\\] Proof: By the definition of conditional probability \\[\\begin{align*} P(A|B) &amp;= \\frac{P(A\\cap B)}{P(B)} \\\\ \\Rightarrow P(A|B) \\cdot P(B) &amp;= P(A \\cap B) \\end{align*}\\] Likewise \\[\\begin{align*} P(B|A) &amp;= \\frac{P(B\\cap A)}{P(A)} \\\\ \\Rightarrow P(B|A) \\cdot P(A) &amp;= P(B \\cap A) \\end{align*}\\] Since \\(P(A \\cap B) = P(B \\cap A)\\) \\[\\begin{align*} P(A|B) \\cdot P(B) &amp;= P(A \\cap B) \\\\ &amp;= P(B \\cap A) \\\\ &amp;= P(B|A) \\cdot P(A) \\\\ \\Rightarrow P(A \\cap B) &amp;= P(A|B) \\cdot P(B) \\\\ &amp;= P(B|A) \\cdot P(A) \\end{align*}\\] 29.1.5 Corollary If \\(A\\) and \\(B\\) are independent, then \\[P(A \\cap B) = P(A) \\cdot P(B)\\] Proof: When \\(A\\) and \\(B\\) are independent, by the definition of independence, \\[\\begin{align*} P(A \\cap B) &amp; = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A) \\\\ \\Rightarrow &amp;= P(A) \\cdot P(B) = P(B) \\cdot P(A) \\end{align*} \\] 29.1.6 Additive Law of Probability The probability of the union of two events is \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Proof: \\(A \\cup B = A \\cup (A^c \\cap B)\\) where \\(A\\) and \\((A^c \\cap B)\\) are mutually exclusive. \\(\\Rightarrow P(A \\cup B) = P(A) + P(A^c \\cap B)\\) \\(B = (A^c \\cap B) \\cup (A \\cap B)\\) where \\((A^c \\cap B)\\) and \\((A \\cap B)\\) are mutually exclusive. \\(\\Rightarrow P(B) = P(A^c \\cap B) + P(A \\cap B)\\\\ \\Rightarrow P(A^c \\cap B) = P(B) - P(A \\cap B)\\) \\(P(A \\cup B) = P(A) + P(A^c \\cap B)\\\\ \\Rightarrow P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) 29.1.7 Corollary If \\(A\\) and \\(B\\) are mutually exclusive events, then \\(P(A \\cup B) = P(A) + P(B)\\). Proof: When \\(A\\) and \\(B\\) are mutually exclusive, \\((A \\cap B) = \\emptyset\\) and \\(P(A \\cap B) = 0\\). By Theorem , \\[\\begin{align*} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ \\Rightarrow P(A \\cup B) &amp;= P(A) + P(B) - 0 \\\\ \\Rightarrow P(A \\cup B) &amp;= P(A) + P(B) \\end{align*}\\] 29.1.8 Theorem: Law of Complements If \\(A\\) is an event, then \\(P(A) = 1 - P(A^c)\\). Proof: Let \\(S\\) be the sample space. \\[\\begin{align*} S &amp;= A \\cup A^c \\\\ \\Rightarrow P(S) &amp;= P(A \\cup A^c) \\\\ \\Rightarrow P(S) &amp;= P(A) + P(A^c) - P(A \\cap A^c) \\\\ \\Rightarrow P(S) &amp;= P(A) + P(A^c) - 0 \\\\ \\Rightarrow P(S) &amp;= P(A) + P(A^c) \\\\ \\Rightarrow 1 &amp;= P(A) + P(A^c) \\\\ \\Rightarrow 1 - P(A^c) &amp;= P(A) \\\\ \\Rightarrow P(A) &amp;= 1 - P(A^c) \\end{align*}\\] 29.1.9 Definition: Partition of a Sample Space For some positive integer \\(k\\), let the sets \\(B_1, B_2, \\ldots, B_k\\) be such that \\(S = B_1 \\cup B_2 \\cup \\ldots \\cup B_k\\). \\(B_i \\cap B_j = \\emptyset\\) for \\(i \\neq j\\). Then the collection of sets \\({B_1, B_2, \\ldots, B_k}\\) is said to be a partition of \\(S\\). 29.1.10 Definition: Decomposition If \\(A\\) is any subset of \\(S\\) and \\({B_1, B_2, \\ldots, B_k}\\) is a partition of \\(S\\), \\(A\\) can be decomposed as follows: \\(A = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_k)\\) 29.1.11 Theorem: Total Law of Probability If \\({B_1, B_2, \\ldots, B_k}\\) is a partition of \\(S\\) such that \\(P(B_i) &gt; 0\\), for \\(i = 1, 2, \\ldots, k\\), then for any event \\(A\\) \\[P(A) = \\sum\\limits_{i=1}^k P(A|B_i)P(B_i)\\] Proof: Any subset \\(A\\) of \\(S\\) can be written as \\[A = A \\cap S = A \\cap (B_1 \\cup B_2 \\cup \\cdots \\cup B_k) = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\cdots \\cup (A \\cap B_k)\\] Since \\({B_1, B_2, \\ldots, B_k}\\) is a partition of \\(S\\), if \\(i \\neq j\\), \\((A \\cap B_i) \\cup (A \\cap B_j) = A \\cap (B_i \\cap B_j) = A \\cap \\emptyset = \\emptyset\\). That is, \\((A \\cap B_i)\\) and \\((A \\cap B_j)\\) are mutually exclusive events. Thus, \\[\\begin{align*} P(A) &amp;= P(A \\cap B_1) + P(A \\cap B_2) + \\cdots + P(A \\cap B_k) \\\\ ^{[1]} \\Rightarrow &amp;= P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_k)P(B_k) \\\\ \\Rightarrow &amp;= \\sum\\limits_{i=1}^k P(A|B_i)P(B_i) \\end{align*}\\] Theorem : Multiplicative Law of Probability 29.1.12 Theorem: Bayes’ Rule If \\({B_1, B_2, \\ldots, B_k}\\) is a partition of \\(S\\) such that \\(P(B_i) &gt; 0\\), for \\(i = 1, 2, \\ldots, k\\), then \\[ P(B_j|A) = \\frac{P(A|B_j)P(B_j)}{\\sum\\limits_{i=1}^k P(A|B_i)P(B_i)} \\] Proof: \\[\\begin{align*} \\ ^{[1]} P(B_j|A) &amp;= \\frac{P(A \\cap B)}{P(A)} \\\\ ^{[2]} \\Rightarrow &amp;= \\frac{P(A|B_j)P(B_j)}{P(A)} \\\\ ^{[3]} \\Rightarrow &amp;= \\frac{P(A|B_j)P(B_j)}{\\sum\\limits_{i=1}^k P(A|B_i)P(B_i)} \\end{align*}\\] Definition , Conditional Probability Definition , Conditional Probability Theorem , Law of Total Probability "],
["real-number-system.html", "30 Real Number System 30.1 Historical Note 30.2 The Field of Real Numbers 30.3 Proof that the Field of Rationals is not Complete 30.4 Preliminary Results in the Field of Real Numbers", " 30 Real Number System This chapter was prepared by Steve MacDonald (McDonald 2003–2007). 30.1 Historical Note The first axiom system known in the history of mathematics was Euclid’s aximoatic development of plane geometry. Euclid’s treatment began with some primitive or undefined terms and some assumed statements, which he called axioms and postulates. For Euclid an axiom was a general “self-evident” truth, such as “The whole is greater than any of its parts” or “equals added to equals are equal,” whereas a postulate was an assumed statement about the relationships among the primitives of his system, such as “Two points determine exactly one line.” From these undefined terms and basic assumptions a whole body of other statements, called theorems was deduced. For centuries Euclidean Geometry, which was assumed to be the true description of physical reality, remained the only mathematical systems with such an axiomatic foundation. Then in the nineteenth century, spurred by Lobatchevsky and others who discovered that by modifying the postulates another logically consistent geometry could be constructed, matematicians began to apply this deductive approach to other branches of mathematics. Not only did this work do much to organize and clarify such familiar disciplines as number theory, analysis, and algebra, but it helped develop new areas of mathematics such as topology. Note that today we do not make Euclid’s distinction between axiom and postulate, using the terms synonymously. As an example of this deductive approach, we now want to give an axiomatic description of the real numbers system and thus place a logical foundation under many of the “rules” you learned in high school algebra. 30.2 The Field of Real Numbers 30.2.1 Definition: The Field of Real Numbers The Field of Real Numbers is a set \\(\\Re\\) of objects called numbers together with two well-defined binary operations, called addition, denoted by +, and multiplication, denoted by \\(\\cdot\\) or juxtaposition, satisfying the Field Axioms. (By well-defined, we mean that if \\(s=s^\\prime\\) and \\(t=t^\\prime\\), then \\(s+t=s^\\prime + t^\\prime\\).) 30.2.2 Field Axioms (Closure for addition) For each pair \\(x,y,\\in\\Re\\), there exists a unique object in \\(\\Re\\), called the sum of \\(x\\) and \\(y\\) denoted by \\(x+y\\). (Associative law for addition) For all \\(x,y,z\\in\\Re,\\ (x+y)=z=x+(y+z)\\). (Additive identity) There exists and object \\(0\\in\\Re\\) such that for all \\(x\\in\\Re,\\ x+0=x=0+x\\). (Additive inverse) For each \\(x\\in\\Re\\), there exists some \\(y\\in\\Re\\) such that \\(x+y=0=y+x\\). We will usually denote the additive inverse of \\(x\\) by \\(-x\\). (Commutative law of addition) For all \\(x,y\\in\\Re,\\ x+y=y+x\\). (Closure for multiplication) For each pair \\(,y,\\in\\Re\\), there exists a unique object in \\(\\Re\\), called the product of \\(x\\) and \\(y\\) and denoted by \\(x\\cdot y\\) or \\(xy\\). (Associative law of multiplication) For all \\(x,y,z\\in\\Re,\\ x\\cdot(y\\cdot z)=(x\\cdot y)\\cdot z\\). (Multiplicative identity) There exists an object \\(1\\in\\Re\\) such that for all \\(x\\in\\Re,\\ 1\\cdot x=x=x\\cdot 1\\). (Multiplicative inverse) For each \\(x\\in\\Re\\) such that \\(x\\neq 0\\), there exists an object \\(y\\in\\Re\\) such that \\(x\\cdot y=1=y\\cdot x\\). We will usually denote the multiplicative inverse of \\(x\\) by \\(x^{-1}\\). (Commutative law of multiplication) For all \\(x,y\\in\\Re,\\ x\\cdot y=y\\cdot x\\). (Distributive law of multiplication over addition) For all \\(x,y,z\\in\\Re, x\\cdot(y+z)=x\\cdot y+x\\cdot z\\). (Positive Elements) There exists a nonempty subset \\(\\Re^+\\subset\\Re\\) closed under \\(+\\) and \\(\\cdot\\). That is, for all \\(x,y\\in\\Re^+,\\ x+y\\in\\Re\\) and \\(x\\cdot y\\in\\Re^+\\). (Trichotomy) For any \\(x\\in\\Re\\), exactly one of these three cases holds: \\(x\\in\\Re^+,\\ -x\\in\\Re^+\\), or \\(x=0\\). 30.2.3 Definiton: Less Than (or Equal To) Let \\(x,y\\in\\Re\\). We say that \\(x\\) is less than \\(y\\), written \\(x&lt;y\\), provided \\(y+-x\\in\\Re^+\\). We say that \\(x\\) is less than or equal \\(y\\) iff and only iff \\(x&lt;y\\) or \\(x=y\\). (Also, \\(x\\) is said to be greater than \\(y\\) if \\(y&lt;x\\).) 30.2.4 Definition: Bounded Above (and Below) A set \\(A\\) of real numbers is said to be bounded above if there exists some \\(b\\in\\Re\\) such that \\(x\\leq b,\\ \\forall x\\in A\\). In this case, \\(b\\) is called an upper bound for \\(A\\). (Bounded below and lower bound are defined similarly.) 30.2.5 Definition: Least Upper (and Lower) Bound Let \\(A\\) be a set of real numbers bounded above. An element \\(\\beta\\in\\Re\\) is called the least upper bound, often written lub for \\(A\\) iff and only if \\(\\beta\\) is an upper bound for \\(A\\) and \\(\\beta\\leq b\\) for every \\(b\\) which is an upper bound for \\(A\\). (A greatest lower bound is defined similarly.) 30.2.6 Completeness Axiom Every nonempty subset of \\(\\Re\\) having an upper bound has a least upper bound. 30.3 Proof that the Field of Rationals is not Complete Let \\(A=\\left\\{p\\in Q^+|p^2&lt;2\\right\\}\\) and \\(B=\\left\\{p\\in Q^+|p^2&gt;2\\right\\}\\). We claim that \\(A\\) has no largest element and that \\(B\\) has no smallest element; i.e., given that \\(p\\in A\\), we can find some \\(q\\in A\\) with \\(q&gt;p\\); and given any \\(p\\in B\\), we can find some \\(q\\in B\\) with \\(q&lt;p\\). For any \\(p\\in Q^+\\), let \\[\\begin{align*} q &amp;=p-\\frac{p^2-2}{p+2} \\\\ &amp;=\\frac{p^2+2p-p^2+2}{p+2} \\\\ &amp;=\\frac{2p+2}{p+2} \\end{align*}\\] Then \\[\\begin{align*} q^2-2 &amp;=\\frac{4p^2+8p+r-2p^2-8p-8}{(p+2)^2} &amp;=\\frac{2p^2-4}{(p+2)^2} &amp;=\\frac{2(p+2)}{(p+2)^2} \\end{align*}\\] Now if \\(p\\in A,\\ p^2-2&lt;0\\), so \\(-\\frac{p^2-2}{p+2}&gt;0\\), whence \\(q=p-\\frac{p^2-2}{p+2}&gt;p\\). But \\(q^2-2=\\frac{2(p^2-2)}{(p+2)^2}&lt;0\\) implies that \\(q^2&lt;2\\). Thus \\(q\\in A\\) and \\(q&gt;p\\). On the other hand, if \\(p\\in B\\), then \\(p^2-2&gt;0\\), so \\(q=p-\\frac{p^2-2}{p+2}&lt;p\\). But \\(q^2-2=\\frac{2(p^2-2)}{(p+2)^2}&gt;0\\) implies that \\(q\\in B\\). Here \\(q\\in B\\) and \\(q&lt;p\\). Now it is clear that every member of \\(B\\) is an upper bound for \\(A\\), and every member of \\(A\\) is a lower bound for set \\(B\\). It is also clear from the above demonstration that among the rationals Q, the nonempty set \\(A\\) has no least upper bound; and the nonempty set \\(B\\) has no greatest lower bound among the rationals . Therefore we have shown that the ordered field of rational numbers does not satisfy the conditions of the Completeness Axiom . Thus it is the Completeness Axiom that distinguishes the ordered field of real numbers from the ordered field of rational numbers. 30.4 Preliminary Results in the Field of Real Numbers 30.4.1 Theorem: Uniqueness of Identities Identity elements are unique. Proof: Suppose \\(u \\star a=a\\star u=a\\) and \\(e\\star a=a\\star e=a,\\ \\forall a\\in\\Re\\). Then \\(u=u\\star e=e\\). 30.4.2 Theorem 2: Uniqueness of Inverses If \\(\\star\\) is an associative operation, inverse elements for \\(\\star\\) are unique. Proof: Suppose \\(a\\star a_1=a_1\\star a=e\\) and \\(a\\star a_2=a_2\\star a=e\\), where \\(e\\) is the identity element for the operation \\(\\star\\). Then \\(a_1=a_1\\star e=a_1\\star(a\\star a_2)=(a_1\\star a)\\star a_2)=e\\star a_2=a_2\\). 30.4.3 Theorem: Left Cancellation Law If \\(a\\) has an inverse \\(a^\\prime\\) with respect to the associative operation \\(\\star\\), and \\(a\\star b=a\\star c\\), then \\(b=c\\). Proof: Suppose \\(a\\star b=a\\star c\\). Then \\[\\begin{align*} a^\\prime\\star(a\\star b) &amp;= a^\\prime\\star(a\\star c)\\\\ \\Rightarrow(a^\\prime\\star a)\\star b &amp;= (a^\\prime\\star a)\\star c\\\\ \\Rightarrow e\\star b &amp;= e\\star c\\\\ \\Rightarrow b &amp;= c \\end{align*}\\] 30.4.4 Corollary: Right Cancellation In the field \\((\\Re,+,\\cdot),\\ a+b=a+c\\Rightarrow b=c\\), and if \\(a\\neq 0\\) and \\(ab=ac\\), then \\(b=c\\). Proof: The Corollary is proved using commutativity and Left Cancellation. 30.4.5 Lemma \\[-0=0\\]. Proof: By axiom 4 \\(-0+0=0\\). Because 0 is the additive identity, \\(-0+0=-0\\). Therefore \\(-0=-0+0=0\\). 30.4.6 Theorem \\(\\forall a\\in\\Re,\\ a&gt;0\\) if and only if \\(a\\in\\Re^+\\). Proof: Suppose \\(0&lt;a\\). Then \\(a-0\\in\\Re^+\\), but \\(a-0=a\\), so \\(a\\in\\Re^+\\). Conversely, suppose \\(a\\in\\Re^+\\). Then \\(a-0=a\\in\\Re^+\\), so \\(0&lt;a\\). 30.4.7 Theorem \\(\\forall x\\in\\Re,\\ x\\cdot 0=0\\). Proof: \\[\\begin{align*} 0 + x\\cdot 0 &amp;= x\\cdot 0\\\\ &amp;= x\\cdot(0+0)\\\\ &amp;= x\\cdot 0 + x\\cdot 0\\\\ Rightarrow 0 &amp;= x\\cdot 0 \\end{align*}\\] 30.4.8 Theorem \\(\\forall x\\in\\Re,\\ -(-x)=x\\). Proof: \\(-(-x)+(-x) = 0\\) and \\(0 = x+(-x)\\). So \\[\\begin{align*} -(-x)+(-x) &amp;= x+(-x) \\Rightarrow -(-x) &amp;= x \\end{align*}\\] 30.4.9 Theorem \\[\\forall x,y\\in\\Re,\\ x\\cdot(-y) = -(x\\cdot y)=(-x)\\cdot y\\] Proof: \\[\\begin{align*} x(-y)+xy &amp;= x(-y+y) &amp;= x\\cdot 0 \\\\ &amp;= 0 \\\\ &amp;= -(xy)+xy\\\\ \\Rightarrow x(-y)+xy &amp;= -(xy)+xy\\\\ \\Rightarrow x(-y) &amp;= -(xy) \\end{align*}\\] Similarly, \\[\\begin{align*} (-x)y+xy &amp;= (-x+x)y \\\\ &amp;= 0\\cdot y \\\\ &amp;= 0 \\\\ &amp;= -(xy)+xy\\\\ \\Rightarrow (-x)y &amp;= -(xy) \\end{align*}\\] By transitivity, \\((-x)y = -(xy) = x(-y)\\). 30.4.10 Theorem \\[\\forall x\\in\\Re,\\ (-1)\\cdot x=-x\\] Proof: By Theorem 30.4.9 \\[\\begin{align*} (-1)x &amp;= -(1x) &amp;= -x \\end{align*}\\] 30.4.11 Corollary \\[\\forall x\\in\\Re,\\ (-1)\\cdot(-x)=x\\] Proof: By 30.4.10 \\[(-1)(-x) = -(-x) = x \\] 30.4.12 Theorem \\[\\forall x\\in\\Re,\\ (-x)(-x)=x\\cdot x\\] Proof: By Corollary 30.4.11 \\[\\begin{align*} (-x)(-x) &amp;= -(x(-x)) \\\\ &amp;= -((-x)x) \\\\ &amp;= -(-(x\\cdot x)) \\\\ &amp;= (x\\cdot x) \\\\ &amp;= x\\cdot x \\end{align*}\\] 30.4.13 Theorem Let \\(x\\) and \\(y\\) be any real numbers. Then exactly one of the following is true: \\(x&gt;y\\) \\(x=y\\) \\(x&lt;y\\) Proof: By Axiom 1, \\(\\Re\\) is closed under addition. Thus, since \\(x,y\\in\\Re\\) , \\(x+(-y)\\in\\Re\\). By Trichotomy, \\(x+(-y)\\in\\Re,\\ -(x+(-y))\\in\\Re\\), or \\(x+(-y)=0\\). \\(x+(-y)\\in\\Re\\ \\Rightarrow x&gt;y\\) (Definition 30.2.3) \\(-(x+(-y))=(-x)+y\\in\\Re\\ \\Rightarrow x&lt;y\\) (Axiom 4) \\(x+(-y)=0\\ \\Rightarrow x=y\\) (Definition 30.2.3). 30.4.14 Theorem \\[\\forall a,b,c\\in\\Re,\\ (a&lt;b \\wedge b&lt;c)\\Rightarrow a&lt;c\\] Proof: \\[\\begin{align*} a&lt;b &amp;\\wedge b&lt;c \\\\ \\Rightarrow b-a&gt;0 &amp;\\wedge c-b&gt;0 \\\\ \\Rightarrow^{[1]}(b-a)+(c-b)&gt;0 \\\\ \\Rightarrow (c-b)+(b-a)&gt;0 \\\\ \\Rightarrow c-b+b-a&gt;0 \\\\ \\Rightarrow c-a&gt;0 \\\\ \\Rightarrow a&lt;c \\end{align*}\\] Axiom 12 30.4.15 Theorem \\(\\forall a,b,c \\in\\Re,\\ (a&lt;b \\ \\Rightarrow a+c&lt;b+c)\\). Proof: \\[\\begin{align*} a &amp;&lt; b \\\\ \\Rightarrow b-a &amp;&gt; 0 \\\\ \\Rightarrow b-c+c-a &amp;&gt; 0 \\\\ \\Rightarrow b+c-a-c &amp;&gt; 0 \\\\ \\Rightarrow (b+c)-(a+c) &amp;&gt; 0 \\\\ \\Rightarrow a+c &amp;&lt; b+c\\ \\end{align*}\\] 30.4.16 Theorem \\(\\forall a,b,c\\in\\Re,\\ (a&lt;b\\wedge c&gt;0)\\Rightarrow ac&lt;bc\\).\\ Proof: \\[\\begin{align*} a &amp;&lt; b \\\\ \\Rightarrow b-a &amp;&gt; 0 \\\\ ^{[1]} \\Rightarrow c(b-a) &amp;&gt; 0 \\\\ \\Rightarrow bc-ac &amp;&gt; 0 \\\\ \\Rightarrow ac &amp;&lt; bc \\end{align*}\\] Axiom 12 states that \\(\\Re^+\\) is closed under \\(\\cdot\\). 30.4.17 Theorem \\(\\forall a,b,c\\in\\Re,\\ (a&lt;b\\wedge c&lt;0)\\Rightarrow bc&lt;ba\\). Proof: \\[\\begin{align*} a &amp;&lt; b \\\\ \\Rightarrow b-a &amp;&gt; 0 \\\\ \\Rightarrow c(b-a) &amp;&lt; 0 \\\\ \\Rightarrow bc-ba &amp;&lt; 0 \\\\ \\Rightarrow bc &amp;&lt; ba \\end{align*}\\] Theorem . 30.4.18 Theorem If \\(a&gt;b&gt;0\\) and \\(c&gt;d&gt;0\\), then \\(ac&gt;bd\\). Proof: \\[\\begin{align*} a&gt;b &amp;\\wedge c&gt;d \\\\ \\Rightarrow a-b&gt;0 &amp;\\wedge c-d&gt;0 \\\\ \\Rightarrow c(a-b)&gt;0 &amp;\\wedge b(c-d)&gt;0 \\\\ \\Rightarrow ac-bc&gt;0 &amp;\\wedge bc-bd&gt;0 \\\\ \\Rightarrow ac-bc+bc-bd&gt;0 &amp; \\\\ \\Rightarrow ac-bd&gt;0 &amp; \\\\ \\Rightarrow ac&gt;bd &amp; \\end{align*}\\] 30.4.19 Theorem In a field containing at least two elements, \\(1\\in\\Re^+\\). Proof: Since in a field of more than one element, \\(1\\neq 0\\) (Theorem ) we may assume that 1 is not 0. Hence, by trichotomy, either \\(1\\in\\Re^+\\) or \\(-1\\in\\Re^+\\). Suppose that \\(-1\\in\\Re^+\\). Then by closure \\((-1)(-1)\\in\\Re^+\\), but by Corollary , \\((-1)(-1)=1\\), so we now have both \\(-1\\in\\Re^+\\) and \\(1\\in\\Re^+\\), which is a contradiction, and hence it is impossible that \\(-1\\in\\Re^+\\). Therefore, \\(1\\in\\Re^+\\). 30.4.20 Theorem If \\(x&gt;0\\), then \\(x^{-1}&gt;0\\). Proof: Since \\(x^{-1}\\) has an inverse \\(x\\), we know that \\(x^{-1}\\neq 0\\). Hence, by Axiom 13, either \\(x^{-1}&gt;0\\) or \\(x^{-1}&lt;0\\). Suppose \\(x^{-1}&lt;0\\). Then \\(-x^{-1}\\in\\Re^+\\), and since \\(\\Re^+\\) is closed under multiplication, \\((-x^{-1})\\cdot x\\in\\Re^+\\). Now by Theorem , \\((-x^{-1})\\cdot x=-(x^{-1}\\cdot x)=-1\\), so this would imply that \\(-1\\in\\Re^+\\), in contradction to Theorem . Since \\(x^{-1}&lt;0\\) must be be false, we conclude that \\(x^{-1}&gt;0\\) whenever \\(x&gt;0\\). References "],
["rounding.html", "31 Rounding 31.1 Floor (Next Lowest Integer) 31.2 Ceiling 31.3 Nearest Integer 31.4 Nearest Multiple 31.5 Nearest Place (Base 10) 31.6 Breaking Ties 31.7 References", " 31 Rounding 31.1 Floor (Next Lowest Integer) Rounding to the next lower integer, denoted \\(\\lfloor x \\rfloor\\), is defined as \\[\\lfloor x \\rfloor = max\\{m \\in \\mathbb{Z} | m \\leq x\\}\\] 31.2 Ceiling Rounding to the next larger integer, denoted \\(\\lceil x \\rceil\\), is defined as \\[\\lceil x \\rceil = min\\{m \\in \\mathbb{Z} | m \\geq z\\}\\] 31.3 Nearest Integer Rounding to the nearest integer is a common operation without a notation with well established concensus. It may be represented as \\(\\lbrack x \\rbrack\\), \\(\\lfloor x \\rceil\\), \\(||x||\\), \\(nint(x)\\) or \\(round(x)\\). Here, we will use the \\(nint(x)\\) notation so that we may extend the \\(round(x)\\) notation beyond just rounding to the nearest integer.. A mathematical representation of rounding would be really nice to have right here 31.4 Nearest Multiple Rounding to a nearest multiple may be obtained through a rescaling of the value \\(x\\) into a integer scale based on multiples of \\(m\\). One available notation for this operation is \\(mround(x, m)\\). \\[mround(x, m) = nint \\Big( \\frac{x}{m} \\Big) \\cdot m\\] 31.5 Nearest Place (Base 10) It is common in mathematical operations to round a value not to an integer, but to a decimal place. This is no different than rounding to a multiple of ten. We define the following conventions. Let \\(x \\in \\mathbb{R}\\) Let \\(p \\in \\mathbb{Z}\\) where \\(p\\) represents the negative power of ten of the desired precision of the result. Thus, when \\(p = 0\\), we with to round to the \\(10 ^ {-0} = 10^0 = 1\\), or ones/integer position. When \\(p = 1\\), we round to the \\(10 ^ {-1} = \\frac{1 / 10} = 0.01\\), or tenths position. And when \\(p = -1\\), we round to the \\(10 ^ 1 = 10\\), or tens position. We define the operation \\(round(x, p)\\) to be the operation of rounding to the nearest decimal place. \\[\\begin{aligned} round(x, p) &amp;= mround(x, 10 ^ {-p}) \\\\ &amp;= round \\Big( \\frac{x}{10 ^ {-p}}, 0 \\Big) \\cdot 10 ^ {-p} \\\\ &amp;= \\frac{round(x \\cdot 10 ^ p, 0)}{10 ^ p} \\end{aligned}\\] Under this definition, \\(nint(x)\\) is a special case of \\(round\\) where \\(nint(x) = round(x, 0)\\). 31.6 Breaking Ties 31.6.1 Rounding Even 31.6.2 Rounding Odd 31.6.3 Round Away From Zero 31.6.4 Round Toward Zero 31.7 References (I really need to be better than this) https://en.wikipedia.org/wiki/Floor_and_ceiling_functions https://en.wikipedia.org/wiki/Nearest_integer_function "],
["sample-size-estimation.html", "32 Sample Size Estimation 32.1 Solving Group Sample Sizes Using Weights", " 32 Sample Size Estimation 32.1 Solving Group Sample Sizes Using Weights Let \\(n\\) be the total sample size obtained by adding two groups such that \\(n = n_1 + n_2\\). Let \\(w\\) represent the proportion of \\(n\\) allocated to \\(n_1\\), referred to as the weight of \\(n_1\\). Then \\(n_2 = \\frac{n_1 \\cdot (1 - w)}{w}\\). Furthermore, \\(n = n_1 + \\frac{n_1 \\cdot (1-w)}{w}\\). Proof: By the assumptions, we know \\[\\begin{align*} n &amp;= n_1 + n_2 \\\\ &amp;= w \\cdot n + (1 - w) \\cdot n \\end{align*}\\] This implies \\[\\begin{align*} n_1 &amp;= w \\cdot n\\\\ n_2 &amp;= (1-w) \\cdot n \\end{align*}\\] We observe the following: \\[\\begin{align*} \\frac{n_2}{n_1} &amp;= \\frac{(1-w) \\cdot n}{w \\cdot n} \\\\ &amp;= \\frac{(1-w)}{w} \\\\ \\Rightarrow n_2 &amp;= \\frac{n_1 \\cdot (1-w)}{w} \\end{align*}\\] This further implies \\[ n = n_1 + \\frac{n_1 \\cdot (1-w)}{w}\\] Notice now that both \\(n\\) and \\(n_2\\) are defined as functions of \\(n_1\\) and \\(w\\). Thus, we may estimate the sample size required in each of two groups by estimating only \\(n_1\\), provided we know the weight \\(w\\). 32.1.1 Corollary For \\(k \\in \\mathbb{N}\\), let \\(n\\) be the total sample size of \\(k\\) subgroups such that \\[n = n_1 + n_2 + n_3 + ... + n_k\\] Suppose, further, that there exists a vector of weights \\(W\\) that satisfy the following conditions: For each \\(w_i \\in W\\), \\(0 \\leq w_i \\leq 1\\) \\(\\sum\\limits_{i=1}^{k} w_i = 1\\) \\(n = w_1 \\cdot n + w_2 \\cdot n + ... + w_k \\cdot n = \\sum\\limits_{i=1}^{k} w_i \\cdot n\\) Let us assign the values \\(n_1 = w_1 \\cdot n\\), \\(n_2 = w_2 \\cdot n\\), …, \\(n_k = w_k \\cdot n\\). Then for all \\(i | i \\leq k\\), \\[\\begin{align*} \\frac{n_i}{n_1} &amp;= \\frac{w_i \\cdot n}{w_1 \\cdot n} \\\\ &amp;= \\frac{w_i}{w_1} \\\\ \\Rightarrow n_i &amp;= \\frac{n_1 \\cdot w_i}{w_1} \\end{align*}\\] Thus, each \\(n_i\\) may be estimated by estimating the value of \\(n_1\\) provided \\(W\\) is a fully specified vector of weights for each of the \\(k\\) groups. "],
["skew-normal-distribution.html", "33 Skew-Normal Distribution 33.1 Preliminary Theorems 33.2 Lemma: A Symmetry Theorem 33.3 Lemma 33.4 Expected Values 33.5 Estimation of \\(\\lambda\\)", " 33 Skew-Normal Distribution 33.1 Preliminary Theorems 33.2 Lemma: A Symmetry Theorem Suppose the pdf of \\(X\\), \\(f_X\\) is symmetric about 0. Let \\(w(\\cdot)\\) be any odd function. Then the pdf of \\(Y=w(X)\\), \\(f_Y\\), is also symmetric about 0. Proof: Recall that if a pdf is symmetric about zero, it must demonstrate the property \\(P(T\\leq t)=P(T\\geq-t)\\). Since \\(f_X\\) is symmetric, we know \\[\\begin{align*} P(X\\leq x) &amp;= P(X\\geq-x) \\\\ \\Rightarrow P\\big[ w(X)\\leq w(x)\\big] &amp;= P\\big[ w(X)\\geq w(-x)\\big] ^{[1]} &amp;= P\\big[ w(X)\\geq-w(x)\\big] \\\\ &amp;= P(Y\\leq y) \\\\ &amp;= P(Y\\geq-y) \\end{align*}\\] Thus \\(f_Y\\) is symmetric about 0. By the definition of an odd function \\(f(-x)=-f(x)\\). 33.3 Lemma Let \\(f_0\\) be a one-dimensional probability density function symmetric over 0. Also, let \\(G\\) be a one dimensional probability distribution function such that \\(G^\\prime\\) exists and is a density function symmetric over 0. Then \\[ \\begin{array}{rl} f(x) = 2 f_0(x) G\\big(w(x)\\big) &amp; (-\\infty&lt;x&lt;\\infty)\\\\ \\end{array} \\] is a probability density function for any odd fuction \\(w(\\cdot)\\).\\ Proof: Let \\(X\\sim f_0\\) and \\(Y\\sim G^\\prime\\). Now consider the random variable \\(X-w(Y)|Y\\). When \\(Y\\) is fixed, \\(X-w(Y)\\) is an odd function of \\(X\\) and, by Lemma , \\(X-w(Y)\\) is symmetric over 0. Thus, \\[\\begin{align*} \\frac{1}{2} &amp;= P\\big(X-w(Y)\\leq0|Y\\big) \\\\ ^{[1]} &amp;= E\\Big[ P\\big(X-w(Y)\\leq0|Y\\big) \\Big] \\\\ \\Rightarrow \\frac{1}{2} &amp;= E\\big[ P\\big( X\\leq w(Y)|Y\\big) \\big]\\\\ \\Rightarrow \\frac{1}{2} &amp;= \\int\\limits_{-\\infty}^{\\infty} P\\big(X \\leq w(Y)|Y\\big) p(x) dx\\\\ \\Rightarrow \\frac{1}{2} &amp;= \\int\\limits_{-\\infty}^{\\infty} G\\big( w(Y) \\big) f_0(x)dx\\\\ \\Rightarrow 1 &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} f_0(X) G\\big( w(Y) \\big) dx \\end{align*}\\] So \\(f(x) = 2 f_0(x)G\\big(w(y)\\big)\\) is a valid density function for all \\(x,\\ x\\in\\Re\\). The expected value of \\(P(X\\leq 0)=\\frac{1}{2}\\) when \\(X\\) is distributed symmetric over 0. 33.4 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{-\\infty}^{\\infty} x \\cdot 2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\Bigg[ \\int\\limits_{-\\infty}^{\\alpha x} \\frac{1}{\\sqrt{s\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}dt \\Bigg]dx \\\\ &amp;= 2\\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{2}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\end{align*}\\] But \\(\\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}\\) cannot be integrated in closed form, so the solution must be found with numerical methods. \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{-\\infty}^{\\infty} x^2 \\cdot 2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x^2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\Bigg[ \\int\\limits_{-\\infty}^{\\alpha x} \\frac{1}{\\sqrt{s\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}dt \\Bigg]dx \\\\ &amp;= 2\\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{2}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\end{align*}\\] But \\(\\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}\\) cannot be integrated in closed form, so the solution must be found with numerical methods. 33.5 Estimation of \\(\\lambda\\) Using the Moment Generating Function, it was shown that the skew of the Skew-Normal distribution can be calculated as \\[ S=sign(\\lambda) \\bigg(2 - \\frac{\\pi}{2}\\bigg) \\Bigg(\\frac{\\lambda^2} {\\frac{pi}{2}+(\\frac{pi}{2}-1)\\lambda^2}\\Bigg)^{\\frac{3}{2}} \\] where \\(S\\) denotes the skew of the distribution. Given a value of skew for the distribution, a link can be made back to \\(\\lambda\\). We begin by noticing that the following process is identical regardless of the sign of \\(\\lambda\\). It is presented here as if \\(\\lambda&gt;0\\) \\[\\begin{align*} S &amp;= \\bigg( 2-\\frac{\\pi}{2} \\bigg) \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg)^{3/2} \\\\ \\Rightarrow \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg) &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg)^{3/2} \\\\ \\Rightarrow \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3} &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg) \\\\ \\Rightarrow T &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg) \\\\ \\Rightarrow \\bigg( \\frac{\\pi}{2} + \\Big(\\frac{\\pi}{2}-1\\Big) \\lambda^2 \\Bigg)T &amp;= \\lambda^2 \\\\ \\Rightarrow \\frac{\\pi}{2}T + \\Big( \\frac{\\pi}{2}-1 \\Big) \\lambda^2 T &amp;= \\lambda^2 \\\\ \\Rightarrow \\frac{\\pi}{2}T &amp;= \\lambda^2 - \\Big( \\frac{\\pi}{2}-1 \\Big) \\lambda^2 T \\\\ \\Rightarrow \\frac{\\pi}{2}T &amp;= \\lambda^2 \\bigg(1 - \\Big( \\frac{\\pi}{2}-1 \\Big) T \\bigg) \\\\ \\Rightarrow \\lambda^2 &amp;= \\frac{ \\frac{\\pi}{2}T } { 1-\\Big( \\frac{\\pi}{2}-1 \\Big) T } \\\\ \\Rightarrow \\lambda^2 &amp;= \\frac{ \\frac{\\pi}{2} \\Big( \\frac{S}{2-\\frac{\\pi}{2}} \\Big)^{2/3} } { 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3}\\\\ } \\\\ \\Rightarrow \\lambda &amp;= \\frac{ \\sqrt{\\frac{\\pi}{2}} \\Big( \\frac{S}{2-\\frac{\\pi}{2}} \\Big)^{1/3} } { \\sqrt{ 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3}} } \\end{align*}\\] Let \\(T=\\big( \\frac{S}{2-\\frac{\\pi}{2}} \\big)^{2/3}\\) This equation is only defined for certain values of \\(S\\). In particular, \\(S\\) cannot be a number such that the denominator is 0, nor can the that which appears under the radical be negative. These two restrictions can be collapsed, and the equation is defined so long as \\[\\begin{align*} 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\bigg(\\frac{S}{2-\\frac{\\pi}{2}}\\bigg)^{2/3} &gt; 0 \\\\ \\Rightarrow 1 &gt; \\Big( \\frac{\\pi}{2}-1 \\Big) \\bigg(\\frac{S}{2-\\frac{\\pi}{2}}\\bigg)^{2/3} \\\\ \\Rightarrow \\Big( \\frac{\\pi}{2}-1 \\Big)^{-1} &gt; \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3} \\\\ \\Rightarrow \\frac{ \\Big( 2-\\frac{\\pi}{2} \\Big)^{2/3} } { \\frac{\\pi}{2}-1 } &gt; S^{2/3} \\\\ \\Rightarrow \\frac{ 2-\\frac{\\pi}{2} } { \\Big( \\frac{\\pi}{2}-1 \\Big)^{3/2} } &gt; S \\\\ \\Rightarrow -.9952 &lt; S &lt; .9952 \\end{align*}\\] We notice that the endpoints of this interval are approximations. Ideally, the interval would span from -1 to 1, as most estimators of skew provide a value in that interval–values close to negative one denoting a strong left skew; values close to one denoting a strong right skew; 0 denoting perfect symmetry. Although this relationship is not perfect, it is quite close to what we would like, and can be practically implemented. "],
["somers-d.html", "34 Somers’ D 34.1 Theorems for Somers’ \\(D\\)", " 34 Somers’ D Somers’ \\(D\\) has an asymptotically \\(Normal\\) Distribution . It may take any value between -1 and 1. It is used to measure classification agreement between a predictor and outcome variable. Somers’ \\(D\\) is related to a form of a concordance index. Concrodance in measured between 0 and 1 and can effectively be calculated by rescaling Somers’ \\(D\\). The rescaling can be accomplished by: \\[ C = \\frac{D+1}{2} \\] 34.1 Theorems for Somers’ \\(D\\) 34.1.1 Theorem: Distribution of Somers’ Derived Concordance Let \\(D \\sim\\) Normal\\((\\mu, \\sigma^2)\\). Then $C \\(Normal\\)(,)$. Proof: \\[ D \\sim Normal(\\mu,\\sigma^2) \\Rightarrow (D+1) \\sim Normal(\\mu+1,\\sigma^2) \\\\ (D+1) \\sim Normal(\\mu+1,\\sigma^2) \\Rightarrow \\frac{D+1}{2} \\sim Normal(\\frac{\\mu+1}{2}, \\frac{\\sigma^2}{4}) \\] By definition, \\(C=\\frac{D+1}{2}\\), so \\(C \\sim Normal(\\frac{\\mu+1}{2},\\frac{\\sigma^2}{4})\\). Note: when the dependent variable is a binary response, the Concordance Index is equal to the area under the Receiver Operator Characteristic (ROC) curve, or AUC. "],
["summation-chapter.html", "35 Summation 35.1 Theorems of Summation", " 35 Summation 35.1 Theorems of Summation 35.1.1 Theorem If \\(c\\) is a constant then \\[\\sum\\limits_{i=1}^{n}c = nc\\] Proof: \\[ \\sum\\limits_{i=1}^{n}c = \\underbrace{c+c+\\cdots+c}_{n\\ \\rm terms} = nc \\] 35.1.2 Theorem If \\(a_1,a_2,\\ldots,a_n\\) are real numbers and \\(c\\) is a constant, then \\[ \\sum\\limits_{i=1}^{n}ca_i = c\\sum\\limits_{i=1}^{n}a_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}ca_i &amp;= ca_1 + ca_2 + \\cdots + ca_n \\\\ &amp;= c(a_1+a_2+\\cdots+a_n) \\\\ &amp;= c\\sum\\limits_{i=1}^{n}a_i \\end{align*}\\] 35.1.3 Theorem If \\(a_1,_2,\\ldots,a_n\\) are real numbers and \\(b_1,b_2,\\ldots,b_n\\) are real numbers, then \\[ \\sum\\limits_{i=1}^{n}(a_i+b_i) = \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}(a_i+b_i) &amp;= a_1 + b_1 + a_2 + b_2 + \\cdots + a_n + b_n \\\\ &amp;= a_1 + a_2 + \\cdots + a_n + b_1 + b_2 + \\cdots + b_n \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\end{align*}\\] 35.1.4 Theorem If \\(a_i\\) and \\(b_j\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j = a_{+} b_{+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_j\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i b_{+} \\\\ &amp;= b_{+} \\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= a_{+} b_{+} \\end{align*}\\] 35.1.5 Theorem If \\(a_i\\) is a real number for \\(i=1,2,\\ldots,n\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b = m a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b &amp;= \\sum\\limits_{i=1}^{n}m a_i b \\\\ &amp;= m b\\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= m a_{+} b \\end{align*}\\] 35.1.6 Theorem If \\(a_j\\) is a real number for \\(j=1,2,\\ldots,m\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b = n a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b &amp;= \\sum\\limits_{i=1}^{n}\\bigg( b \\sum\\limits_{j=1}^{m} a_j \\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_{+}b \\\\ &amp;= n a_{+} b \\end{align*}\\] 35.1.7 Theorem If \\(a_i\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} = \\sum\\limits_{i=1}^{n}a_ib_{i+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_{ij}\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_ib_{i+} \\end{align*}\\] 35.1.8 Theorem If \\(a_j\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[\\ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} = \\sum\\limits_{i=1}^{n}a_jb_{+ j} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} &amp;= a_1b_{11}+a_2b_{12}+\\cdots+a_mb_{1m} \\\\ &amp; \\ \\ \\ \\ +a_1b_{21}+a_2b_{22}+\\cdots+a_mb_{2m} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_1b_{n1}+a_1b_{n1}+\\cdots+a_1b_{nm} \\\\ &amp;= a_1b_{11}+a_1b_{21}+\\cdots+a_1b_{n1} \\\\ &amp; \\ \\ \\ \\ +a_2b_{12}+a_2b_{22}+\\cdots+a_2b_{n2} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_mb_{1m}+a_mb_{2m}+\\cdots+a_nb_{nm} \\\\ &amp;= a_1(b_{11}+b_{21}+\\cdots+b_{n1}) \\\\ &amp; \\ \\ \\ \\ +a_2(b_{12}+b_{22}+\\cdots+b_{n2}) \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_m(b_{1m}+b_{2m}+\\cdots+b_{nm}) \\\\ &amp;= a_1b_{+ 1}+a_2b_{+ 2}+\\cdots+a_mb_{+ m} \\\\ &amp;=\\sum\\limits_{j=1}^{m}a_jb_{+ j} \\end{align*}\\] "],
["method-of-transformations.html", "36 Method of Transformations 36.1 Example: Cauchy Distribution", " 36 Method of Transformations Suppose we wish to find the distribution funciton for the random variable \\(Y\\) that is either a strictly increasing or strictly decreasing function (Such a function is sure to have an inverse, whereas a function like \\(Y=X^2\\) does not have an inverse).. If we know the distribution of \\(X\\), we may use the following to determine the cdf of \\(Y\\). \\[\\begin{align*} P(Y\\leq y) &amp;= P(h(X)\\leq y) \\\\ &amp;= P(h^{-1}(h(y))\\leq h^{-1}(x)) \\\\ &amp;= P(Y\\leq h^{-1}(x)) \\\\ \\Rightarrow F_Y(y) &amp;= F_X(h^{-1}(x)) \\end{align*}\\] The pdf can now be found by taking the deriviative of the cdf. \\[\\begin{align*} f_Y(y) &amp;= \\frac{d(F_Y(y))}{d y} \\\\ &amp;= \\frac{d F_Y(h^{-1}(y))}{d y} \\\\ &amp;= f_X(h^{-1}(y))\\frac{d(h^{-1}(y))}{dy} \\end{align*}\\] 36.1 Example: Cauchy Distribution Let \\(X\\) have the Uniform pdf \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{\\pi}, &amp; \\frac{-\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\\\ 0 &amp; otherwise \\end{array}\\right. \\] Let \\(Y = \\tan (X)\\). The pdf of \\(Y\\) can be found as follows: \\[\\begin{align*} h(x) &amp;= \\tan (x) \\\\ \\Rightarrow h^{-1}(x) &amp;= \\tan^{-1}(x) \\\\ \\Rightarrow \\frac{d h^{-1}(x)}{d x} &amp;= \\frac{1}{1+x^2} \\\\ \\Rightarrow f_Y(y) &amp;= f_X(h^{-1}(y))\\frac{d(h^{-1}(y))}{d y} \\\\ &amp;= f_X(tan^{-1}(x))\\frac{1}{1+y^2} \\\\ &amp;= \\frac{1}{\\pi}\\frac{1}{1+y^2} \\\\ &amp;= \\frac{1}{\\pi(1+y^2)} \\end{align*}\\] The domain of \\(Y\\) is transformed \\[ \\frac{-\\pi}{2} &lt; x &lt; \\frac{-\\pi}{2} \\\\ \\Rightarrow \\tan\\big(\\frac{-\\pi}{2}\\big) &lt; \\tan(x) &lt; \\tan\\big(\\frac{-\\pi}{2}\\big) \\\\ \\Rightarrow -\\infty &lt; y &lt; \\infty \\] Thus the pdf of the \\(Y\\), known as the Cauchy distribution, is \\[f_Y(y) = \\frac{1}{\\pi(1+y^2)},\\ \\ -\\infty&lt;y&lt;\\infty \\] "],
["t-test.html", "37 T-test 37.1 One-Sample T-test 37.2 Two-Sample T-test 37.3 One Sided Test 37.4 References", " 37 T-test 37.1 One-Sample T-test The t-test is commonly used to look for evidence that the mean of a normally distributed random variable may differ from a hypothesized (or previously observed) value. 37.1.1 T-Statistic The \\(t\\)-statistic is a standardized measure of the magnitude of difference between a sample’s mean and some known, non-random constant. It is similar to a \\(z\\)-statistic, but differs in that a \\(t\\)-statistic may be calculated without knowledge of the population variance. 37.1.2 Definitions and Terminology Let \\(\\bar{x}\\) be a sample mean from a sample with standard deviation \\(s\\). Let \\(\\mu_0\\) be a constant, and \\(s_\\bar{x} = s/\\sqrt{n}\\) be the standard error of the parameter \\(\\bar{x}\\). \\(t\\) is defined: \\[t = \\frac{\\bar{x} - \\mu_0}{s_\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\] and has \\(\\nu = n-1\\) degrees of freedom. 37.1.3 Hypotheses The hypotheses for these test take the forms: For a two-sided test: \\[ \\begin{align*} H_0: \\mu &amp;= \\mu_0\\\\ H_a: \\mu &amp;\\neq \\mu_0 \\end{align*} \\] For a one-sided test: \\[ \\begin{align*} H_0: \\mu &amp;&lt; \\mu_0\\\\ H_a: \\mu &amp;\\geq \\mu_0 \\end{align*} \\] or \\[ \\begin{align*} H_0: \\mu &amp;&gt; \\mu_0\\\\ H_a: \\mu &amp;\\leq \\mu_0 \\end{align*} \\] To compare a sample \\((X_1, \\ldots, X_n)\\) against the hypothesized value, a T-statistic is calculated in the form: \\[T = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\] Where \\(\\bar{x}\\) is the sample mean and \\(s\\) is the sample standard deviation. 37.1.4 Decision Rule The decision to reject a null hypothesis is made when an observed T-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of shaded areas on both sides equaling the corresponding \\(\\alpha\\). It follows, then, that the decision rule is: Reject \\(H_0\\) when \\(T \\leq t_{\\alpha/2, \\nu}\\) or when \\(T \\geq t_{1-\\alpha/2, \\nu}\\). By taking advantage of the symmetry of the T-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha/2, \\nu}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 37.1: The example displayed uses 25 degrees of freedom In the one-sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(T \\leq t_{\\alpha, \\nu}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(T \\geq t_{1-\\alpha, \\nu}\\). Using the symmetry of the T-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha, \\nu}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 37.2: The example displayed uses 25 degrees of freedom The decision rule can also be written in terms of \\(\\bar{x}\\): Reject \\(H_0\\) when \\(\\bar{x} \\leq \\mu_0 - t_\\alpha \\cdot s/\\sqrt{n}\\) or \\(\\bar{x} \\geq \\mu_0 + t_\\alpha \\cdot s/\\sqrt{n}\\). This change can be justified by: \\[ \\begin{align*} |T| &amp;\\geq t_{1-\\alpha, \\nu}\\\\ \\Big|\\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\Big| &amp;\\geq t_{1-\\alpha, \\nu} \\end{align*} \\] \\[ \\begin{align*} -\\Big(\\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\Big) &amp;\\geq t_{1-\\alpha, \\nu} &amp; \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} &amp;\\geq t_{1-\\alpha, \\nu}\\\\ \\bar{x} - \\mu_0 &amp;\\leq - t_{1-\\alpha, \\nu} \\cdot s/\\sqrt{n} &amp; \\bar{x} - \\mu_0 &amp;\\geq t_{1-\\alpha, \\nu} \\cdot s/\\sqrt{n}\\\\ \\bar{x} &amp;\\leq \\mu_0 - t_{1-\\alpha, \\nu} \\cdot s/\\sqrt{n} &amp; \\bar{x} &amp;\\geq \\mu_0 + t_{1-\\alpha, \\nu} \\cdot s/\\sqrt{n} \\end{align*} \\] For a two-sided test, both the conditions apply. The left side condition is used for a left-tailed test, and the right side condition for a right-tailed test. 37.1.5 Power The derivations below make use of the following symbols: \\(\\bar{x}\\): The sample mean \\(s\\): The sample standard deviation \\(n\\): The sample size \\(\\mu_0\\): The value of population mean under the null hypothesis \\(\\mu_a\\): The value of the population mean under the alternative hypothesis. \\(\\alpha\\): The significance level \\(\\gamma(\\mu)\\): The power of the test for the parameter \\(\\mu\\). \\(t_{\\alpha, \\nu}\\): A quantile of the central t-distribution for a probability, \\(\\alpha\\) and \\(n-1\\) degrees of freedom. \\(T\\): A calculated value to be compared against a t-distribution. \\(C\\): The critical region (rejection region) of the test. Two-Sided Test \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_\\mu\\big(\\bar{x} \\leq \\mu_0 - t_{\\alpha/2, \\nu} \\cdot s/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + t_{1-\\alpha/2, \\nu} \\cdot s/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\leq \\mu_0 - \\mu_a - t_{\\alpha/2, \\nu} \\cdot s/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + t_{1-\\alpha/2, \\nu} \\cdot s/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} \\leq \\frac{\\mu_0 - \\mu_a - t_{\\alpha/2, \\nu} \\cdot s/\\sqrt{n}}{s/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + t_{1-\\alpha/2, \\nu} \\cdot s/\\sqrt{n}}{s/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\leq \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}} - t_{\\alpha/2, \\nu}\\Big) + P_{\\mu_a}\\Big(T \\geq \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}} + t_{1-\\alpha/2, \\nu}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\leq -t_{\\alpha/2, \\nu} + \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(T \\geq t_{1-\\alpha/2, \\nu} + \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\leq -t_{\\alpha/2, \\nu} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{s}\\Big) + P_{\\mu_a}\\Big(T \\geq t_{1-\\alpha/2, \\nu} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{s}\\Big) \\end{align*} \\] Both \\(t_{\\alpha/2, \\nu}\\) and \\(t_{1-\\alpha/2, \\nu}\\) have non-central T-distributions with non-centrality parameter \\(\\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{s}\\). One-Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha}\\). Thus, where \\(T\\) occurs in the derivation below, it may reasonably be replaced with \\(|T|\\). \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + t_{1-\\alpha, \\nu} \\cdot s / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + t_{1-\\alpha, \\nu} \\cdot s / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu_a}{s/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + t_{1-\\alpha, \\nu} \\cdot s / \\sqrt{n}}{s / \\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\geq \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}} + t_{1-\\alpha, \\nu} \\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\geq t_{1-\\alpha, \\nu} + \\frac{\\mu_0 - \\mu_a}{s/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(T \\geq t_{1-\\alpha, \\nu} + \\frac{\\sqrt{n} \\cdot (\\mu_0 -\\mu_a)}{s}\\Big) \\end{align*} \\] Where \\(t_{1-\\alpha, \\nu} + \\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{s}\\) has a non-central t-distribution with non-centrality parameter \\(\\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{s}\\) 37.1.6 Confidence Interval The confidence interval for \\(\\theta\\) is written: \\[\\bar{x} \\pm t_{1-\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = t_{1-\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\\] 37.2 Two-Sample T-test The two sample t-test is commonly used to look for evidence that the mean of one normally distributed random variable may differ from that of another normally distributed random variable. The hypotheses for this test take the forms: 37.2.1 T-Statistic The \\(t\\)-statistic is a standardize measure of the magnitude of difference between two sample means and some known, non-random difference of population means. It is similar to a two sample \\(z\\)-statistic, but differes in that a \\(t\\)-statistic may be calculated without knowledge of the population variances. 37.2.2 Definitions and Terminology Let \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) be sample means from two independent samples with standard deviations \\(s_1\\) and \\(s_2\\). Let \\(\\mu_1\\) and \\(\\mu_2\\) be constants representing the means of the populations from which \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) obtained. \\(t\\) is defined: \\[ t = \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\] Where \\[ SE^* = \\left\\{ \\begin{array}{rl} \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}, &amp; \\sigma_1^2 \\neq \\sigma_2^2 \\\\ \\sqrt{\\frac{(n_1-1) \\cdot s_1^2 + (n_2-1) \\cdot s_2^2} {n_1 + n_2 - 2}} \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, &amp; \\sigma_1^2 = \\sigma_2^2 \\end{array} \\right. \\] and the degrees of freedom \\(\\nu\\) are (by the Welch-Satterthwaite equation) \\[ \\nu = \\left\\{ \\begin{array}{rl} \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2} {\\frac{s_1^2/n_1}{n_1 - 1} + \\frac{s_2^2/n_2}{n_2 - 1}}, &amp; \\sigma_1^2 \\neq \\sigma_2^2 \\\\ n_1 + n_2 - 2, &amp; \\sigma_1^2 = \\sigma_2^2 \\end{array} \\right. \\] 37.2.3 Hypotheses For a two-sided test: \\[H_0 : \\mu_1 = \\mu_2 \\\\ H_a : \\mu_1 \\neq \\mu_2 \\] For a one-sided test: \\[ H_0 : \\mu_1 \\leq \\mu_2 \\\\ H_a : \\mu_1 &gt; \\mu_2 \\] or \\[ H_0 : \\mu_1 \\geq \\mu_1 \\\\ H_a : \\mu_1 &lt; \\mu_1 \\] 37.2.4 Decision Rule The decision to reject a null hypothesis is made when an observed T-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. 37.2.4.1 Two Sided Test In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of the shaded areas on both sides equally the corresponding \\(\\alpha\\). It follows then that the decision rule is: Reject \\(H_0\\) when \\(T \\leq t_{\\alpha/2, \\nu}\\) or when \\(T \\geq t_{1 - \\alpha/2, \\nu}\\). By taking advantage of the symmetry of the T-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha/2, \\nu}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 37.3: The example displayed uses 25 degrees of freedom 37.3 One Sided Test In the one sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follow, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(T \\leq t_{\\alpha, \\nu}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(T \\geq t_{1-\\alpha, \\nu}\\). Using the symmetry of the \\(T\\)-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha, \\nu}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 37.4: The example displayed uses 25 degrees of freedom The decision rule can also be written in terms of \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\). Reject \\(H_0\\) when \\(\\bar{x_1} - \\bar{x_2} \\leq (\\mu_1 - \\mu_2) - t_{\\alpha, \\nu} \\cdot SE^*\\) or \\(\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{\\alpha, \\nu} \\cdot SE^*\\) This change can be justified by: \\[\\begin{aligned} |T| &amp; \\geq t_{1 - \\alpha, \\nu} \\\\ \\Big| \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\Big | &amp; \\geq t_{1 - \\alpha, \\nu} \\end{aligned}\\] \\[ \\begin{aligned} -\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq t_{1-\\alpha, \\nu} &amp; \\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq t_{1-\\alpha, \\nu} \\\\ (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp; \\leq -t_{1 - \\alpha, \\nu} \\cdot SE^* &amp; (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp;\\geq t_{1 - \\alpha, \\nu} \\cdot SE^* \\\\ \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) - t_{1-\\alpha, \\nu} \\cdot SE^* &amp; \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) + t_{1-\\alpha, \\nu} \\cdot SE^* \\end{aligned}\\] 37.3.1 Power Two Sided Test \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) \\leq (\\mu_1 - \\mu_2) - t_{\\alpha/2, \\nu} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{1-\\alpha/2, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\leq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - t_{\\alpha/2, \\nu} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha/2, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - t_{\\alpha/2, \\nu} \\cdot SE^*}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha/2, \\nu} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} - t_{\\alpha/2, \\nu}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + t_{1-\\alpha/2, \\nu}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\leq -t_{\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] Both \\(-t_{\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) and \\(t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) have non-central T-distributions with non-centrality parameter \\(\\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\). One Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha}\\). Thus, where \\(T\\) occurs in the derivation below, it may reasonably be replaced with \\(|T|\\). \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{1-\\alpha, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha, \\nu} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + t_{1-\\alpha, \\nu}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq t_{1-\\alpha, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] \\(t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) has a non-central T-distribution with non-centrality parameter \\(\\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\). 37.3.2 Confidence Interval The confidence interval for \\(\\mu_1 - \\mu_2\\) is written: \\[(\\bar{x_1} - \\bar{x_2}) \\pm t_{1-\\alpha/2} \\cdot SE^*\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = t_{1-\\alpha/2} \\cdot SE^*\\] 37.4 References Wackerly, Mendenhall, Scheaffer, Mathematical Statistics with Applications, 6th ed., Duxbury, 2002, ISBN 0-534-37741-6. Daniel, Biostatistics, 8th ed., John Wiley &amp; Sons, Inc., 2005, ISBN: 0-471-45654-3. Hogg, McKean, Craig, Introduction to Mathematical Statistics, 6th ed., Pearson, 2005, ISBN: 0-13-008507-3 Wikipedia, “Student’s T test”, https://en.wikipedia.org/wiki/Student%27s_t-test Wikipedia, “Welch-Satterthwaite Equation”, https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation "],
["uniform-distribution.html", "38 Uniform Distribution 38.1 Probability Density Function 38.2 Cumulative Density Function 38.3 Expected Values 38.4 Moment Generating Function 38.5 Theorems for the Uniform Distribution 38.6 Validity of the Distribution", " 38 Uniform Distribution 38.1 Probability Density Function A random variable \\(X\\) is said to have a Uniform Distribution with parameters \\(a\\) and \\(b\\) if its pdf is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{b-a}, &amp; a\\leq x \\leq b\\\\ 0 &amp; elsewhere \\end{array} \\right. \\] 38.2 Cumulative Density Function \\[\\begin{align*} F(x) &amp;= \\int\\limits_{a}^{x}\\frac{1}{b-a}dt \\\\ &amp;= \\frac{t}{b-a}|_{a}^{x} \\\\ &amp;= \\frac{x}{b-a}-\\frac{a}{b-a} \\\\ &amp;= \\frac{x-a}{b-a} \\end{align*}\\] \\[F(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;a\\\\ \\frac{x-a}{b-a},&amp; a\\leq x\\leq b\\\\ 1 &amp; elsewhere \\end{array}\\right. \\] Figure 38.1: The figures on the left and right display the Uniform probability and cumulative distirubtion functions, respectively, for \\(a=0, b=5\\). 38.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{a}^{b}x\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}x\\ dx \\\\ &amp;= \\frac{1}{b-a}\\cdot \\Big[\\frac{x^2}{2}\\Big]_a^b \\\\ &amp;= \\frac{1}{b-a}\\cdot\\Big[\\frac{b^2}{2}-\\frac{a^2}{2}\\Big] \\\\ &amp;= \\frac{1}{b-a}\\cdot \\frac{b^2-a^2}{2} \\\\ &amp;= \\frac{b^2-a^2}{2(b-a)} \\\\ &amp;= \\frac{(b-a)(b+a)}{2(b-a)} \\\\ &amp;= \\frac{b+a}{2} \\\\ \\\\ \\\\ E(X^2) &amp;= \\int\\limits_{a}^{b}x^2\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}x^2\\ \\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{x^3}{3}\\Big]_a^b \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{b^3-a^3}{3}\\Big] \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{(b-a)(b^2+ab+a^2)}{3}\\Big] \\\\ &amp;= \\frac{(b-a)(b^2+ab+a^2)}{3(b-a)} \\\\ &amp;= \\frac{(b^2+ab+a^2)}{3} \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\frac{b+a}{1} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\frac{b^2+ab+a^2}{3} - \\frac{(b-a)^2}{4} \\\\ &amp;= \\frac{4(b^2+ab+a^2)-3(b+a)^2}{12} \\\\ &amp;= \\frac{4(b^2+ab+a^2-3(b^2+2ab+a^2)}{12} \\\\ &amp;= \\frac{4b^2+4ab+4a^2-3b^2-6ab-3a^2)}{12} \\\\ &amp;= \\frac{4b^2-3b^2+4ab-6ab+4a^2-3a^2}{12} \\\\ &amp;= \\frac{b^2-2a+a^2}{12}=\\frac{(b-a)^2}{12} \\end{align*}\\] 38.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX})=\\int\\limits_{a}^{b}e^{tx}\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}e^{tx}dx \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{e^{tb}-e^{ta}}{t}\\Big] \\\\ &amp;= \\frac{e^{t(b-a)}}{t(b-a)} \\end{align*}\\] \\(M_X^{(k)}(0)\\) will lead to an undefined operation (division by 0). Thus, in the case of the Uniform distribution, we are unable to use the method of moments to identify parameter values. 38.5 Theorems for the Uniform Distribution 38.6 Validity of the Distribution \\[\\int\\limits_{a}^{b}\\frac{1}{b-a} = 1\\] Proof: \\[\\begin{align*} \\int\\limits_{a}^{b}\\frac{1}{b-a} &amp;= \\frac{x}{b-a}\\Big|_a^b \\\\ &amp;= \\frac{b}{b-a}-\\frac{a}{b-a} \\\\ &amp;= \\frac{b-a}{b-a} \\\\ &amp;= 1 \\end{align*}\\] "],
["variance-parameter.html", "39 Variance Parameter 39.1 Defining Variance With Expected Values 39.2 Unbiased Estimator 39.3 Computational Formulae", " 39 Variance Parameter 39.1 Defining Variance With Expected Values In the case of a discrete random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\sum\\limits_{x=0}^{\\infty}(x-\\mu)^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2-2\\mu x+\\mu^2)p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2p(x)-2\\mu x\\cdot p(x)+\\mu^2p(x)) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\sum\\limits_{x=0}^{\\infty}2\\mu x\\cdot p(x) + \\sum\\limits_{x=0}^{\\infty}\\mu^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\sum\\limits_{x=0}^{\\infty}x\\cdot p(x) + \\mu^2\\sum\\limits_{x=0}^{\\infty}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2\\\\ \\end{align*}\\] In the case of a continuous random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}(x-\\mu)^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2-2\\mu x+\\mu^2)f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2f(x)-2\\mu x\\cdot f(x)+\\mu^2f(x))dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\int\\limits_{-\\infty}^{\\infty}2\\mu x\\cdot f(x)dx + \\int\\limits_{-\\infty}^{\\infty}\\mu^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\int\\limits_{-\\infty}^{\\infty}x\\cdot f(x)dx + \\mu^2\\int\\limits_{-\\infty}^{\\infty}f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2 \\end{align*}\\] In general, these results may be summarized as follows: \\[\\begin{align*} \\sigma^2 &amp;= E[(X-\\mu)^2] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)] \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align*}\\] 39.2 Unbiased Estimator \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg) &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{n(\\sum\\limits_{i=1}^{n}x_i)^2}{n^2}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2)-nE(\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big]\\\\\\\\ &amp;= \\frac{1}{n}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;=\\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n}\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Since \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg)\\neq\\sigma^2\\) it is a biased estimator. Notice, however, that the bias can be eliminated by dividing by \\(n-1\\) instead of by \\(n\\) \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg) &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - 2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{(\\sum\\limits_{i=1}^{n}x_i)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{n\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n^2} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-nE(\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2)-nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big] \\\\ &amp;= \\frac{1}{n-1}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;= \\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n-1}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n-1}\\sigma^2 \\\\ &amp;=\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Thus \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg)\\) is an unbiased estimator of \\(\\sigma^2\\), and we define the estimator \\[s^2= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\] 39.3 Computational Formulae 39.3.1 Computational Formula for \\(\\sigma\\)^2 \\[\\begin{align*} \\sigma^2 &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i^2-2\\mu x_i+\\mu^2)}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-\\sum\\limits_{i=1}^{N}2\\mu x_i + \\sum\\limits_{i=1}^{N}\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\mu\\sum\\limits_{i=1}^{N}x_i+N\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2 -2\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\sum\\limits_{i=1}^{N}x_i + N\\Big(\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\Big)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N} + \\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] 39.3.2 Computational Formula for \\(s\\)^2 \\[\\begin{align*} s^2 &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\sum\\limits_{i=1}^{n}2\\bar x x_i + \\sum\\limits_{i=1}^{n}\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\bar x\\sum\\limits_{i=1}^{n}x_i+n\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n}x_i + n\\Big(\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\Big)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n} + \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] "],
["weibull-distribution.html", "40 Weibull Distribution 40.1 Probability Distribution Function 40.2 Cumulative Distribution Function 40.3 Expected Values 40.4 Theorems for the Weibull Distribution", " 40 Weibull Distribution 40.1 Probability Distribution Function A random variable \\(X\\) is said to have a Weibull Distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its probability density function is: \\[f(x)=\\left\\{ \\begin{array}{ll} \\alpha\\beta x^{\\beta-1}e^{-\\alpha x^{\\beta}},&amp;0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 40.2 Cumulative Distribution Function \\[\\begin{align*} \\int\\limits_{0}^{x}\\alpha\\beta t^{\\beta-1}e^{-\\alpha t^\\beta}dt &amp;= \\alpha\\beta\\int\\limits_{0}^{x}t^{\\beta-1}e^{-\\alpha t^\\beta}dt \\\\ &amp;= \\alpha\\beta\\Big[\\frac{-1}{\\alpha\\beta}e^{-\\alpha t^\\beta}\\Big]_0^x \\\\ &amp;= \\alpha\\beta\\Big[\\frac{-1}{\\alpha\\beta}e^{-\\alpha x^\\beta} + \\frac{1}{\\alpha\\beta}\\Big] \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\big(-e^{-\\alpha x^\\beta}+1\\big) \\\\ &amp;= 1-e^{-\\alpha x^\\beta} \\end{align*}\\] Using this result, we can write the Cumulative Distribution Function as \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{-\\alpha x^\\beta},&amp; 0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 40.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^{\\beta}}dx \\\\ &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x x^{\\beta-1}e^{-\\alpha x^{\\beta}}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty} \\Big(\\big(\\frac{y}{\\alpha}\\big)^\\frac{1}{\\beta}\\Big)^\\beta e^{-y}\\frac{1}{\\alpha\\beta} \\Big(\\frac{y}{\\alpha}\\Big)^{\\frac{1}{\\beta}-1}dy \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty} \\Big(\\frac{y}{\\alpha}\\Big)^\\frac{\\beta+1}{\\beta} \\Big(\\frac{y}{\\alpha}\\Big)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\Big(\\frac{y}{\\alpha}\\Big) ^{\\frac{\\beta+1}{\\beta}-\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\Big(\\frac{y}{\\alpha}\\Big) ^{\\frac{\\beta+1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\frac{1}{\\alpha^{\\frac{\\beta+1}{\\beta}-\\frac{\\beta}{\\beta}}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+1}{\\beta}-1}e^{-y} \\\\ &amp;= \\alpha^{-\\frac{1}{\\beta}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+1}{\\beta}-1}e^{-y} \\\\ ^{[2]} &amp;= \\alpha^{-\\frac{1}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big) \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x=(\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx=\\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx =\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^2x^{\\beta-1}e^{-\\alpha x^\\beta}dx \\\\ &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^{\\beta+1}e^{-\\alpha x^\\beta}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}\\Big(\\big(\\frac{y}{\\alpha}\\big)^\\frac{1}{\\beta}\\Big)^{\\beta+1} e^{-y}\\frac{1}{\\alpha\\beta}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}dy \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty}\\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{\\beta+1}{\\beta}} \\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{\\beta+1}{\\beta}+\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\frac{1}{\\alpha^{\\frac{\\beta+2}{\\beta}-\\frac{\\beta}{\\beta}}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+2}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+2}{\\beta}-1}e^{-y}dy \\\\ ^{[2]} = \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x=(\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx=\\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx =\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\alpha^{-\\frac{1}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) - \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)^2 \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\Big[\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) - \\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)^2\\Big] \\end{align*}\\] 40.4 Theorems for the Weibull Distribution 40.4.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^\\beta}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^\\beta}dx &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^{\\beta-1}e^{-\\alpha x^\\beta}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}\\Big(\\big(\\frac{y}{\\alpha}\\big)^ \\frac{1}{\\beta}\\Big)^{\\beta-1} e^{-y}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}\\frac{1}{\\alpha\\beta}dy &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty}\\big(\\frac{y}{\\alpha}\\big)^\\frac{\\beta}{-1} \\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{\\beta-1}{\\beta}+\\frac{1-\\beta}{\\beta}}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\frac{y^0}{\\alpha^0}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}y^{1-1}e^{-y}dy \\\\ ^{[2]} &amp;= \\Gamma(1)=1 \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x = (\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx = \\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx = \\Gamma(\\alpha)\\) "],
["z-test-of-sample-proportions.html", "41 Z-test of Sample Proportions 41.1 One-Sample Z-test 41.2 References", " 41 Z-test of Sample Proportions 41.1 One-Sample Z-test The \\(z\\)-test of proportions is one approach used to look for evidence that the proportion of a sample may differ from a hypothesized (or previously observed) value. It assumes a normal distribution approximation to a Binomial distribution. 41.1.1 Z-Statistic The \\(z\\)-statistic is a standardized measure of the magnitude of difference between a sample’s proportion and some known, non-random constant. 41.1.2 Definitions and Terminology Let \\(p\\) be a sample proportion from a sample. Let \\(\\pi_0\\) be a constant. \\(z\\) is defined: \\[z = \\frac{p - \\pi_0}{\\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}} \\] 41.1.3 Hypotheses The hypotheses for these test take the forms: For a two-sided test: \\[ \\begin{align*} H_0: \\pi &amp;= \\pi_0\\\\ H_a: \\pi &amp;\\neq \\pi_0 \\end{align*} \\] For a one-sided test: \\[ \\begin{align*} H_0: \\pi &amp;&lt; \\pi_0\\\\ H_a: \\pi &amp;\\geq \\pi_0 \\end{align*} \\] or \\[ \\begin{align*} H_0: \\pi &amp;&gt; \\pi_0\\\\ H_a: \\pi &amp;\\leq \\pi_0 \\end{align*} \\] To compare a sample \\((X_1, \\ldots, X_n)\\) against the hypothesized value, a Z-statistic is calculated in the form: \\[Z = \\frac{p - \\pi_0}{\\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}\\] Where \\(p\\) is the sample proportion. 41.1.4 Decision Rule The decision to reject a null hypothesis is made when an observed Z-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of shaded areas on both sides equaling the corresponding \\(\\alpha\\). It follows, then, that the decision rule is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha/2}\\) or when \\(Z \\geq z_{1-\\alpha/2}\\). By taking advantage of the symmetry of the Z-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha/2}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 41.1: Rejection regions for the Z-test of proportions In the one-sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha, \\nu}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(Z \\geq z_{1-\\alpha, \\nu}\\). Using the symmetry of the Z-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 41.2: Rejection regions for one-tailed Z-test The decision rule can also be written in terms of \\(p\\): Reject \\(H_0\\) when \\(p \\leq \\pi_0 - z_\\alpha \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\) or \\(p \\geq \\pi_0 + z_\\alpha \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\). This change can be justified by: \\[ \\begin{align*} |Z| &amp;\\geq z_{1-\\alpha}\\\\ \\Big|\\frac{p - \\pi_0}{\\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}\\Big| &amp;\\geq z_{1-\\alpha} \\end{align*} \\] \\[ \\begin{align*} -\\Big(\\frac{p - \\pi_0}{\\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}\\Big) &amp;\\geq z_{1-\\alpha} &amp; \\frac{p - \\pi_0}{\\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}} &amp;\\geq z_{1-\\alpha}\\\\ p - \\pi_0 &amp;\\leq - z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}} &amp; p - \\pi_0 &amp;\\geq z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\\\ p &amp;\\leq \\pi_0 - z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}} &amp; p &amp;\\geq \\pi_0 + z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}} \\end{align*} \\] For a two-sided test, both the conditions apply. The left side condition is used for a left-tailed test, and the right side condition for a right-tailed test. 41.1.5 Power The derivations below make use of the following symbols: \\(p\\): The sample proportion \\(n\\): The sample size \\(\\pi_0\\): The value of population mean under the null hypothesis \\(\\pi_a\\): The value of the population mean under the alternative hypothesis. \\(\\alpha\\): The significance level \\(\\gamma(\\mu)\\): The power of the test for the parameter \\(\\mu\\). \\(z_{\\alpha}\\): A quantile of the Standard Normal distribution for a probability, \\(\\alpha\\). \\(Z\\): A calculated value to be compared against a Standard Normal distribution. \\(C\\): The critical region (rejection region) of the test. Two-Sided Test \\[ \\begin{align*} \\gamma(\\pi_a) &amp;= P_{\\pi_a}(p \\in C)\\\\ &amp;= P_\\mu\\Big(p \\leq \\pi_0 - z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\Big) + P_{\\pi_a}\\Big(p \\geq \\pi_0 + z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(p - \\pi_a \\leq \\pi_0 - \\pi_a - z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\Big) + \\\\ &amp; \\ \\ \\ \\ \\ P_{\\pi_a}\\Big(p - \\pi_a \\geq \\pi_0 - \\pi_a + z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(\\frac{p - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} \\leq \\frac{\\pi_0 - \\pi_a - z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big) +\\\\ &amp; \\ \\ \\ \\ \\ P_{\\pi_a}\\Big(\\frac{p - \\mu}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} \\geq \\frac{\\pi_0 - \\pi_a + z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\leq \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} - z_{\\alpha/2}\\Big) + P_{\\pi_a}\\Big(Z \\geq \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} + z_{1-\\alpha/2}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big) + P_{\\pi_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\pi_0 - \\pi_a)}{\\sqrt{\\pi_a \\cdot (1 - \\pi_a)}}\\Big) + P_{\\pi_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\pi_0 - \\pi_a)}{\\sqrt{\\pi_a \\cdot (1 - \\pi_a)}}\\Big) \\end{align*} \\] Both \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) have Standard Normal distributions. One-Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). Thus, where \\(Z\\) occurs in the derivation below, it may reasonably be replaced with \\(|Z|\\). \\[ \\begin{align*} \\gamma(\\pi_a) &amp;= P_{\\pi_a}(p \\in C)\\\\ &amp;= P_{\\pi_a}\\big(p \\geq \\pi_0 + z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\big)\\\\ &amp;= P_{\\pi_a}\\big(p - \\pi_a \\geq \\pi_0 - \\pi_a + z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\big)\\\\ &amp;= P_{\\pi_a}\\Big(\\frac{p - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} \\geq \\frac{\\pi_0 - \\pi_a + z_{1-\\alpha} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\geq \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}} + z_{1-\\alpha} \\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\geq z_{1-\\alpha} + \\frac{\\pi_0 - \\pi_a}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big)\\\\ &amp;= P_{\\pi_a}\\Big(Z \\geq z_{1-\\alpha} + \\frac{\\sqrt{n} \\cdot (\\pi_0 -\\pi_a)}{\\sqrt{\\frac{\\pi_a \\cdot (1 - \\pi_a)}{n}}}\\Big) \\end{align*} \\] Where \\(z_{1-\\alpha}\\) has a Standard Normal distribution. 41.1.6 Confidence Interval The confidence interval for \\(\\theta\\) is written: \\[p \\pm z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = z_{1-\\alpha/2} \\cdot \\sqrt{\\frac{\\pi_0 \\cdot (1 - \\pi_0)}{n}}\\] 41.2 References Wackerly, Mendenhall, Scheaffer, Mathematical Statistics with Applications, 6th ed., Duxbury, 2002, ISBN 0-534-37741-6. Daniel, Biostatistics, 8th ed., John Wiley &amp; Sons, Inc., 2005, ISBN: 0-471-45654-3. Hogg, McKean, Craig, Introduction to Mathematical Statistics, 6th ed., Pearson, 2005, ISBN: 0-13-008507-3 "],
["z-test.html", "42 Z-test 42.1 One-Sample Z-test 42.2 Two-Sample T-test 42.3 One Sided Test 42.4 References", " 42 Z-test 42.1 One-Sample Z-test The t-test is commonly used to look for evidence that the mean of a normally distributed random variable may differ from a hypothesized (or previously observed) value. 42.1.1 Z-Statistic The \\(z\\)-statistic is a standardized measure of the magnitude of difference between a sample’s mean and some known, non-random constant. Calculation of the \\(z\\)-statistic assumes knowledge of the population variance. 42.1.2 Definitions and Terminology Let \\(\\bar{x}\\) be a sample mean from a sample with standard deviation \\(\\sigma\\). Let \\(\\mu_0\\) be a constant, and \\(\\sigma\\) be the population standard deviation. \\(z\\) is defined: \\[z = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] 42.1.3 Hypotheses The hypotheses for these test take the forms: For a two-sided test: \\[ \\begin{align*} H_0: \\mu &amp;= \\mu_0\\\\ H_a: \\mu &amp;\\neq \\mu_0 \\end{align*} \\] For a one-sided test: \\[ \\begin{align*} H_0: \\mu &amp;\\leq \\mu_0\\\\ H_a: \\mu &amp;&gt; \\mu_0 \\end{align*} \\] or \\[ \\begin{align*} H_0: \\mu &amp;\\geq \\mu_0\\\\ H_a: \\mu &amp;&lt; \\mu_0 \\end{align*} \\] To compare a sample \\((X_1, \\ldots, X_n)\\) against the hypothesized value, a Z-statistic is calculated in the form: \\[Z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\] Where \\(\\bar{x}\\) is the sample mean and \\(\\sigma\\) is the population standard deviation. 42.1.4 Decision Rule The decision to reject a null hypothesis is made when an observed Z-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of shaded areas on both sides equaling the corresponding \\(\\alpha\\). It follows, then, that the decision rule is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha/2}\\) or when \\(Z \\geq z_{1-\\alpha/2}\\). By taking advantage of the symmetry of the Z-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha/2}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 42.1: Critical regions for the two-sided Z-Test In the one-sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(Z \\geq z_{1-\\alpha}\\). Using the symmetry of the Z-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 42.2: Critical regions for the two-sided Z-Test The decision rule can also be written in terms of \\(\\bar{x}\\): Reject \\(H_0\\) when \\(\\bar{x} \\leq \\mu_0 - z_\\alpha \\cdot \\sigma/\\sqrt{n}\\) or \\(\\bar{x} \\geq \\mu_0 + z_\\alpha \\cdot \\sigma/\\sqrt{n}\\). This change can be justified by: \\[ \\begin{align*} |Z| &amp;\\geq z_{1-\\alpha}\\\\ \\Big|\\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\Big| &amp;\\geq z_{1-\\alpha} \\end{align*} \\] \\[ \\begin{align*} -\\Big(\\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\Big) &amp;\\geq z_{1-\\alpha} &amp; \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} &amp;\\geq z_{1-\\alpha}\\\\ \\bar{x} - \\mu_0 &amp;\\leq - z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} &amp; \\bar{x} - \\mu_0 &amp;\\geq z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n}\\\\ \\bar{x} &amp;\\leq \\mu_0 - z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} &amp; \\bar{x} &amp;\\geq \\mu_0 + z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} \\end{align*} \\] For a two-sided test, both the conditions apply. The left side condition is used for a left-tailed test, and the right side condition for a right-tailed test. 42.1.5 Power The derivations below make use of the following symbols: \\(\\bar{x}\\): The sample mean \\(\\sigma\\): The population standard deviation \\(n\\): The sample size \\(\\mu_0\\): The value of population mean under the null hypothesis \\(\\mu_a\\): The value of the population mean under the alternative hypothesis. \\(\\alpha\\): The significance level \\(\\gamma(\\mu)\\): The power of the test for the parameter \\(\\mu\\). \\(t_{\\alpha, \\nu}\\): A quantile of the central t-distribution for a probability, \\(\\alpha\\) and \\(n-1\\) degrees of freedom. \\(T\\): A calculated value to be compared against a t-distribution. \\(C\\): The critical region (rejection region) of the test. Two-Sided Test \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_\\mu\\big(\\bar{x} \\leq \\mu_0 - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\leq \\mu_0 - \\mu_a - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\leq \\frac{\\mu_0 - \\mu_a - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}}{\\sigma/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} - z_{\\alpha/2}\\Big) + P_{\\mu_a}\\Big(Z \\geq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} + z_{1-\\alpha/2}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{\\sigma}\\Big) + P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{\\sigma}\\Big) \\end{align*} \\] ** This sentence needs work ** Both \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) have normal distributions with non-centrality parameter \\(\\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{\\sigma}\\). One-Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). Thus, where \\(Z\\) occurs in the derivation below, it may reasonably be replaced with \\(|Z|\\). \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu_a}{\\sigma/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}}{\\sigma / \\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} + z_{1-\\alpha} \\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha} + \\frac{\\sqrt{n} \\cdot (\\mu_0 -\\mu_a)}{\\sigma}\\Big) \\end{align*} \\] ** This sentence is not accurate ** Where \\(z_{1-\\alpha} + \\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{\\sigma}\\) has a non-central t-distribution with non-centrality parameter \\(\\frac{\\sqrt{n} (\\mu_0 -\\mu_a)}{\\sigma}\\) 42.1.6 Confidence Interval The confidence interval for \\(\\theta\\) is written: \\[\\bar{x} \\pm z_{1-\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = z_{1-\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] 42.2 Two-Sample T-test The two sample t-test is commonly used to look for evidence that the mean of one normally distributed random variable may differ from that of another normally distributed random variable. The hypotheses for this test take the forms: 42.2.1 T-Statistic The \\(t\\)-statistic is a standardize measure of the magnitude of difference between two sample means and some known, non-random difference of population means. It is similar to a two sample \\(z\\)-statistic, but differes in that a \\(t\\)-statistic may be calculated without knowledge of the population variances. 42.2.2 Definitions and Terminology Let \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) be sample means from two independent samples with standard deviations \\(s_1\\) and \\(s_2\\). Let \\(\\mu_1\\) and \\(\\mu_2\\) be constants representing the means of the populations from which \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) obtained. \\(z\\) is defined: \\[ z = \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\] Where \\[ SE^* = \\left\\{ \\begin{array}{rl} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}, &amp; \\sigma_1^2 \\neq \\sigma_2^2 \\\\ \\sqrt{\\frac{(n_1-1) \\cdot \\sigma_1^2 + (n_2-1) \\cdot \\sigma_2^2} {n_1 + n_2 - 2}} \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, &amp; \\sigma_1^2 = \\sigma_2^2 \\end{array} \\right. \\] 42.2.3 Hypotheses For a two-sided test: \\[H_0 : \\mu_1 = \\mu_2 \\\\ H_a : \\mu_1 \\neq \\mu_2 \\] For a one-sided test: \\[ H_0 : \\mu_1 \\leq \\mu_2 \\\\ H_a : \\mu_1 &gt; \\mu_2 \\] or \\[ H_0 : \\mu_1 \\geq \\mu_1 \\\\ H_a : \\mu_1 &lt; \\mu_1 \\] 42.2.4 Decision Rule The decision to reject a null hypothesis is made when an observed T-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. 42.2.4.1 Two Sided Test In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of the shaded areas on both sides equally the corresponding \\(\\alpha\\). It follows then that the decision rule is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha/2}\\) or when \\(Z \\geq z_{1 - \\alpha/2}\\). By taking advantage of the symmetry of the Z-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha/2}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 42.3: The example displayed uses 25 degrees of freedom 42.3 One Sided Test In the one sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follow, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(T \\leq t_{\\alpha, \\nu}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(T \\geq t_{1-\\alpha, \\nu}\\). Using the symmetry of the \\(T\\)-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha, \\nu}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 42.4: The example displayed uses 25 degrees of freedom The decision rule can also be written in terms of \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\). Reject \\(H_0\\) when \\(\\bar{x_1} - \\bar{x_2} \\leq (\\mu_1 - \\mu_2) - t_{\\alpha, \\nu} \\cdot SE^*\\) or \\(\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{\\alpha, \\nu} \\cdot SE^*\\) This change can be justified by: \\[\\begin{aligned} |T| &amp; \\geq t_{1 - \\alpha, \\nu} \\\\ \\Big| \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\Big | &amp; \\geq t_{1 - \\alpha, \\nu} \\end{aligned}\\] \\[ \\begin{aligned} -\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq t_{1-\\alpha, \\nu} &amp; \\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq t_{1-\\alpha, \\nu} \\\\ (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp; \\leq -t_{1 - \\alpha, \\nu} \\cdot SE^* &amp; (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp;\\geq t_{1 - \\alpha, \\nu} \\cdot SE^* \\\\ \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) - t_{1-\\alpha, \\nu} \\cdot SE^* &amp; \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) + t_{1-\\alpha, \\nu} \\cdot SE^* \\end{aligned}\\] 42.3.1 Power Two Sided Test \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) \\leq (\\mu_1 - \\mu_2) - t_{\\alpha/2, \\nu} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{1-\\alpha/2, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\leq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - t_{\\alpha/2, \\nu} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha/2, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - t_{\\alpha/2, \\nu} \\cdot SE^*}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha/2, \\nu} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} - t_{\\alpha/2, \\nu}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + t_{1-\\alpha/2, \\nu}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\leq -t_{\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] Both \\(-t_{\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) and \\(t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) have non-central T-distributions with non-centrality parameter \\(\\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\). One Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|T| \\geq t_{1-\\alpha}\\). Thus, where \\(T\\) occurs in the derivation below, it may reasonably be replaced with \\(|T|\\). \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + t_{1-\\alpha, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha, \\nu} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + t_{1-\\alpha, \\nu} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + t_{1-\\alpha, \\nu}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(T \\geq t_{1-\\alpha, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] \\(t_{1-\\alpha/2, \\nu} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) has a non-central T-distribution with non-centrality parameter \\(\\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\). 42.3.2 Confidence Interval The confidence interval for \\(\\mu_1 - \\mu_2\\) is written: \\[(\\bar{x_1} - \\bar{x_2}) \\pm t_{1-\\alpha/2} \\cdot SE^*\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = t_{1-\\alpha/2} \\cdot SE^*\\] 42.4 References Wackerly, Mendenhall, Scheaffer, Mathematical Statistics with Applications, 6th ed., Duxbury, 2002, ISBN 0-534-37741-6. Daniel, Biostatistics, 8th ed., John Wiley &amp; Sons, Inc., 2005, ISBN: 0-471-45654-3. Hogg, McKean, Craig, Introduction to Mathematical Statistics, 6th ed., Pearson, 2005, ISBN: 0-13-008507-3 Wikipedia, “Student’s T test”, https://en.wikipedia.org/wiki/Student%27s_t-test Wikipedia, “Welch-Satterthwaite Equation”, https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation "],
["z-test-of-sample-means.html", "43 Z-test of Sample Means 43.1 One-Sample Z-test 43.2 Two-Sample Z-test 43.3 One Sided Test 43.4 References", " 43 Z-test of Sample Means 43.1 One-Sample Z-test The \\(z\\)-test is commonly used to look for evidence that the mean of a normally distributed random variable may differ from a hypothesized (or previously observed) value. It’s utility is limited, as it assume knowledge of the population standard deviation, which is rarely known. 43.1.1 Z-Statistic The \\(z\\)-statistic is a standardized measure of the magnitude of difference between a sample’s mean and some known, non-random constant. 43.1.2 Definitions and Terminology Let \\(\\bar{x}\\) be a sample mean from a sample with known population standard deviation \\(\\sigma\\). Let \\(\\mu_0\\) be a constant, and \\(\\sigma_\\bar{x} = \\sigma/\\sqrt{n}\\) be the standard error of the parameter \\(\\bar{x}\\). \\(z\\) is defined: \\[z = \\frac{\\bar{x} - \\mu_0}{\\sigma_\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\] 43.1.3 Hypotheses The hypotheses for these test take the forms: For a two-sided test: \\[ \\begin{align*} H_0: \\mu &amp;= \\mu_0\\\\ H_a: \\mu &amp;\\neq \\mu_0 \\end{align*} \\] For a one-sided test: \\[ \\begin{align*} H_0: \\mu &amp;&lt; \\mu_0\\\\ H_a: \\mu &amp;\\geq \\mu_0 \\end{align*} \\] or \\[ \\begin{align*} H_0: \\mu &amp;&gt; \\mu_0\\\\ H_a: \\mu &amp;\\leq \\mu_0 \\end{align*} \\] To compare a sample \\((X_1, \\ldots, X_n)\\) against the hypothesized value, a T-statistic is calculated in the form: \\[Z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\] Where \\(\\bar{x}\\) is the sample mean and \\(\\sigma\\) is the population standard deviation. 43.1.4 Decision Rule The decision to reject a null hypothesis is made when an observed T-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of shaded areas on both sides equaling the corresponding \\(\\alpha\\). It follows, then, that the decision rule is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha/2}\\) or when \\(Z \\geq z_{1-\\alpha/2}\\). By taking advantage of the symmetry of the Z-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha/2}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 43.1: Rejection regions for the Z-test In the one-sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follows, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha, \\nu}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(Z \\geq z_{1-\\alpha, \\nu}\\). Using the symmetry of the Z-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 43.2: Rejection regions for one-tailed Z-test The decision rule can also be written in terms of \\(\\bar{x}\\): Reject \\(H_0\\) when \\(\\bar{x} \\leq \\mu_0 - z_\\alpha \\cdot \\sigma/\\sqrt{n}\\) or \\(\\bar{x} \\geq \\mu_0 + z_\\alpha \\cdot \\sigma/\\sqrt{n}\\). This change can be justified by: \\[ \\begin{align*} |Z| &amp;\\geq z_{1-\\alpha}\\\\ \\Big|\\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\Big| &amp;\\geq z_{1-\\alpha} \\end{align*} \\] \\[ \\begin{align*} -\\Big(\\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\Big) &amp;\\geq z_{1-\\alpha} &amp; \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} &amp;\\geq z_{1-\\alpha}\\\\ \\bar{x} - \\mu_0 &amp;\\leq - z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} &amp; \\bar{x} - \\mu_0 &amp;\\geq z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n}\\\\ \\bar{x} &amp;\\leq \\mu_0 - z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} &amp; \\bar{x} &amp;\\geq \\mu_0 + z_{1-\\alpha} \\cdot \\sigma/\\sqrt{n} \\end{align*} \\] For a two-sided test, both the conditions apply. The left side condition is used for a left-tailed test, and the right side condition for a right-tailed test. 43.1.5 Power The derivations below make use of the following symbols: \\(\\bar{x}\\): The sample mean \\(s\\): The sample standard deviation \\(n\\): The sample size \\(\\mu_0\\): The value of population mean under the null hypothesis \\(\\mu_a\\): The value of the population mean under the alternative hypothesis. \\(\\alpha\\): The significance level \\(\\gamma(\\mu)\\): The power of the test for the parameter \\(\\mu\\). \\(z_{\\alpha}\\): A quantile of the Standard Normal distribution for a probability, \\(\\alpha\\). \\(Z\\): A calculated value to be compared against a Standard Normal distribution. \\(C\\): The critical region (rejection region) of the test. Two-Sided Test \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_\\mu\\big(\\bar{x} \\leq \\mu_0 - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\leq \\mu_0 - \\mu_a - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big) + P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\leq \\frac{\\mu_0 - \\mu_a - z_{\\alpha/2} \\cdot \\sigma/\\sqrt{n}}{\\sigma/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + z_{1-\\alpha/2} \\cdot \\sigma/\\sqrt{n}}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} - z_{\\alpha/2}\\Big) + P_{\\mu_a}\\Big(Z \\geq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} + z_{1-\\alpha/2}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big) + P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{\\sigma}\\Big) + P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{\\sqrt{n} \\cdot (\\mu_0 - \\mu_a)}{\\sigma}\\Big) \\end{align*} \\] Both \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) have Standard Normal distributions. One-Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). Thus, where \\(Z\\) occurs in the derivation below, it may reasonably be replaced with \\(|Z|\\). \\[ \\begin{align*} \\gamma(\\mu_a) &amp;= P_{\\mu_a}(\\bar{x} \\in C)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} \\geq \\mu_0 + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\big(\\bar{x} - \\mu_a \\geq \\mu_0 - \\mu_a + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}\\big)\\\\ &amp;= P_{\\mu_a}\\Big(\\frac{\\bar{x} - \\mu_a}{\\sigma/\\sqrt{n}} \\geq \\frac{\\mu_0 - \\mu_a + z_{1-\\alpha} \\cdot \\sigma / \\sqrt{n}}{\\sigma / \\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}} + z_{1-\\alpha} \\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq z_{1-\\alpha} + \\frac{\\mu_0 - \\mu_a}{\\sigma/\\sqrt{n}}\\Big)\\\\ &amp;= P_{\\mu_a}\\Big(Z \\geq t_{1-\\alpha} + \\frac{\\sqrt{n} \\cdot (\\mu_0 -\\mu_a)}{\\sigma}\\Big) \\end{align*} \\] Where \\(z_{1-\\alpha}\\) has a Standard Normal distribution. 43.1.6 Confidence Interval The confidence interval for \\(\\theta\\) is written: \\[\\bar{x} \\pm z_{1-\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = z_{1-\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] 43.2 Two-Sample Z-test The two sample z-test is commonly used to look for evidence that the mean of one normally distributed random variable may differ from that of another normally distributed random variable. The hypotheses for this test take the forms: 43.2.1 Z-Statistic The \\(z\\)-statistic is a standardized measure of the magnitude of difference between two sample means and some known, non-random difference of population means. It is similar to a two sample \\(t\\)-statistic, but differs in that a \\(z\\)-statistic may not be calculated without knowledge of the population variances. 43.2.2 Definitions and Terminology Let \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) be sample means from two independent samples with standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\). Let \\(\\mu_1\\) and \\(\\mu_2\\) be constants representing the means of the populations from which \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) obtained. \\(z\\) is defined: \\[ z = \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\] Where \\[ SE^* = \\left\\{ \\begin{array}{rl} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}, &amp; \\sigma_1^2 \\neq \\sigma_2^2 \\\\ \\sqrt{\\frac{(n_1-1) \\cdot \\sigma_1^2 + (n_2-1) \\cdot \\sigma_2^2} {n_1 + n_2 - 2}} \\cdot \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}, &amp; \\sigma_1^2 = \\sigma_2^2 \\end{array} \\right. \\] 43.2.3 Hypotheses For a two-sided test: \\[H_0 : \\mu_1 = \\mu_2 \\\\ H_a : \\mu_1 \\neq \\mu_2 \\] For a one-sided test: \\[ H_0 : \\mu_1 \\leq \\mu_2 \\\\ H_a : \\mu_1 &gt; \\mu_2 \\] or \\[ H_0 : \\mu_1 \\geq \\mu_1 \\\\ H_a : \\mu_1 &lt; \\mu_1 \\] 43.2.4 Decision Rule The decision to reject a null hypothesis is made when an observed Z-value lies in a critical region that suggests the probability of that observation is low. We define the critical region as the upper bound we are willing to accept for \\(\\alpha\\), the Type I Error. 43.2.4.1 Two Sided Test In the two-sided test, \\(\\alpha\\) is shared equally in both tails. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below, with the sum of the shaded areas on both sides equally the corresponding \\(\\alpha\\). It follows then that the decision rule is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha/2}\\) or when \\(Z \\geq z_{1 - \\alpha/2}\\). By taking advantage of the symmetry of the Z-distribution, we can simplify the decision rule to: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha/2}\\) ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 43.3: Rejection region for a two-tailed test 43.3 One Sided Test In the one sided test, \\(\\alpha\\) is placed in only one tail. The rejection regions for the most common values of \\(\\alpha\\) are depicted in the figure below. In each case, \\(\\alpha\\) is the area in the tail of the figure. It follow, then, that the decision rule for a lower tailed test is: Reject \\(H_0\\) when \\(Z \\leq z_{\\alpha}\\). For an upper tailed test, the decision rule is: Reject \\(H_0\\) when \\(Z \\geq z_{1-\\alpha}\\). Using the symmetry of the \\(Z\\)-distribution, we can simplify the decision rule as: Reject \\(H_0\\) when \\(|Z| \\geq z_{1-\\alpha}\\). ## Warning: Ignoring unknown aesthetics: ymax ## Warning: Ignoring unknown aesthetics: ymax Figure 43.4: Rejection regions for one-tailed tests The decision rule can also be written in terms of \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\). Reject \\(H_0\\) when \\(\\bar{x_1} - \\bar{x_2} \\leq (\\mu_1 - \\mu_2) - z_{\\alpha} \\cdot SE^*\\) or \\(\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + z_{\\alpha} \\cdot SE^*\\) This change can be justified by: \\[\\begin{aligned} |Z| &amp; \\geq z_{1 - \\alpha} \\\\ \\Big| \\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\Big | &amp; \\geq z_{1 - \\alpha} \\end{aligned}\\] \\[ \\begin{aligned} -\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq z_{1-\\alpha} &amp; \\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*}\\Big) &amp; \\geq z_{1-\\alpha} \\\\ (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp; \\leq -z_{1 - \\alpha} \\cdot SE^* &amp; (\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2) &amp;\\geq z_{1 - \\alpha} \\cdot SE^* \\\\ \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) - z_{1-\\alpha} \\cdot SE^* &amp; \\bar{x_1} - \\bar{x_2} &amp;\\leq (\\mu_1 - \\mu_2) + z_{1-\\alpha} \\cdot SE^* \\end{aligned}\\] 43.3.1 Power Two Sided Test \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) \\leq (\\mu_1 - \\mu_2) - z_{\\alpha/2} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + z_{1-\\alpha/2} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\leq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - z_{\\alpha/2} \\cdot SE^*\\big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + z_{1-\\alpha/2} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_1 - \\mu_2)}{SE^*} \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) - z_{\\alpha/2} \\cdot SE^*}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + z_{1-\\alpha/2} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\leq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} - z_{\\alpha/2}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + z_{1-\\alpha/2}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\leq -z_{\\alpha/2} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) + \\\\ &amp;\\ \\ \\ \\ P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\geq z_{1-\\alpha/2} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] Both \\(-z_{\\alpha/2} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) and \\(z_{1-\\alpha/2} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) have Standard Normal distributions. One Sided Test For convenience, the power for only the upper tailed test is derived here. Recall that the symmetry of the t-test allows us to use the decision rule: Reject \\(H_0\\) when \\(|Z| \\geq t_{1-\\alpha}\\). Thus, where \\(Z\\) occurs in the derivation below, it may reasonably be replaced with \\(|Z|\\). \\[ \\begin{align*} \\gamma(\\mu_{1a} - \\mu_{2a}) &amp;= P_{\\mu_{1a} - \\mu_{2a}}(\\bar{x} \\in C)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2} \\geq (\\mu_1 - \\mu_2) + z_{1-\\alpha} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\big((\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a}) \\geq (\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + z_{1-\\alpha} \\cdot SE^*\\big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(\\frac{(\\bar{x_1} - \\bar{x_2}) - (\\mu_{1a} - \\mu_{2a})}{SE^*} \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a}) + z_{1-\\alpha} \\cdot SE^*}{SE^*}\\Big)\\\\ %%% &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\geq \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*} + z_{1-\\alpha}\\Big)\\\\ &amp;= P_{\\mu_{1a} - \\mu_{2a}}\\Big(Z \\geq z_{1-\\alpha} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\Big) \\end{align*} \\] \\(z_{1-\\alpha/2} + \\frac{(\\mu_1 - \\mu_2) - (\\mu_{1a} - \\mu_{2a})}{SE^*}\\) has a Normal Distribution. 43.3.2 Confidence Interval The confidence interval for \\(\\mu_1 - \\mu_2\\) is written: \\[(\\bar{x_1} - \\bar{x_2}) \\pm z_{1-\\alpha/2} \\cdot SE^*\\] The value of the expression on the right is often referred to as the margin of error, and we will refer to this value as \\[E = z_{1-\\alpha/2} \\cdot SE^*\\] 43.4 References Wackerly, Mendenhall, Scheaffer, Mathematical Statistics with Applications, 6th ed., Duxbury, 2002, ISBN 0-534-37741-6. Daniel, Biostatistics, 8th ed., John Wiley &amp; Sons, Inc., 2005, ISBN: 0-471-45654-3. Hogg, McKean, Craig, Introduction to Mathematical Statistics, 6th ed., Pearson, 2005, ISBN: 0-13-008507-3 "],
["references-7.html", "44 References", " 44 References "]
]
