[
["index.html", "It Can Be Shown 1 Introduction", " It Can Be Shown Notes on Statistical Theory Benjamin Nutter 2016-08-19 1 Introduction There is one phrase that makes me cringe every time I see it. It’s a phrase that embodies feelings of frustration, inadequacy, and failure to understand. That phrase: It can be shown Everytime I read that phrase, I would look at the subsequent result and think “Really? It can?” This book is a collection of notes that I’ve put together to avoid having to feel that way in the future. It is, essentially, a collection of definitions and proofs that have helped me understand and apply mathematical and statistical theory. Most imporantly, it spells even the smallest steps along each development so that I don’t have to worry about solving it again in the future. You won’t find much in the way of application. There are no exercises. There is only minimal explanation. My intent is to show development of statistical theory and nothing else. "],
["analysis-of-variance.html", "2 Analysis of Variance 2.1 One-Way Design 2.2 Computational Formulas 2.3 Randomized Complete Block Design", " 2 Analysis of Variance 2.1 One-Way Design 2.1.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+} + \\bar x_{i+} - x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (x_{ij} - \\bar x{i+}) + (\\bar x_{i+} - \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++}) + (x_{ij} - \\bar x{i+}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} \\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + (x_{ij} - \\bar x_{i+})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\bigg(\\sum\\limits_{j=1}^{n_i} x_{ij} - \\sum\\limits_{j=1}^{n_i}\\bar x_{i+}\\bigg) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - n_i \\bar x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\big(x_{i+} - n_i \\frac{x_{i+}}{n_i}\\big) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) (x_{i+} - x_{i+}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 2\\sum\\limits_{i=1}^{a} n_i (\\bar x_{i+} - \\bar x_{++}) \\cdot 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + 0 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2\\\\ \\end{align*}\\] The components are commonly referred to as \\[ SS_{Factor} = \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\] and \\[ SS_{Error} = \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{i+})^2 \\] Notice that \\(SS_{Factor}\\) compares the factor means to the overall mean, and it can be said that \\(SS_{Factor}\\) measures the variation between factors. \\(SS_{Error}\\) compares each observation to the overall mean, and can be said to describe the variation within factors. When \\(n_1 = n_2 = \\cdots n_i = n\\), the design is said to be balanced. 2.2 Computational Formulas \\(SS_{Total}\\) and \\(SS_{Factor}\\) can be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} (x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{n_i} x_{ij}^2 - x_{++} \\sum\\limits_{j=1}^{n_i}\\frac{1}{n_i}\\\\ \\end{align*}\\] See Theorem 30.3.1 \\[\\begin{align*} SS_{Factor} &amp;= \\sum\\limits_{i=1}^{a} n_i(\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\frac{\\bar x_{i+}^2}{n_i} - \\bar x_{++} \\sum\\limits_{i=1}^{a}\\frac{1}{n_i} \\end{align*}\\] See Theorem 30.3.1 \\(SS_{Error}\\) does not simplify to a convenient form, but \\[\\begin{align*} SS_{Total} &amp;= SS_{Factor} + SS_{Error} \\\\ \\Rightarrow SS_{Error} &amp;= SS_{Total} - SS_{Factor} \\end{align*}\\] 2.3 Randomized Complete Block Design Blocking in ANOVA is a method of eliminate the effect of a controllable nuisance variable. To implement this design, suppose we have \\(a\\) treatments we want to compare, and \\(b\\) blocks. We may analyze the data by use of the sums of squares, similar to the one-way design. 2.3.1 Decomposition of Sums of Squares \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} + \\bar x_{i+} - \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{+ j} + \\bar x_{++} - \\bar x_{+ +} - \\bar x_{++})^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++}) + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big]^2 \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) + (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++}) (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{i+} - \\bar x_{++})(\\bar x_{+ j} - \\bar x_{++}) + 2(\\bar x_{i+} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\\\ &amp; \\ \\ \\ \\ + 2(\\bar x_{+ j} - \\bar x_{++})(x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++}) \\big] \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+} - \\bar x_{++})^2 + (\\bar x_{+ j} - \\bar x_{++})^2 + (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 + 0 + 0 + 0 \\big] \\\\ &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\\\ &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 + a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] It is shown that the cross products are equal to zero in Section 2.3.3 These terms are commonly referred to as \\[\\begin{align*} SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ SS_{Error} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (x_{ij} - \\bar x_{i+} - \\bar x_{+ j} + \\bar x_{++})^2 \\end{align*}\\] 2.3.2 Computational Formulae \\(SS_{Total}\\), \\(SS_{Factor}\\), and \\(SS_{Block}\\) can all be simplified for convenient computation. \\[\\begin{align*} SS_{Total} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}(x_{ij} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} x_{ij}^2 - \\frac{x_{++}}{ab}\\\\ \\\\ SS_{Factor} &amp;= b \\sum\\limits_{i=1}^{a} (\\bar x_{i+} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{b}\\sum\\limits_{i=1}^{a}x_{i+}^2 - \\frac{x_{++}^2}{ab} \\\\ \\\\ SS_{Block} &amp;= a \\sum\\limits_{j=1}^{b} (\\bar x_{+ j} - \\bar x_{++})^2 \\\\ ^{[1]} &amp;= \\frac{1}{a}\\sum\\limits_{j=1}^{b} x_{+ j}^2 - \\frac{x_{++}^2}{ab} \\end{align*}\\] See Theorem 30.3.1 \\(SS_{Error}\\) does not simplify to any convenient form, but may be calculated from the other terms as \\(SS_{Error} = SS_{Total} - SS_{Factor} - SS_{Block}\\) 2.3.3 RCBD Cross Products The cross products of the RCBD design \\[\\begin{align*} 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp; \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) &amp;= 0 \\end{align*}\\] Proof: \\[ 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\\\ \\ \\ = 2\\bigg(\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b} (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++})\\bigg)\\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ (\\bar x_{i+}-\\bar x_{++}) (\\bar x_{+ j}-\\bar x{++}) + (\\bar x_{+ j}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\\\ \\ \\ \\ \\ + (\\bar x_{+ i}-\\bar x_{++}) (x_{ij} + \\bar x_{i+} + \\bar x_{+ j} - \\bar x_{++}) \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\big[ \\bar x_{i+}\\bar x_{+ j} - \\bar x_{i+}\\bar x_{++} - \\bar x_{+ j}\\bar x_{++} + \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}\\bar x_{+ j} - \\bar x_{+ j}^2 + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\\\ \\ \\ \\ \\ + x_{ij}\\bar x_{+ j} - \\bar x_{i +}^2 - \\bar x_{i+}\\bar x_{+ j} + \\bar x_{+ j}\\bar x_{++} - x_{ij}\\bar x_{++} + \\bar x_{i+}\\bar x_{++} + \\bar x_{+ j}\\bar x_{++} - \\bar x_{++}^2 \\big] \\\\ \\ \\ = 2\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}( -\\bar x_{++}^2 - \\bar x_{i+}^2 - \\bar x_{+ j}^2 + x_{ij}\\bar x_{i+} + x_{ij}\\bar x_{+ j} - 2 x_{ij}\\bar x_{++} - \\bar x_{i+}\\bar x_{+ j} \\\\ \\ \\ \\ \\ + 2\\bar x_{i+}\\bar x_{++} + 2\\bar x_{+ j}\\bar x_{++} ) \\\\ \\ \\ = 2\\bigg(-\\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{++}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}^2 - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j} \\\\ \\ \\ \\ \\ - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2 x_{ij}\\bar x_{++} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ = 2\\bigg( \\frac{ab\\bar x_{++}^2}{a^2b^2} - \\frac{b}{b^2}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{a}{a^2}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{i+} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[1]} =2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}x_{ij}\\bar x_{+ j}\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[2]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}\\bar x_{i+}\\bar x_{+ j} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[3]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{i+}\\bar x_{++} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg) \\\\ \\ \\ ^{[4]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\] \\[ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\sum\\limits_{i=1}^{a}\\sum\\limits_{j=1}^{b}2\\bar x_{+ j}\\bar x_{++} \\bigg)\\\\ \\ \\ ^{[5]} = 2\\bigg( \\frac{\\bar x_{++}^2}{ab} - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2\\\\ \\ \\ \\ \\ - \\frac{2\\bar x{++}^2}{ab} - \\frac{\\bar x_{++}}{ab} + \\frac{2\\bar x_{++}^2}{ab} + \\frac{2\\bar x_{++}^2}{ab} \\bigg)\\\\ \\ \\ = 2\\bigg(\\frac{4\\bar x_{++}^2}{ab} - \\frac{4\\bar x_{++}^2}{ab} + \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 - \\frac{1}{b}\\sum\\limits_{i=1}^{a}\\bar x_{i+}^2 + \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 - \\frac{1}{a}\\sum\\limits_{j=1}^{b}\\bar x_{+ j}^2 \\bigg)\\\\ \\ \\ = 2(0 + 0 + 0) \\\\ = 2(0) \\\\ = 0 \\] See Summation Theorem 27.1.7 See Summation Theorem 27.1.8 See Summation Theorem 27.1.4 See Summation Theorem 27.1.5 See Summation Theorem 27.1.6 Using the theorems in Chapter it is can be shown that each of the three cross products is equal to zero. However, the physical tedium of reducing each cross product is much greater than the approach taken above. "],
["bernoulli-distribution.html", "3 Bernoulli Distribution 3.1 Probability Mass Function 3.2 Cumulative Mass Function 3.3 Expected Values 3.4 Moment Generating Function 3.5 Theorems for the Bernoulli Distribution", " 3 Bernoulli Distribution 3.1 Probability Mass Function A random variable is said to have a Bernoulli Distribution with parameter \\(p\\) if its probability mass function is: \\[p(x)=\\left\\{ \\begin{array}{ll} p^x(1-p)^{1-x}, &amp; x=0,1\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(p\\) is the probability of a success. 3.2 Cumulative Mass Function \\[P(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;0\\\\ 1-p &amp; x=0\\\\ 1 &amp; 1\\leq x \\end{array} \\right. \\] (#fig:Bernoulli_Distribution)The graphs on the left and right show a Bernoulli Probability Distribution and Cumulative Distribution Function, respectively, with \\(p=.4\\). Note that this is identical to a Binomial Distribution with parameters \\(n=1\\) and \\(p=.4\\). 3.3 Expected Values \\[ \\begin{align*} E(X) &amp;= \\sum\\limits_{i=0}^{1} x\\cdot p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} x \\cdot p^{x} (1-p)^{1-x}\\\\ &amp;= 0 \\cdot p^{0} (1-p)^{1-0} + 1 \\cdot p^{1} (1-p)^{1-1}\\\\ &amp;= 0 + p (1-p)^{0}\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^{2}) &amp;= \\sum\\limits_{i=0}^{1} x^2 \\cdot p(x)\\\\ &amp;= \\sum\\limits_{i=0}^{1} x^{2} \\cdot p^x (1-p)^{1-x}\\\\ &amp;= \\sum\\limits_{i=0}^{1} 0^{2} \\cdot p^0 (1-p)^{1-0} + 1^2 \\cdot p^1 (1-p)^{1-1}\\\\ &amp;= 0 \\cdot 1 \\cdot 1 + 1 \\cdot p \\cdot 1 \\\\ &amp;= 0 + p\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X) = p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p-p^2 \\\\ &amp;= p(1-p) \\end{align*} \\] 3.4 Moment Generating Function \\[\\begin{align*} M_{X}(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p(x) \\\\ &amp;= \\sum\\limits_{i=0}^{1} e^{tx} p^{x} (1-p)^{1-x}\\\\ &amp;= e^{t0} p^0 (1-p)^{1-0} + e^t p^t (1-p)^{1-1}\\\\ &amp;= (1-p) + e^t p\\\\ &amp;=pe^t + (1-p) \\\\ \\\\ \\\\ M^{(1)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ M^{(2)}_X(t) &amp;= pe^t\\\\ \\\\ \\\\ E(X) &amp;=M^{(1)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ E(X^2) &amp;= M^{(2)}_X(0)\\\\ &amp;= pe^0\\\\ &amp;= p\\\\ \\\\ \\\\ \\mu &amp;= E(X)\\\\ &amp;= p\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= p - p^2 \\\\ &amp;= p (1-p) \\end{align*} \\] 3.5 Theorems for the Bernoulli Distribution 3.5.1 Validity of the Distribution \\[\\sum\\limits_{x=0}^{1}p^x(1-p)^{1-x}=1\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^{1} p^x (1-p)^{1-x} &amp;= p^0 (1-p)^1 + p^1 (1-p)^0 \\\\ &amp;= (1-p) + p \\\\ &amp;= 1 \\end{align*}\\] 3.5.2 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\) Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p)) (pe^t+(1-p)) \\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the moment generating function of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-distribution.html", "4 Binomial Distribution 4.1 Probability Mass Function 4.2 Cumulative Mass Function 4.3 Expected Values 4.4 Moment Generating Function 4.5 Maximum Likelihood Estimator 4.6 Theorems for the Binomial Distribution", " 4 Binomial Distribution 4.1 Probability Mass Function A random variable is said to follow a Binomial distribution with parameters \\(n\\) and \\(p\\) if its probability mass function is: \\[p(x)= \\left\\{ \\begin{array}{ll} {n \\choose x} p^x (1-p)^{n-x}, &amp; x=0,1,2,\\ldots,n\\\\ 0 &amp; \\mathrm{otherwise} \\end{array} \\right. \\] Where \\(n\\) is the number of trials performed and \\(p\\) is the probability of a success on each individual trial. 4.2 Cumulative Mass Function \\[ P(x)= \\left\\{ \\begin{array} {lll} 0 &amp; x&lt;0\\\\ \\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} &amp; 0 \\leq x=0,1,2,\\ldots,n\\\\ 1 &amp; n\\leq x \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} F(x+1) &amp;= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\ &amp;= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\ &amp;= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\ &amp;= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x) \\end{align*}\\] (#fig:Binomial_Distribution)The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with \\(n=10\\) and \\(p=.4\\). 4.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\ &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\ &amp;= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\ &amp;= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\ &amp;= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1} + \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\ &amp;= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x}) \\\\ ^{[2]} &amp;= np(p+q)^{n-1} \\\\ ^{[1]} &amp;= np(p+(1-p))^{n-1} \\\\ &amp;= np(p+1-p)^{n-1} \\\\ &amp;= np(1)^{n-1} \\\\ &amp;= np(1) \\\\ &amp;= np \\end{align*}\\] Let \\(q = (1 - p)\\) By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[\\begin{align*} E(X^2) &amp;= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x} \\\\ &amp;= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1} + \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n} \\\\ &amp;= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1} + 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2} + \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big] \\\\ &amp;= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\ &amp;= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\\\ ^{[2]} &amp;= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y} \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big] \\\\ &amp;= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big] \\\\ ^{[3]} &amp;= np(mp+1) \\\\ &amp;= np[(n-1)p+1] \\\\ &amp;=np(np-p+1) \\\\ &amp;=n^2p^2 - np^2 + np \\end{align*}\\] \\(q = (1 - p)\\) Let \\(y = x - 1\\) and \\(n = m + 1\\) \\(\\Rightarrow\\) \\(x = y + 1\\) and \\(m = n - 1\\) \\(\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}\\) is of the form of the expected value of \\(Y\\), and \\(E(Y)=mp=(n-1)p\\). \\(\\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}\\) is the sum of all probabilities over the domain of \\(Y\\) which is 1. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= n^2p^2 - np^2 + np - n^2p^2 \\\\ &amp;= -np^2 + np \\\\ &amp;= np(-p-1) \\\\ &amp;= np(1-p) \\end{align*}\\] 4.4 Moment Generating Function \\[ \\begin{align*} M_X(t) &amp;= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x} \\\\ &amp;= \\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x} \\\\ ^{[1]} &amp;= [(1-p)+pe^t]^n \\end{align*} \\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) \\[ \\begin{align*} M_X^{(1)}(t) &amp;= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\\\ \\\\ M_X^{(2)}(t) &amp;= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\\\ &amp;= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= n[(1-p)+pe^0]^{n-1}pe^0 \\\\ &amp;= n[1-p+p^{n-1}p\\\\ &amp;= n(1)^{n-1}p &amp;= np\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\\cdot0}[(1-p) + pe^0]^{n-2} \\\\ &amp;= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\\\ &amp;= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\\\ &amp;= np+n(n-1)p^2 \\\\ &amp;= np+(n^2-n)p^2 \\\\ &amp;= np + n^2 + n^2p^2 - np^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= np \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= np + n^2p^2 - np^2 - n^2p^2 \\\\ &amp;= np - np^2\\\\ &amp;= np(1-p) \\end{align*}\\] 4.5 Maximum Likelihood Estimator Since \\(n\\) is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for \\(n\\). The mean and variance can both be estimated from the single parameter \\(p\\). Let \\(X\\) be a Binomial random variable with parameter \\(p\\) and \\(n\\) outcomes \\((x_1,x_2,\\ldots,x_n)\\). Let \\(x_i=0\\) for a failure and \\(x_i=1\\) for a success. In other words, \\(X\\) is the sum of \\(n\\) Bernoulli trials with equal probability of success and \\(X=\\sum\\limits_{i=1}^{n}x_i\\). 4.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\ &amp;= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\ &amp;= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\ &amp;= \\theta^X(1-\\theta)^{n-X} \\end{align*}\\] 4.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln L(\\theta) \\\\ &amp;= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\ &amp;= X\\ln(\\theta)+(n-X)\\ln(1-\\theta) \\end{align*}\\] 4.5.3 MLE for p \\[\\begin{align*} \\frac{d\\ell(p)}{d p} &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\\\ \\\\ 0 &amp;= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\ \\Rightarrow \\frac{X}{p} &amp;= \\frac{n-X}{1-p} \\\\ \\Rightarrow (1-p)X &amp;= p(n-X) \\\\ \\Rightarrow X-pX &amp;= np-pX \\\\ \\Rightarrow X &amp;= np \\\\ \\Rightarrow \\frac{X}{n} &amp;= p \\\\ \\end{align*}\\] So \\(\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(p\\). 4.6 Theorems for the Binomial Distribution 4.6.1 Validity of the Distribution \\[\\begin{align*} \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x} = 1 \\end{align*}\\] Proof: \\[\\begin{align*} \\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x} \\\\ ^{[1]} &amp;= \\big(p + (1-p)\\big)^n \\\\ &amp;= (1)^n \\\\ &amp;= 1 \\end{align*}\\] By the Binomial Theorem (5.1.2), \\(\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n\\) 4.6.2 Sum of Binomial Random Variables Let \\(X_1,X_2,\\ldots,X_k\\) be independent random variables where \\(X_i\\) comes from a Binomial distribution with parameters \\(n_i\\) and \\(p\\). That is \\(X_i\\sim(n_i,p)\\). Let \\(Y = \\sum\\limits_{i=1}{k} X_i\\). Then \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\ &amp;= \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\\\ &amp;= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i} \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(\\sum\\limits_{i=1}^{k}n_i\\) and \\(p\\). Thus \\(Y\\sim\\)Binomial\\((\\sum\\limits_{i=1}^{k}n_i,p)\\). 4.6.3 Sum of Bernoulli Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent and identically distributed random variables from a Bernoulli distribution with parameter \\(p\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Binomial\\((n,p)\\) Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\ &amp;= (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p)) \\\\ &amp;= (pe^t+(1-p))^n \\end{align*}\\] Which is the mgf of a Binomial random variable with parameters \\(n\\) and \\(p\\). Thus, \\(Y\\sim\\) Binomial\\((n,p)\\). "],
["binomial-theorem.html", "5 Binomial Theorem 5.1 Traditional Proof 5.2 General Approach 5.3 Other Theorems", " 5 Binomial Theorem The Binomial Theorem is useful in developing theory around the Binomial and Hypergeometric Distributions. Two proofs of the Theorem are provided here; one using the traditional approach, and one using a more general approach. Other useful theorems are provided at the end of this chapter. 5.1 Traditional Proof 5.1.1 Lemma: Pascal’s rule Let \\(n\\) and \\(x\\) be non-negative integers such that \\(x\\leq n\\). Then \\({n-1\\choose x} + {n-1\\choose x-1} = {n\\choose x}\\). Proof: \\[\\begin{align*} {n-1\\choose x} + {n-1\\choose x-1} &amp;= \\frac{(n-1)!}{x!(n-1-x)!} + \\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-1-x+1)!}\\\\ &amp;= \\frac{(n-1)!}{x!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)!}\\\\ &amp;= \\frac{(n-1)!}{x(x-1)!(n-x-1)!} + \\frac{(n-1)!}{(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!}{x(x-1)!(n-x)(n-x-1)!} +\\frac{(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{x(n-1)!+(n-x)(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{(x+n-x)(x-1)!}{x(x-1)!(n-x)(n-x-1)!}\\\\ &amp;= \\frac{n(n-1)!}{x(x-1)!(n-x)(n-x-1)!} \\\\ &amp;= \\frac{n!}{x!(n-x)!} \\\\ &amp;= {n\\choose x} \\end{align*}\\] 5.1.2 The Binomial Theorem Let \\(a\\) and \\(b\\) be constants and let \\(n\\) be any positive integer. Then \\[(a+b)^n = \\sum\\limits_{x=0}^{n} {n\\choose x} a^{n-x} b^x\\] Proof: This proof is completed by mathematical induction. Base Step: \\(n=1\\) \\[\\begin{align*} (a+b)^1 &amp;= \\sum\\limits_{x=0}^{1} {1\\choose x} a^{1-x} b^x \\\\ &amp;= {1\\choose 0} a^{1-0} b^0 + {1\\choose 1} a^{1-1} b^1 \\\\ &amp;= 1\\cdot a\\cdot 1 + 1\\cdot 1\\cdot b \\\\ &amp;= a+b \\end{align*}\\] Inductive Step: Assume that the Theorem holds for \\(n\\), and show it is true for \\(n+1\\). \\[\\begin{align*} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= a(a+b)^n + b(a+b)^n \\\\ &amp;= a(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x + b^n) + b(a^n + \\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^n) \\\\ &amp;= (a^{n+1}+a\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^x+b^{n+1}) \\\\ &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x+1}ab^x) + (a^nb+\\sum\\limits_{x=1}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[1]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}a^{n-x+1}b^x) + (\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1}+b^{n+1}) \\\\ ^{[2]} &amp;= (a^{n+1}+\\sum\\limits_{x=1}^{n}{n\\choose x}a^{n-x+1}b^x) + \\sum\\limits_{x-1}^{n-1}{n\\choose x-1}a^{n-x+1}b^{x+1-1}+b^{n+1}) \\\\ ^{[3]} &amp;= a^{n+1} + \\sum\\limits_{x+1}^{n}{n+1\\choose x}a^{n-x+1}b^x + b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{x=1}^{n}{n+1\\choose x}a^{(n+1)-x}b^x+b^{n+1} \\\\ ^{[4]} &amp;= \\sum\\limits_{x=0}^{n+1}{n+1\\choose x}a^{(n+1)-x}b^x \\end{align*}\\] This completes both the inductive step and the proof. \\(ab^n={n\\choose n}a^{n-n+1}b^n\\) which is the term for \\(x=n\\) in the first summation. \\(a^nb={n\\choose 0}a^{n-0}b^1\\) which is the term for \\(x=0\\) in the second summation. \\(\\sum\\limits_{x=0}^{n-1}{n\\choose x}a^{n-x}b^{x+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-(x-1)}b^{(x-1)+1} \\\\ \\ \\ \\ \\ = \\sum\\limits_{x=1}^{n}{n\\choose x-1}a^{n-x+1}b^x\\) This step is made using Pascal’s Rule with \\(n=n-1\\). \\(a^{n+1}={n+1\\choose 0}a^{(n+1)-0}b^0\\) which is the term for \\(x=0\\) in the summation. \\(\\ \\ b^{n+1}={n+1\\choose n+1}a^{(n+1)-(n+1)}b^{n+1}\\) which is the term for \\(x=n+1\\) in the summation 5.2 General Approach 5.2.1 A Binomial Expansion Theorem This theorem and its corrolary are provided by Brunette. For any positive integer \\(n\\), let \\(B_n = (x_1+y_1) (x_2+y_2) \\cdots (x_n+y_n)\\). In the expansion \\(B_n\\), before combining possible like terms, the following are true: There will be \\(2^n\\) terms. Each of these terms will be a product of \\(n\\) factors. In each such product there will be one factor from each binomial (in \\(B_n\\)). Every such product of \\(n\\) factors, one from each binomial, is represented in the expansion. Proof: Proof is done by induction. For the case \\(n=1\\), the result is clear. Now assume that the theorem is true for a particular \\(n\\) and consider \\(B_{n+1}\\). \\[ B_{n+1} = B_n(x_{n+1} + y_{n+1}) = B_nx_{n+1} + B_ny_{n+1} \\] By the inductive assumption, \\(B_n = T_1 + T_2 + \\cdots + T_{2^n}\\) where each \\(T_i\\) is a product of \\(n\\) factors, one factor from each binomial. It follows that every term in the expansion of \\(B_n+1\\) is either of the type \\(T_ix_{n+1}\\) or \\(T_iy_{n+1}\\), for some \\(1\\leq i \\leq 2^n\\). But each term of either of the above types is clearly a product of \\(n+1\\) factors with one factor coming from each binomial. thus, if (ii) and (iii) are true for \\(B_n\\), then they are true for \\(B_n+1\\). Next, by the inductive assumption, the expansion of \\(B_n\\) is a sum of \\(2^n+2^n\\) terms, i.e., \\(2^{n+1}\\) terms. This completes the inductive step for (i). Lastly, it remains for us to consider a product of the type \\(p_1 p_2 \\cdots p_n p_{n+1}\\) where, for each \\(1\\leq i\\leq n+1\\), \\(p_i = x_i\\) or \\(p_i = y_i\\). By the inductive hypothesis, \\(p_1 p_2 \\cdots p_n\\) is a term in the expansion of \\(B_n\\). If \\(p_{n+1} = x_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_nx_{n+1}\\), and so of \\(B_{n+1}\\). Likewise, if \\(p_{n+1}=y_{n+1}\\), then \\(p_1 p_2 \\cdots p_n p_{n+1}\\) is a term in the expansion of \\(B_n y_{n+1}\\), and so of \\(B_{n+1}\\). This completes the inductive step and the proof. 5.2.2 Corollary: Binomial Theorem Let \\(x\\) and \\(y\\) be constants and let \\(n\\) be any positive integer. Then \\(\\displaystyle (x+y)^n = \\sum\\limits_{i=0}^{n} {n\\choose i} x^{n-i} y^i\\\\\\) Proof: Since each term in the expansion will have \\(n\\) terms, each term must follow the form \\(x^{n-i} y^i\\) for \\(0 \\leq i \\leq n\\), and in all, there are \\(2^n\\) such terms. For any given value of \\(i\\), the number of terms of the form \\(x^{n-i}y^i\\) is clearly the number of ways one can choose the \\(i\\) factors of \\(y\\) from the \\(n\\) available binomials, i.e., \\({n\\choose i}\\), which gives \\[(x+y)^n = \\sum\\limits_{i=0}^{n}{n\\choose i} x^{n-i} y^i\\] 5.3 Other Theorems 5.3.1 Theorem \\[{N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n}\\] where \\(0 \\leq n \\leq N_1 + N_2\\). Proof: Using the Binomial Theorem we establish \\[ (1+a)^{N-1} (1+a)^{N_2} = (1+a)^{N_1+N_2} \\\\ \\Rightarrow [{N_1\\choose 0}a^0+\\cdots+{N_1\\choose N_1}a^{N_1}]\\cdot [{N_2\\choose 0}a^0+\\cdots+{N_2\\choose N_2}a^{N_2}] \\\\ \\ \\ \\ \\ ={N_1+N_2\\choose 0}+{N_1+N_2\\choose 1}a+\\cdots +{N_1+N_2\\choose N_1+N_2}a^{N_1+N_2} \\] Expanding the left side of the equation gives \\[ {N_1\\choose 0}{N_2\\choose 0} + {N_1\\choose 0}{N_2\\choose 1}a + \\cdots + {N_1\\choose 0}{N_2\\choose N_2}a^{N_2} + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose 1}{N_2\\choose N_2}a^{N_2+1} + \\cdots + {N_1\\choose N_1}{N_2\\choose 0}a^{N_1} + {N_1\\choose N_1}{N_2\\choose 1}a^{N_1+1} \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\\\ = {N_1\\choose 0}{N_2\\choose 0}+{N_1\\choose 0}{N_2\\choose 1}a + {N_1\\choose 1}{N_2\\choose 0}a \\\\ \\ \\ \\ \\ + {N_1\\choose 0}{N_2\\choose 2}a^2+{N_1\\choose 1}{N_2\\choose 1}a^2 + {N_1\\choose 2}{N_2\\choose 0}a^2 \\\\ \\ \\ \\ \\ + \\cdots + {N_1\\choose N_1}{N_2\\choose N_2}a^{N_1+N_2} \\] Notice that for any \\(n\\) where \\(0 \\leq n \\leq N_1 + N_2\\), the coefficient for \\(a^n\\), found by combining like terms, is \\({N_1\\choose 0}{N_2\\choose n} + {N_1\\choose 1}{N_2\\choose n-1} + \\cdots+{N_1\\choose n-1}{N_2\\choose 1} + {N_1\\choose 0}{N_2\\choose n}\\) and, by the equivalence of the first equation in the proof, is equal to the coefficient \\({N_1 + N_2\\choose n}\\). 5.3.2 Theorem \\[\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}} = 1\\] for \\(0 \\leq n \\leq N_1 + N_2\\).\\ Proof: Theorem 5.3.1 establishes the equality \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1} + \\cdots + {N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i} = {N_1+N_2\\choose n} \\\\ \\Rightarrow\\frac{\\sum\\limits_{i=1}^{n} {N_1\\choose i}{N_2\\choose n-i}} {{N_1+N_2\\choose n}} = 1 \\] "],
["chebychevs-theorem.html", "6 Chebychev’s Theorem 6.1 Chebychev’s Theorem 6.2 Alternate Proof of Chebychev’s Theorem 6.3 Chebychev’s Theorem for Absolute Deviation", " 6 Chebychev’s Theorem 6.1 Chebychev’s Theorem In any finite set of numbers and for any real number \\(h &gt; 1\\), at least \\((1 - \\frac{1}{h^2}) \\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Under these conditions we know \\[|x_1-\\mu| &gt; h\\sigma,\\ |x_2-\\mu| &gt; h\\sigma, \\ldots,\\ |x_r-\\mu| &gt; h\\sigma\\] Squaring gives \\[(x_1-\\mu)^2 &gt; h^2\\sigma^2,\\ (x_2-\\mu)^2 &gt; h^2\\sigma^2,\\ldots,\\ (x_r-\\mu)^2 &gt; h^2\\sigma^2\\\\ \\ \\ \\ \\ \\Rightarrow\\sum\\limits_{i=1}^{r}(x_1-\\mu)^2 &gt; \\sum\\limits_{i=1}^{r}h^2\\sigma^2 = rh^2\\sigma^2 \\] Since all \\((x_i-\\mu)^2\\) must necessarily be positive, \\[\\begin{align*} \\sum\\limits_{i=1}^{r}(x_i-\\mu)^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2\\sigma^2 &amp;&lt; \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2 \\\\ \\ \\ \\ \\ ^{[1]} \\Rightarrow rh^2\\sigma^2 &amp;&lt; n\\sigma^2 \\\\ \\ \\ \\ \\ \\Rightarrow rh^2 &amp;&lt; n \\\\ \\ \\ \\ \\ \\Rightarrow\\frac{r}{n} &amp;&lt; \\frac{1}{h^2} \\end{align*}\\] \\(\\sigma^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) \\(\\ \\ \\ \\ \\Rightarrow n\\sigma^2 = \\sum\\limits_{i=1}^{n}(x_i-\\mu)^2\\) and \\(\\frac{r}{n}\\) is the fraction of numbers outside \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). By the law of complements, the fraction of numbers inside the interval is \\(1 - \\frac{r}{n}\\), which implies \\(1 - \\frac{r}{n} &gt; 1 - \\frac{1}{h^2}\\). Thus, more than \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the points lie within \\(h\\) standard deviations of the mean, or within the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\). 6.2 Alternate Proof of Chebychev’s Theorem In any finite set of numbers and for any real number \\(h&gt;1\\), at least \\((1-\\frac{1}{h^2})\\cdot 100\\%\\) of the numbers lie within \\(h\\) standard deviations of the mean. In other words, they lie within the interval \\((\\mu-h\\cdot\\sigma,\\mu+h\\cdot\\sigma)\\).\\ Proof: The proof here is done for the discrete case, but is applicable also in the continuous case by replacing the summations with integrals (with integrals, the limits will be from \\(-\\infty\\) to \\(\\infty\\)). \\[\\begin{align*} \\sigma^2 &amp;= E(x-\\mu)^2 \\\\ &amp;= \\sum\\limits_{y=0}^{\\infty}(y-\\mu)^2p(y) \\\\ &amp;= \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu-h\\sigma+1}^{\\mu+h\\sigma-1}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y) \\\\ ^{[1]} \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}(y-\\mu)^2p(y) + \\sum\\limits_{y=\\mu+h\\sigma}^{\\infty}(y-\\mu)^2p(y)\\\\ \\end{align*}\\] Since all the \\((y-\\mu)^2\\) must be positive, removing the middle term will surely result in this inequality. In both of these summations \\(y\\) is outside the interval \\((\\mu-h\\cdot\\sigma , \\mu+h\\cdot\\sigma)\\), so \\[\\begin{align*} |y-\\mu| &amp;\\geq h\\sigma \\\\ \\Rightarrow (y-\\mu^2) &amp;\\geq h^2\\sigma^2 \\\\ \\Rightarrow \\sigma^2 &amp;\\geq \\sum\\limits_{y=0}^{\\mu-h\\sigma}h^2\\sigma^2p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}h^2\\sigma^2p(y) \\\\ \\Rightarrow\\sigma^2 &amp;\\geq h^2\\sigma^2\\Big[\\sum\\limits_{y=0}^{\\mu-h\\sigma}p(y) + \\sum\\limits_{\\mu+h\\sigma}^{\\infty}p(y)\\Big] \\end{align*}\\] The first summation is the sum of all probabilities that \\(y-\\mu &lt; h\\sigma\\), i.e. \\(P(y-\\mu &lt; h\\sigma)\\). Likewise, the second summation is \\(P(y-\\mu &gt; h\\sigma)\\). \\[\\begin{align*} \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(y-\\mu&lt;h\\sigma) + P(y-\\mu&gt;h\\sigma)] \\\\ \\Rightarrow \\sigma^2 &amp;\\geq h^2\\sigma^2[P(|y-\\mu|&gt;h\\sigma)] \\\\ \\Rightarrow \\frac{1}{h^2} &amp;\\geq P(|y-\\mu|&gt;h\\sigma) \\\\ \\Rightarrow 1-\\frac{1}{h^2} &amp;\\leq P(|y-\\mu|&gt;h\\sigma) \\end{align*}\\] 6.3 Chebychev’s Theorem for Absolute Deviation This theorem is provided by Brunette In any finite set of numbers, and for any real number \\(h &gt; 1\\), at least \\(1 - \\frac{1}{h}\\) of the numbers lie within \\(h\\) absolute deviations of the mean, where the absolute deviation is defined \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}{n}|x_i-\\bar x|\\). In other words, \\(1-\\frac{1}{h}\\) of the numbers are in the interval \\((\\bar x-h\\cdot Ab , \\bar x+h\\cdot Ab)\\). Proof: For a set \\(\\{x_1,x_2,\\ldots,x_r,x_{r+1},\\ldots,x_n\\}\\) where, by choice of labeling, \\(\\{x_1,x_2,\\ldots,x_r\\}\\) lie outside of \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). Also, \\(\\{x_{r+1},\\ldots,x_n\\}\\) are within the interval. Accordingly, \\[h \\cdot Ab \\leq |x_1-\\bar x| ,\\ h \\cdot Ab \\leq |x_1-\\bar x| ,\\ldots ,\\ h \\cdot Ab \\leq |x_1-\\bar x| \\] \\[\\begin{align*} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{r}|x_i-\\bar x| \\\\ \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq \\sum\\limits_{i=1}^{n}|x_i-\\bar x| \\\\ ^{[1]} \\Rightarrow r \\cdot h \\cdot Ab &amp;\\leq n \\cdot Ab\\\\ \\Rightarrow \\frac{r}{n} &amp;\\leq \\frac{1}{h}\\\\ \\Rightarrow -\\frac{r}{n} &amp;\\geq -\\frac{1}{h}\\\\ \\Rightarrow 1-\\frac{r}{n} &amp;\\geq 1-\\frac{1}{h} \\end{align*}\\] \\(Ab = \\frac{1}{n}\\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) \\(\\Rightarrow n \\cdot Ab = \\sum\\limits_{i=1}^{n}|x_i-\\bar x|\\) Now \\(\\frac{r}{n}\\) is the fraction of numbers outside the interval. So \\(1-\\frac{r}{n}\\) is the fraction of numbers within \\(h\\) absolute deviations of the mean, or within the interval \\((\\mu-h\\cdot Ab , \\mu+h\\cdot Ab)\\). "],
["chi-square-distribution.html", "7 Chi-Square Distribution 7.1 Probability Distribution Function 7.2 Cumulative Distribution Function 7.3 Expected Values 7.4 Moment Generating Function 7.5 Maximum Likelihood Function 7.6 Theorems for the Chi-Square Distribution", " 7 Chi-Square Distribution 7.1 Probability Distribution Function A random variable \\(X\\) is said to have a Chi-Square Distribution with parameter \\(\\nu\\) if its probability distribution function is \\[f(x) = \\left\\{ \\begin{array}{ll} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} &amp; 0&lt;x,\\ 0&lt;\\nu\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\(\\nu\\) is commonly referred to as the degrees of freedom. 7.2 Cumulative Distribution Function The cumulative distribution function for the Chi-Square Distribution cannot be written in closed form. It’s integral form is expressed as \\[ F(x) = \\left\\{ \\begin{array}{ll} \\displaystyle\\int\\limits_{0}^{x} \\frac{t^{\\frac{\\nu}{2}-1}e^{-\\frac{t}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} dt &amp; 0&lt;x,\\ 0&lt;\\nu\\\\\\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:ChiSquare_Distribution)The graphs on the top and bottom depict the Chi-Square probability distribution and cumulative distribution functions, respectively, for \\(\\nu=4,7,10\\). As \\(\\nu\\) gets larger, the distribution becomes flatter with thicker tails. 7.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma\\Big(\\frac{\\nu}{2}+1\\Big)2^{\\frac{\\nu}{2}+1}\\Big] \\\\ &amp;= \\frac{\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+1}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{2\\nu}{2} \\\\ &amp;= \\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}+1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Gamma(\\frac{\\nu}{2}+2)2^{\\frac{\\nu}{2}+2}\\Big] \\\\ &amp;= \\frac{\\Gamma\\Big(\\frac{\\nu}{2}+2\\Big)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{(\\frac{\\nu}{2}+1)\\Gamma(\\frac{\\nu}{2}+1)2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\frac{\\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\Gamma(\\frac{\\nu}{2})2^{\\frac{\\nu}{2}+2}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= \\Big(\\frac{\\nu}{2}+1\\Big)\\frac{\\nu}{2}\\cdot 2^2=2\\Big(\\frac{\\nu}{2}+1\\Big)\\nu \\\\ &amp;= (\\nu+2)\\nu=\\nu^2+2\\nu \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}e^{tx}\\cdot x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx}e^{-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{tx-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{2tx}{2}-\\frac{x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-\\frac{2tx-x}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{-2t+1}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{-x\\frac{1-2t}{2}}dx \\\\ &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1} e^{\\frac{-x}{\\frac{2}{1-2t}}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[\\Big(\\frac{2}{1-2t}\\Big)^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big]\\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{\\nu}{2}}} \\\\ &amp;= (1-2t)^{-\\frac{\\nu}{2}} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\frac{\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1}(-2) \\\\ &amp;= \\frac{2\\nu}{2}(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-2t)^{-\\frac{\\nu}{2}-1} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= (-\\frac{\\nu}{2}-1)\\nu(1-2t)^{-\\frac{\\nu}{2}-2}(-2) \\\\ &amp;= (\\frac{2\\nu}{2}+2)\\nu(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu+2)\\nu)(1-2t)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-2t)^{-\\frac{\\nu}{2}-2}\\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= \\nu(1-2\\cdot 0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1-0)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu(1)^{-\\frac{\\nu}{2}-1} \\\\ &amp;= \\nu \\\\ M_X^{(2)}(0) &amp;= (\\nu^2+2\\nu)(1-2\\cdot 0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1-0)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu)(1)^{-\\frac{\\nu}{2}-2} \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\nu\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= (\\nu^2+2\\nu) \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\nu \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= \\nu^2+2\\nu-\\nu^2 \\\\ &amp;= 2\\nu \\end{align*}\\] 7.5 Maximum Likelihood Function Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from a Chi-square distribution with parameter \\(\\nu\\). 7.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\nu/2-1}e^{-x_1/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdot \\frac{x_2^{\\nu/2-1}e^{-x_2/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\cdots \\frac{x_n^{\\nu/2-1}e^{-x_n/2}}{2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\nu/2-1}e^{-x_i/2}} {2^{\\nu/2}\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}e^{-x_i/2} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\sum\\limits_{i=1}^{n}\\frac{x_i}{2} \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\\\ &amp;= \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\end{align*}\\] 7.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\big(L(\\theta)\\big) \\\\ &amp;= \\ln\\Bigg[ \\bigg(2^{\\nu/2}\\Gamma\\Big(\\frac{\\nu}{2}\\Big)\\bigg) \\cdot \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\cdot \\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1} \\Bigg] \\\\ &amp;= \\ln\\Bigg[ \\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) \\Bigg] + \\ln\\Bigg( \\exp\\bigg\\{ \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\Bigg) + \\ln\\bigg(\\prod\\limits_{i=1}^{n}x_i^{\\nu/2-1}\\bigg) \\\\ &amp;= -n \\ln\\bigg( 2^{\\nu/2}\\Gamma \\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i \\bigg) \\\\ &amp;= -n\\bigg( \\ln(2^{\\nu/2}) + \\Gamma\\Big(\\frac{\\nu}{2}\\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\bigg(\\frac{\\nu}{2} \\ln 2 + \\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) \\bigg) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n\\nu}{2} \\ln 2 - n\\ln \\Gamma\\Big( \\frac{\\nu}{2} \\Big) + \\frac{1}{2}\\sum\\limits_{i=1}^{n}x_i + \\bigg( \\frac{\\nu}{2}-1 \\bigg) \\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 7.5.3 MLE for \\(\\nu\\) \\[\\begin{align*} \\frac{d\\ell}{d\\nu} &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) \\cdot \\frac{1}{2} + 0 + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{2} \\ln 2 - \\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big) + \\frac{1}{2} \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\Rightarrow \\frac{n}{2} \\ln 2 - \\frac{1}{2}\\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{2\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow n\\ln 2 - \\sum\\limits_{i=1}^{n}\\ln x_i &amp;= -\\frac{n}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\Gamma^\\prime\\Big(\\frac{\\nu}{2}\\Big)\\\\ \\Rightarrow \\frac{\\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln 2}{n} &amp;= \\frac{\\Gamma^\\prime\\big(\\frac{\\nu}{2}\\big)}{\\Gamma\\big(\\frac{\\nu}{2}\\big)} \\end{align*}\\] Due to the complexity of the Gamma function in this equation, no solution can be developed for \\(\\nu\\) in closed form. Thus, we have to rely on numerical methods to obtain a solution to the equation and find the maximum likelihood estimator. 7.6 Theorems for the Chi-Square Distribution 7.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}} {2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}dx &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\int\\limits_{0}^{\\infty}x^{\\frac{\\nu}{2}-1}e^{-\\frac{x}{2}}dx \\\\ ^{[1]} &amp;= \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})} \\Big[2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})\\Big] \\\\ &amp;= \\frac{2^{\\frac{\\nu}{2}}\\Gamma(\\frac{\\nu}{2})}{2^{\\frac{\\nu}{2}} \\Gamma(\\frac{\\nu}{2})} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 7.6.2 Sum of Chi-Square Random Variables Let \\(X_1 , X_2 , \\ldots , X_n\\) be independent Chi-Square random variables with parameter \\(\\nu_i\\), that is \\(X_i\\sim\\chi^2(\\nu_i),\\ i=1,2,\\ldots,n\\). Suppose \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\chi^2(\\sum\\limits_{i=1}^{n}\\nu_i)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}=E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-2t)^{-\\frac{\\nu_1}{2}}(1-2t)^{-\\frac{\\nu_2}{2}}\\cdots (1-2t)^{-\\frac{\\nu_n}{2}} \\\\ &amp;= (1-2t)^{\\sum\\limits_{i=1}^{n}\\nu_i} \\end{align*}\\] Which is the mgf of a Chi-Square random variable with parameter \\(\\sum\\limits_{i=1}^{n}\\nu_i\\). Thus \\(Y\\sim\\chi^2\\bigg(\\sum\\limits_{i=1}^{n}\\nu_i\\bigg)\\). 7.6.3 Square of a Standard Normal Random Variable If \\(Z\\sim N(0,1)\\), then \\(Z^2\\sim\\chi^2(1)\\). Proof: \\[\\begin{align*} M_{Z^2}(t) &amp;= E(e^{tZ^2}) \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}e^{tz^2}\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty}e^{tz^2} e^{-\\frac{z^2}{2}}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(-2t+1)}dz \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}\\int\\limits_{-\\infty}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[1]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty} e^{-\\frac{z^2}{2}(1-2t)}dz \\\\ ^{[2]} &amp;= \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty}e^{-u} \\frac{\\sqrt{2}u^{-\\frac{1}{2}}}{2(1-2t)^{\\frac{1}{2}}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}e^{-u}u^{-\\frac{1}{2}}du \\\\ &amp;= \\frac{2\\sqrt{2}}{2\\sqrt{2\\pi}(1-2t)^{\\frac{1}{2}}} \\int\\limits_{0}^{\\infty}u^{\\frac{1}{2}-1}e^{-u}du \\\\ ^{[3]} &amp;= \\frac{1}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}}\\Gamma(\\frac{1}{2}) \\\\ &amp;= \\frac{\\sqrt{\\pi}}{\\sqrt{\\pi}(1-2t)^{\\frac{1}{2}}} \\\\ &amp;= \\frac{1}{(1-2t)^{\\frac{1}{2}}}=(1-2t)^{-\\frac{1}{2}} \\\\ \\end{align*}\\] \\(\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\) when f(x) is an even function () Let \\(u=\\frac{z^2}{2}(1-2t) \\ \\ \\ \\ \\Rightarrow z=\\frac{\\sqrt{2}u^{\\frac{1}{2}}}{(1-2t)^{\\frac{1}{2}}}\\) So \\(dz=\\frac{\\sqrt{2}u^{-\\frac{1}{2}}} {2(1-2t)^{\\frac{1}{2}}}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) Which is the mgf of a Chi-Square random variable with 1 degree of freedom. Thus \\(Z^2\\sim\\chi^2(1)\\). "],
["combinations.html", "8 Combinations", " 8 Combinations 8.0.1 Lemma A set of \\(n\\) elements may be partitioned into \\(m\\) distinct groups containing \\(k_1 , k_2 , \\ldots , k_m\\) objects, respectively, where each object appears in exactly one group and \\(\\sum\\limits_{i=1}^{m}k_i=n\\), in \\(\\displaystyle N={n\\choose k_1k_2\\ldots k_m}=\\frac{n!}{k_1!k_2!\\ldots k_m!}\\) ways.\\ Proof: \\(N\\) is the number of ways all \\(n\\) of the elements of the set can be arranged in \\(m\\) groups where the order within each group is not important (i.e. rearrangements of elements in a group do not qualify as distinct groups). The number of distinct arrangements of the \\(n\\) elements in which the order of selection is important, \\(P_k^n\\), is equal to \\(N\\) multiplied by the number of ways each individual group of \\(k_i\\) can be selected in which the order is important, i.e. \\[\\begin{align*} P_n^n &amp;= N \\cdot P_{k_1}^{k_1} P_{k_2}^{k_2} \\cdots P_{k_m}^{k_m} \\\\ \\Rightarrow n! &amp;= N \\cdot k_1! k_2! \\cdots k_m! \\\\ \\Rightarrow N &amp;= \\frac{n!}{k_1! k_2! \\cdots k_m!} \\end{align*}\\] 8.0.2 Combinations Theorem Given a set of \\(n\\) elements, the number of possible ways to select a subset of size \\(k\\), without regard to the order of their selection, is \\(\\frac{n!}{k!(n-k)!}\\).\\ Proof: This theorem is a special case of the Lemma with \\(n=n\\), \\(m=2\\), \\(k_1=k\\) and \\(k_2=n-k\\). thus, \\[\\displaystyle N=\\frac{n!}{k!(n-k)!}\\] The formula \\(\\displaystyle \\frac{n!}{k!(n-k)!}\\) is denoted in a number of ways, depending on the author. Denotations may be \\(C_k^n\\), \\(_nC_k\\), \\(C_{n,k}\\), \\(C(n,k)\\), and \\({n\\choose k}\\). Throughout this book, the form \\({n\\choose k}\\) is used and may be read “\\(n\\) choose \\(k\\) objects.” 8.0.3 Theorem For any integer \\(a\\) such that \\(0\\leq a\\leq k\\), \\[ {n\\choose k} = \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)}{n-a\\choose k-a} \\] Proof: \\[\\begin{align*} {n\\choose k} &amp;= \\frac{n!}{k!(n-k)!} \\\\ &amp;= \\frac{n(n-1)!}{k(k-1)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)!}{k(k-1)(k-2)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)(n-a)!}{k(k-1)(k-2)\\cdots(k-a+1)(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)!(n-a+a-k)!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)+(a-k)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot \\frac{(n-a)!}{(k-a)![(n-a)-(k-a)]!} \\\\ &amp;= \\frac{n(n-1)(n-2)\\cdots(n-a+1)}{k(k-1)(k-2)\\cdots(k-a+1)} \\cdot {n-a\\choose k-a} \\end{align*}\\] "],
["correlation-pearsons.html", "9 Correlation (Pearson’s) 9.1 Theorems on Pearson’s Correlation 9.2 Computational Formula for \\(\\rho\\)", " 9 Correlation (Pearson’s) Pearson’s correlation coefficient of the variables \\(X\\) and \\(Y\\) is a measure of the linear relationship between \\(X\\) and \\(Y\\). It is defined \\[\\rho = \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\cdot \\sigma_Y^2}}\\] Notice that if \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y,)=0\\) and \\(\\rho=0\\) and there is no linear relationship between the variables. 9.1 Theorems on Pearson’s Correlation 9.2 Computational Formula for \\(\\rho\\) \\[\\rho = \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\sum\\limits_{i=1}^{n}(x_i-\\mu_X)\\sum\\limits_{j=1}^{m}(y_i-\\mu_Y)}\\] Proof: \\[\\begin{align*} \\rho &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{Cov(X,Y)}{\\sqrt{\\sigma_X^2\\sigma_Y^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)\\frac{1}{N}} {\\sqrt{\\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2}{N}\\frac{\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}{N}}} \\\\ &amp;= \\frac{\\frac{1}{N}\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_j-\\mu_Y)} {\\frac{1}{N}\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)^2\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)(y_i-\\mu_Y)} {\\sqrt{\\sum\\limits_{i=1}^{N}(x_i-\\mu_X)\\sum\\limits_{i=1}^{N}(y_i-\\mu_Y)}} \\end{align*}\\] "],
["covariance.html", "10 Covariance 10.1 Definition of Covariance 10.2 Theorems on Covariance", " 10 Covariance 10.1 Definition of Covariance For any two random variables \\(X\\) and \\(Y\\), the covariance of \\(X\\) and \\(Y\\) is defined as \\[Cov(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)]\\] 10.2 Theorems on Covariance 10.2.1 Theorem Let \\(X\\) be a random variable. Then \\[Cov(X,X) = V(X)\\] Proof: \\[\\begin{align*} Cov(X,X) &amp;= E[(X-\\mu)(X-\\mu)] \\\\ &amp;= E[(X-\\mu)^2] \\\\ &amp;= V(X) \\end{align*}\\] 10.2.2 Theorem Let \\(X\\) and \\(Y\\) be random variables. Then \\[Cov(X,Y) = E(XY)-E(X)E(Y)\\] Proof: \\[\\begin{align*} Cov(X,Y) &amp;= E[(X-\\mu_x)(Y-\\mu_Y)] \\\\ &amp;= E[XY - X\\mu_y - Y\\mu_X + \\mu_X\\mu_Y] \\\\ &amp;= E(XY) - E(X)\\mu_Y - \\mu_XE(Y) + \\mu_X\\mu_Y \\\\ &amp;= E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - 2E(X)E(Y) + E(X)E(Y) \\\\ &amp;= E(XY) - E(X)E(Y) \\end{align*}\\] 10.2.3 Covariance Let \\(X\\) and \\(Y\\) be random variables and let \\(a\\) and \\(b\\) be constants. Then \\[Cov(aX,bY) = abCov(X,Y)\\] Proof: \\[\\begin{align*} Cov(aX,bY) &amp;= E(aXbY) - E(aX)E(bY) \\\\ &amp;= abE(XY) - abE(X)E(Y) \\\\ &amp;= ab[E(XY) - E(X)E(Y)] \\\\ &amp;= abCov(X,Y) \\end{align*}\\] 10.2.4 Theorem Let \\(X_1 , X_2 , \\ldots , X_n\\) be random variables with \\(E(X_i) = \\mu_i\\) for \\(i = 1,2,\\ldots,n\\) and let \\(Y_1,Y_2,\\ldots,Y_m\\) be random variables with \\(E(Y_j) = \\phi_j\\) for \\(j=1,2,\\ldots,m\\). Also, let \\(a_1,a_2,\\ldots,a_n\\) and \\(b_1,b_2,\\ldots,b_m\\) be constants.\\ If \\(U_1 = \\sum\\limits_{i=1}^{n}a_iX_i\\) and \\(U_2 = \\sum\\limits_{i=1}^{m}b_iY_i\\), then \\[Cov(U_1,U_2) = \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jCov(X_i,Y_j)\\] Proof: \\[\\begin{align*} Cov(U_1,U_2) &amp;= E[(U_1-E(U_1))(U_2-E(U_2))] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-\\sum\\limits_{i=1}^{n}a_i\\mu_i\\big) \\big(\\sum\\limits_{j=1}^{m}b_jY_j-\\sum\\limits_{j=1}^{m}b_j\\phi_j\\big)\\Big] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_i(X_i-\\mu_i)\\big)\\big(\\sum\\limits_{j=1}^{m}b_j(Y_j-\\phi_j)\\big)\\Big] \\\\ &amp;= E\\Big[\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j(X_i-\\mu_i)(Y_j-\\phi_j)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_jE[(X_i-\\mu_i)(Y_j-\\phi_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_j\\ Cov(X_i,Y_j) \\end{align*}\\] 10.2.5 Theorem Let \\(X_1,X_2,\\ldots,X_n\\) be random variables with \\(E(X_i)=\\mu_i\\) for \\(i=1,2,\\ldots,n\\) and let \\(a_1,a_2,\\ldots,a_n\\) be constants. If \\(Y = \\sum\\limits_{i=1}^{n}a_iX_i\\) then \\[V(Y) = \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ }a_ia_jCov(X_i,X_j)\\] Proof: \\[\\begin{align*} V(Y) &amp;= E[(Y-\\mu_Y)^2] \\\\ &amp;= E[(Y-\\mu_Y)(Y-\\mu_Y)] \\\\ &amp;= E\\Big[\\big(\\sum\\limits_{i=1}^{n}a_iX_i-a_i\\mu_i\\big) \\big(\\sum\\limits_{n=1}^{n}a_jX_j-a_j\\mu_j\\big)\\Big] \\\\ &amp;= \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2Cov(X_i,X_i)+ \\sum\\limits_{\\ \\ i\\neq}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jE[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i^2V(X_i)+ 2\\sum\\limits_{\\ \\ i&lt;}\\sum\\limits_{j\\ \\ \\ \\ }a_ia_jCov(X_i,X_j) \\end{align*}\\] "],
["experimental-designs.html", "11 Experimental Designs 11.1 Designs in Categorical Data Analysis", " 11 Experimental Designs 11.1 Designs in Categorical Data Analysis Studies in Categorical Data Analysis can be classified into several designs. These designs fall into the following two categories: Retrospective Design: looks at and analyzes measurements that have already been taken. Prospective Design: specifies the measurements to be collected at a future time. 11.1.1 Case Control Study In case control studies, the marginal distribution of the response variable is fixed by the sampling design. In other words, researchers select particular numbers of each category of the response variable in order to ensure that enough of each case are included in the sample. The result is that the marginal distribution of the response is non-random. Unfortunately, in order to calculate conditional probabilities, the marginal distribution of interest must be random. The difference of proportions for the response and the relative risk are both based on the marginal distribution of the response, and are both invalid procedures in case-control studies. In taking the measurements, researchers idenitfy people who are already classified into the response variable, making the design retrospective. 11.1.2 Cross Sectional Study 11.1.3 Cohort Study In Cohort Studies, subjects make their own choice about which group in the explanatory variable to join and researchers monitor the subjects with respect to a response variable over a period of time. Both the explanatory and response variables are random and only the total sample size is fixed by the researcher. Thus, conditional probabilities may be computed for both the predictor and response variables; differences in proportions may be estimated; and the relative risk is defined for the response variable. Since subjects select the group in which they will be and a measurement of their response is taken later, cohort studies are prospective. 11.1.4 Randomized Study In randomized Studies, the researcher randomly assigns subjects to the explanatory variable and then observes their response (making this a prospective study). The marginal distribution of the explanatory variable is therefore fixed, and conditional probabilities may not be computed. The response variable, on the other hand, is random and conditional probabilites may be computed, as well as the difference of proportions and relative risk. 11.1.5 Summary of Designs conditional conditional probability probability difference of Relative Odds explanatory response proportions Risk Ratio Case Control xxx xxx Cross-Sectional xxx xxx xxx xxx xxx Cohort xxx xxx xxx xxx xxx Randomized xxx xxx xxx xxx "],
["exponential-distribution.html", "12 Exponential Distribution 12.1 Probability Distribution Function 12.2 Cumulative Distribution Function 12.3 Expected Values 12.4 Moment Generating Function 12.5 Maximum Likelihood Estimator 12.6 Theorems for the Exponential Distribution", " 12 Exponential Distribution 12.1 Probability Distribution Function A random variable is said to have an Exponential Distribution with parameter \\(\\beta\\) if its probability distribution function is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{\\beta}e^{\\frac{-x}{B}}, &amp; 0&lt;x,\\ \\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 12.2 Cumulative Distribution Function \\[\\begin{align*} F(x) &amp;= \\int\\limits_{0}^{x}\\frac{1}{\\beta}\\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}dt \\\\ &amp;= \\exp\\Big\\{{\\frac{-t}{\\beta}}\\Big\\}\\Big|_0^x \\\\ &amp;= \\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\}-\\exp\\Big\\{{\\frac{-0}{\\beta}}\\Big\\} \\\\ &amp;= \\exp\\Big\\{{\\frac{0}{\\beta}}\\Big\\}-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\\\ &amp;= 1-\\exp\\Big\\{{\\frac{-x}{\\beta}}\\Big\\} \\end{align*}\\] And so the cumulative distribution function is given by \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{\\frac{-x}{\\beta}}, &amp; 0&lt;x,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:Exponential_Distribution)The figures on the top and bottom display the Exponential probability and cumulative distirubtion functions, respectively, for \\(\\beta=1,3\\). 12.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}xf(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}xe^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{2-1}e^{\\frac{-x}{\\beta}}dx\\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^2\\Gamma(2)) \\\\ &amp;=\\frac{\\beta^2\\cdot 1!}{\\beta} \\\\ &amp;=\\beta \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2f(x)dx \\\\ &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^2e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}x^{3-1}e^{\\frac{-x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\beta}(\\beta^3\\Gamma(3)) \\\\ &amp;= \\frac{\\beta^3\\cdot 2!}{\\beta} \\\\ &amp;= 2\\beta^2 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2)-E(X)^2 \\\\ &amp;= 2\\beta^2-\\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 12.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx}e^{\\frac{-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{tx-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx}{\\beta}-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{\\beta tx-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}\\int\\limits_{0}^{\\infty}e^{\\frac{-x(\\beta 1-\\beta t}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\beta}(\\frac{-\\beta}{1-\\beta t})e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}e^{\\frac{-x(1-\\beta t}{\\beta}}|_0^\\infty \\\\ &amp;= \\frac{-1}{1-\\beta t}\\cdot 0-\\frac{-1}{1-\\beta t}e^0 \\\\ &amp;= \\frac{1}{1-\\beta t}=(1-\\beta t)^{-1} \\\\ \\\\ \\\\ M_X^{(1)}(t) &amp;= -1(1-\\beta t)^{-2}(-\\beta) \\\\ &amp;= \\beta(1-\\beta t)^{-2} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= -2\\beta(1-\\beta t)^{-3}(-\\beta) \\\\ &amp;= 2\\beta^2(1-\\beta t)^{-3} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\beta(1-\\beta\\cdot 0)^{-2} \\\\ &amp;= \\beta(1-0)^{-2} \\\\ &amp;= \\beta(1)^{-2} \\\\ &amp;= \\beta \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= 2\\beta^2(1-\\beta\\cdot 0)^{-3} \\\\ &amp;= 2\\beta^2(1-0)^{-3} \\\\ &amp;= 2\\beta^2(1)^{-3} \\\\ &amp;= 2\\beta^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= 2\\beta^2 - \\beta^2 \\\\ &amp;= \\beta^2 \\end{align*}\\] 12.5 Maximum Likelihood Estimator Let \\(x_1,x_2,\\ldots,x_n\\) be a random sample from an Exponential distribution with parameter \\(\\beta\\). 12.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= f(x_1|\\theta)f(x_2|\\theta)\\cdots f(x_n|\\theta)\\\\ &amp;= \\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_1}{\\theta}\\bigg\\} \\cdot\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\cdots\\frac{1}{\\theta}\\exp\\bigg\\{-\\frac{x_n}{\\theta}\\bigg\\} \\\\ &amp;= \\frac{1}{\\theta^n}\\exp\\bigg\\{-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\end{align*}\\] 12.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln(L(\\theta)) \\\\ &amp;= \\ln(1)-n\\ln(\\theta)-\\frac{1}{\\theta}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= 0-n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\\\ &amp;= -n\\ln(\\theta)-\\theta^{-1}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] 12.5.3 MLE for \\(\\beta\\) \\[\\begin{align*} \\frac{d\\ell(\\beta)}{d\\beta} &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n}{\\beta}+\\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n}{\\beta} &amp;= \\beta^2\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow\\frac{n\\beta^2}{\\beta} &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow n\\beta &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] So \\(\\hat\\beta=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) is the maximum likelihood estimator for \\(\\beta\\). 12.6 Theorems for the Exponential Distribution 12.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{B}}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{1}{\\beta}e^{\\frac{-x}{\\beta}}dx &amp;= -e^{\\frac{-x}{\\beta}}\\Big|_0^\\infty \\\\ &amp;= -e^{\\frac{-\\infty}{\\beta}}-(-e^{\\frac{-0}{\\beta}}) \\\\ &amp;= e^{\\frac{0}{\\beta}}-e^{\\frac{-\\infty}{\\beta}} \\\\ &amp;= 1-0 \\\\ &amp;= 1 \\end{align*}\\] 12.6.2 Sum of Exponential Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from an Exponential distribution with parameter \\(\\beta\\), i.e. \\(X_i\\sim\\)Exponential\\((\\beta)\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Gamma\\((n,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-1}(1-\\beta t)^{-1}\\cdots(1-\\beta t)^{-1} \\\\ &amp;= (1-\\beta t)^{-n} \\end{align*}\\] Which is the mgf for a Gamma random variable with parameters \\(n\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((n,\\beta)\\). "],
["functions.html", "13 Functions 13.1 Fundamental Concepts and Definitions 13.2 Identities and Inverses 13.3 Odd and Even Functions 13.4 Theorems", " 13 Functions 13.1 Fundamental Concepts and Definitions Much of this chapter is taken from the lectures of Dr. John Brunette, University of Southern Maine A function is a collection of ordered pairs in which no two pairs have the same first element. The set of all first members of the pairs is called the domain. The set of all second members of the pairs is called the range. Suppose now that for any function \\(f\\) we have two items \\(x\\) and \\(y\\) such that \\(x\\in dom(f)\\) and \\(y\\in ran(x)\\) where \\(dom(f)\\) and \\(ran(f)\\) denote the domain and range of \\(f\\), respectively. It is said that \\(f\\) maps \\(x\\) onto \\(y\\), written \\[f:\\ x\\mapsto y\\] It is common to write the \\(ran(f)\\) as some expression of \\(x\\). For example, \\(f: x\\mapsto x^2\\) takes each element in the domain, and pairs it with it’s square. The common shorthand for this is \\(f(x)=x^2\\), meaning that whatever appears between the parentheses following the \\(f\\) is to be squared. 13.1.1 Function Operations The three basic operations that can be performed on functions are addition, multipilication, and composition. For any two functions \\(f\\) and \\(g\\) these operations are defined as: Addition \\(\\lbrack f+g\\rbrack(x)=:\\big\\{\\big(x,f(x)+g(x)\\big)\\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Multiplication \\(\\lbrack f\\cdot g\\rbrack(x):=\\big\\{\\big(x,f(x)\\cdot g(x)\\big) \\mid x\\in dom(f)\\cap dom(g)\\big\\}\\) Composition \\(\\lbrack f\\circ g\\rbrack(x)=\\big\\{\\big(x,f\\big(g(x)\\big) \\mid x\\in dom(g)\\) and \\(g(x)\\in dom(f)\\big\\}\\) Notice that the composition \\(\\lbrack f\\circ g\\rbrack(x)=f \\circ g: g(x)\\mapsto f(x)\\). In other words, the result of \\(g\\) is then applied to \\(f\\) to produce the result of the composition. 13.2 Identities and Inverses Recall that addition and multiplication have identity properties. Specifically, for any real number \\(x\\), applying one of these identities returns the value \\(x\\), i.e. \\(x+0=x\\) and \\(x\\cdot 1\\)=x. Functions also have an identity, denoted \\(id(x)\\), that is defined as \\[id:\\ x\\mapsto x\\] Furthermore, the composition of \\(id\\) with \\(f\\) behaves in this way: \\[id\\circ f=f\\circ id=f\\] Functions may also exhibit the property of inverses that are exhibited by addition and multiplication. In the latter two, combining any real number \\(x\\) and its inverse returns the identity of that operation, i.e. \\(x+-x=0\\) and \\(x\\cdot x^{-1}=1,\\ x\\neq 0\\). Likewise, some functions have an inverse function. If a function \\(f\\) has an inverse \\(f^{-1}\\), then \\[f\\circ f^{-1}=f^{-1}\\circ f=id\\] On closer observation, we see \\[f^{-1}\\circ f\\big(dom(x)\\big)=f^{-1}\\Big(f\\big(dom(x)\\big)\\Big)=f^{-1}\\big(ran(x)\\big)=dom(x)\\] So \\(f^{-1}\\) must be the set of all ordered pairs \\((y,x)\\) where \\(x\\in dom(x)\\) and \\(y\\in ran(x)\\), i.e. \\(f^{-1}(x)=\\{(y,x) \\mid x\\in dom(x)\\) and \\(y\\in ran(x)\\}\\). By the definition of functions, no two first elements in \\(f^{-1}\\) can be the same. But the first elements in \\(f^{-1}\\) are the second elements in \\(f\\). So \\(f^{-1}\\) only exists if no two second elements in \\(f\\) are the same. We thus make the following definition: A function \\(f\\) is called a one-to-one function if it has no two ordered pairs with the same second element. For any one-to-one function \\(f\\), no two of the first elements are the same, and no two of the second elements are the same. Thus, \\(f^{-1}\\) is a function, because no two of its first elements are the same, and because the range of \\(f^{-1}\\) is the domain of \\(f\\), no two second elements in \\(f^{-1}\\) are the same, and \\(f^{-1}\\) is a one-to-one function. Thus, every one-to-one function has an inverse. If a function \\(f\\) is not one-to-one, however, then there exist two pairs in \\(f\\) that have the same second element. The inverse \\(f^{-1}\\) therefore has two pairs where the first element is the same. When such is the case, the definition of a function is violated, and \\(f^{-1}\\) cannot be a function. Thus, if a function is invertible, it must be one-to-one. 13.3 Odd and Even Functions A function is said to be even if for any real number \\(x,\\ f(-x)=f(x)\\). A function is said to be odd if for any real number \\(x,\\ f(-x)=-f(x)\\). If neither of these criteria are met, the function is simply said to be neither odd nor even. 13.4 Theorems 13.4.1 Operations on Even Functions Let \\(f\\) and \\(g\\) both be even functions. Then: \\([f+g](x)\\) is an even function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x)+g(-x) \\\\ &amp;= f(x)+g(x) \\\\ &amp;= [f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an even function. 13.4.2 Operations on Odd Functions Let \\(f\\) and \\(g\\) both be odd functions. Then: \\([f+g](x)\\) is an odd function \\([f\\cdot g](x)\\) is an even function \\([f\\circ g](x)\\) is an odd function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\\\ &amp;= -[f+g](x) \\end{align*}\\] so \\([f+g](x)\\) is an odd function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= -f(x)\\cdot -g(x) \\\\ &amp;= f(x)\\cdot g(x) \\\\ &amp;= [f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an even function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= -f\\big(g(x)\\big) \\\\ &amp;= -[f\\circ g](x) \\end{align*}\\] so \\([f\\circ g](x)\\) is an odd function. 13.4.3 Operations on an Odd and Even Function Let \\(f\\) be an even function and let \\(g\\) both be an odd function. Then: \\([f+g](x)\\) is neither an odd nor an even function \\([f\\cdot g](x)\\) is an odd function \\([f\\circ g](x)\\) is an even function \\([g\\circ f](x)\\) is an even function. Proof: \\[\\begin{align*} [f+g](-x) &amp;= f(-x) + g(-x) \\\\ &amp;= -f(x) - g(x) \\end{align*}\\] so \\([f+g](x)\\) is neither an odd nor an even function. \\[\\begin{align*} [f\\cdot g](-x) &amp;= f(-x)\\cdot g(-x) \\\\ &amp;= f(x)\\cdot -g(x) \\\\ &amp;= -\\big(f(x)\\cdot g(x)\\big) \\\\ &amp;= -[f\\cdot g](x) \\end{align*}\\] so \\([f\\cdot g](x)\\) is an odd function. \\[\\begin{align*} [f\\circ g](-x) &amp;= f\\big(g(-x)\\big) \\\\ &amp;= f\\big(-g(x)\\big) \\\\ &amp;= f\\big(g(x)\\big) \\\\ &amp;= [f\\circ g](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\[\\begin{align*} [g\\circ f](-x) &amp;= g\\big(f(-x)\\big) \\\\ &amp;= g\\big(f(x)\\big) \\\\ &amp;= [g\\circ f](x) \\end{align*}\\] so \\(\\lbrack f\\circ g\\rbrack(x)\\) is an even function. \\end{itemize} 13.4.4 Derivatives and Anti-derivatives of Odd Functions Let \\(f\\) be an odd function and let \\(f^\\prime\\) and \\(F\\) denote the derivative and anti-derivative of \\(f\\), respectively. Then \\(f^\\prime\\) and \\(F\\) are both even functions. Proof: \\[\\begin{align*} f(-x) &amp;= -f(x)\\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack f(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack-f(x)\\big\\rbrack \\\\ \\Rightarrow f^\\prime(-x)\\cdot -1 &amp;= -f^\\prime(x) \\\\ \\Rightarrow -f^\\prime(-x) &amp;= -f^\\prime(x) \\\\ \\Rightarrow f^\\prime(-x) &amp;= f^\\prime(x) \\end{align*}\\] So \\(f^\\prime\\) is an even function. \\[\\begin{align*} f(-x) &amp;= -f(x) \\\\ \\Rightarrow \\int f(-x) &amp;= \\int-f(x)\\\\ \\Rightarrow F(-x)\\cdot-1 &amp;= -F(x)\\\\ \\Rightarrow -F(-x) &amp;= -F(x)\\\\ \\Rightarrow F(-x) &amp;= F(x) \\end{align*}\\] So \\(F\\) is also an even function. 13.4.5 Derivatives and Anti-derivatives of Even Functions Let \\(g\\) be an even function, and let \\(g^\\prime\\) and \\(G\\) denote the derivative and anti-derivative of \\(g\\), respectively. Then \\(g^\\prime\\) and \\(G\\) are both odd functions. Proof: \\[\\begin{align*} g(-x) &amp;= g(x) \\\\ \\Rightarrow \\frac{d}{dx}\\big\\lbrack g(-x)\\big\\rbrack &amp;= \\frac{d}{dx}\\big\\lbrack g(x)\\big\\rbrack \\\\ \\Rightarrow g^\\prime(-x)\\cdot -1 &amp;= g^\\prime(x) \\\\ \\Rightarrow -g^\\prime(-x) &amp;= g^\\prime(x) \\\\ \\Rightarrow g^\\prime(-x) &amp;= -g^\\prime(x) \\end{align*}\\] So \\(g^\\prime\\) is an odd function. \\[\\begin{align*} g(-x) &amp;= g(x)\\\\ \\Rightarrow \\int g(-x) &amp;= \\int g(x)\\\\ \\Rightarrow G(-x)\\cdot-1 &amp;= G(x)\\\\ \\Rightarrow -G(-x) &amp;= G(x)\\\\ \\Rightarrow G(-x) &amp;= -G(x) \\end{align*}\\] So \\(G\\) is also an odd function. "],
["gamma-distribution.html", "14 Gamma Distribution 14.1 Probability Distribution Function 14.2 Cumulative Distribution Function 14.3 Expected Values 14.4 Moment Generating Function 14.5 Maximum Likelihood Estimators 14.6 Theorems for the Gamma Distribution", " 14 Gamma Distribution 14.1 Probability Distribution Function A random variable \\(X\\) is said to have a Gamma Distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its probability distribution function is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}}{\\Gamma(\\alpha)\\beta^\\alpha}, &amp; 0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array} \\right. \\] Where \\(\\alpha\\) is a scale parameter and\\ \\(\\beta\\) is a shape parameter. 14.2 Cumulative Distribution Function The cumulative distribution function for the Gamma Distribution cannot be expressed in closed form. It’s interval form is expressed here. \\[F(x) = \\left\\{ \\begin{array}{ll} \\int\\limits_{0}^{x}\\frac{t^{\\alpha-1}e^{-\\frac{t}{\\beta}}}{\\Gamma(\\alpha)\\beta^\\alpha}, &amp; 0&lt;t,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ \\\\ 0 &amp; otherwise \\end{array} \\right. \\] (#fig:Gamma_Distribution)The figures on the left and right display the Gamma probability and cumulative distirubtion functions, respectively, for the combinations of \\(\\alpha=2,3\\) and \\(\\beta=1,3\\). 14.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x\\cdot x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} [\\Gamma(\\alpha+1)\\beta^{\\alpha+1}] \\\\ &amp;= \\frac{\\Gamma(\\alpha+1)\\beta^{\\alpha+1}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{\\alpha\\Gamma(\\alpha)\\beta^{\\alpha+1}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\alpha\\beta \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{0}^{\\infty}x^2\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^2\\cdot x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha+1}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}} [\\Gamma(\\alpha+2)\\beta^{\\alpha+2}] \\\\ &amp;= \\frac{\\Gamma(\\alpha+2)\\beta^{\\alpha+2}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{(\\alpha+1)\\Gamma(\\alpha+1)\\beta^{\\alpha+2}} {\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\frac{(\\alpha+1)\\alpha\\Gamma(\\alpha)\\beta^{\\alpha+2}} {\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\alpha(\\alpha+1)\\beta^2 \\\\ &amp;= (\\alpha^2+\\alpha)\\beta^2 \\\\ &amp;= \\alpha^2\\beta^2+\\alpha\\beta^2 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\alpha\\beta \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^2\\beta^2 + \\alpha\\beta^2 - \\alpha^2\\beta^2 \\\\ &amp;= \\alpha\\beta^2 \\end{align*}\\] 14.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\int\\limits_{0}^{\\infty}e^{tx} \\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}e^{tx} x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{tx}e^{-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{tx-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{\\frac{\\beta tx}{\\beta}-\\frac{x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{\\frac{\\beta tx-x}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{-x\\frac{-\\beta t+1}{\\beta}}dx \\\\ &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1} e^{-x\\frac{1-\\beta t}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\Big[\\Gamma(\\alpha)\\Big(\\frac{\\beta}{1-\\beta t}\\Big)\\alpha)\\Big] \\\\ &amp;= \\frac{\\Gamma(\\alpha)\\beta^\\alpha} {\\Gamma(\\alpha)\\beta^\\alpha(1-\\beta t)^\\alpha} \\\\ &amp;= \\frac{1}{(1-\\beta t)^\\alpha}=(1-\\beta t)^{-\\alpha} \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx =\\beta^\\alpha\\Gamma(\\alpha)\\) \\[\\begin{align*} M_X^{(1)}(t) &amp;= -\\alpha(1-\\beta t)^{-\\alpha-1}(-\\beta) \\\\ &amp;= \\alpha\\beta(1-\\beta t)^{-\\alpha-1} \\\\ M_X^{(2)}(t) &amp;= (-\\alpha-1)\\alpha\\beta(1-\\beta t)^{-\\alpha-2}(-\\beta) \\\\ &amp;= (\\alpha+1)\\alpha\\beta^2(1-\\beta t)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1-\\beta t)^{-\\alpha-2} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0)=\\alpha\\beta(1-\\beta\\cdot 0)^{-\\alpha-1} \\\\ &amp;= \\alpha\\beta(1-0)^{\\alpha-1}=\\alpha\\beta(1)^{-\\alpha-1} \\\\ &amp;= \\alpha\\beta \\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0)=(\\alpha^2\\beta^2+\\alpha\\beta^2)(1-\\beta 0)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1-0)^{-\\alpha-2} \\\\ &amp;= (\\alpha^2\\beta^2+\\alpha\\beta^2)(1)^{-\\alpha-2} \\\\ &amp;= \\alpha^2\\beta^2+\\alpha\\beta^2 \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\alpha\\beta\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^2\\beta^2 + \\alpha\\beta^2 - \\alpha^2\\beta^2 \\\\ &amp;= \\alpha\\beta^2 \\end{align*}\\] 14.5 Maximum Likelihood Estimators Let \\(x_1,x_2,\\ldots,x_n\\) denote a random sample from a Gamma Distribution with parameters \\(\\alpha\\) and \\(\\beta\\). 14.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= f(x_1|\\theta) f(x_2|\\theta) \\cdots f(x_n|\\theta) \\\\ &amp;= \\frac{x_1^{\\alpha-1}e^{-x_1/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\frac{x_2^{\\alpha-1}e^{-x_2/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\cdots \\frac{x_n^{\\alpha-1}e^{-x_n/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\prod\\limits_{i=1}^{n}\\frac{x_i^{\\alpha-1}e^{-x_i/\\beta}}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= \\bigg(\\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}\\bigg)^n \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1}e^{-x_i/\\beta} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1}e^{-x_i/\\beta} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{\\sum\\limits_{i=1}^{n}-\\frac{x_i}{\\beta} \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\\\ &amp;= \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{-\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\end{align*}\\] 14.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln\\bigg[ \\big( \\Gamma(\\alpha)\\beta^\\alpha \\big)^{-n} \\exp\\bigg\\{-\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg] \\\\ &amp;= \\ln\\big( \\Gamma(\\alpha) \\beta^\\alpha \\big)^{-n} + \\ln\\bigg( \\exp \\bigg\\{ -\\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i \\bigg\\} \\bigg) + \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg) \\\\ &amp;= -n\\ln\\big( \\Gamma(\\alpha) \\beta^\\alpha \\big) - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + \\ln\\bigg( \\prod\\limits_{i=1}^{n}x_i^{\\alpha-1} \\bigg) \\\\ &amp;= -n\\big[ \\ln\\big( \\Gamma(\\alpha)\\beta^\\alpha\\big) \\big] - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + \\sum\\limits_{i=1}^{n}(\\alpha-1)\\ln x_i \\\\ &amp;= -n\\ln\\Gamma(\\alpha) - n\\alpha\\ln\\beta - \\frac{1}{\\beta}\\sum\\limits_{i=1}^{n}x_i + (\\alpha-1)\\sum\\limits_{i=1}^{n}\\ln x_i \\end{align*}\\] 14.5.3 MLE for \\(\\alpha\\) \\[\\begin{align*} \\frac{d\\ell}{d\\alpha} &amp;= -n\\frac{1}{\\Gamma(\\alpha)}\\Gamma^\\prime(\\alpha) - n\\ln\\beta - 0 + \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ &amp;= -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i\\\\ \\\\ \\\\ 0 &amp;= -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i \\\\ \\Rightarrow n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} &amp;= \\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln\\beta \\\\ \\Rightarrow \\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} &amp;= \\frac{1}{n}\\bigg( \\sum\\limits_{i=1}^{n}\\ln x_i - n\\ln\\beta \\bigg) \\end{align*}\\] However, this must be solved numerically. Notice also that the MLE for \\(\\alpha\\) depends on \\(\\beta\\). 14.5.4 MLE for \\(\\beta\\) \\[\\begin{align*} \\frac{d\\ell}{d\\beta} &amp;= 0 - n\\alpha\\frac{1}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i + 0 \\\\ &amp;= -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\\\ \\\\ 0 &amp;= -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\frac{n\\alpha}{\\beta} &amp;= \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow n\\alpha\\beta &amp;= \\sum\\limits_{i=1}^{n}x_i \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n\\alpha} \\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] This estimate, however, depends on \\(\\alpha\\). Since each estimator depends on the value of the other parameter, we must maximize the likelihood functions simulatneously. That is, we must simultaneously solve the system \\[ \\left\\{ \\begin{array}{rl} -n\\frac{\\Gamma^\\prime(\\alpha)}{\\Gamma(\\alpha)} - n\\ln\\beta + \\sum\\limits_{i=1}^{n}\\ln x_i &amp; = 0\\\\ -\\frac{n\\alpha}{\\beta} + \\frac{1}{\\beta^2}\\sum\\limits_{i=1}^{n}x_i &amp; = 0\\\\ \\end{array} \\right. \\] Solving this system will require numerical methods. 14.5.5 Approximation of \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) Approximations of \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) can be obtained by noticing that\\ \\[\\begin{align*} \\frac{d\\ell}{d\\beta} &amp;= 0 \\\\ \\Rightarrow \\beta &amp;= \\frac{1}{n\\alpha}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha\\beta &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i \\end{align*}\\] So \\(\\widehat{\\alpha\\beta} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\). Recall that \\(\\alpha\\beta\\) and \\(\\alpha\\beta^2\\) are the mean and variance of the Gamma Distribution, respectively. We utilize \\[ \\frac{\\alpha\\beta^2}{\\alpha\\beta} = \\beta \\] If we assume that \\(\\widehat{\\alpha\\beta^2} = \\frac{1}{n-1}\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\), then \\[ \\frac{\\widehat{\\alpha\\beta^2}}{\\widehat{\\alpha\\beta}} = \\beta^* \\approx \\hat\\beta \\] Where \\(\\beta^*\\) denotes an approximation to \\(\\hat\\beta\\) We now substitute \\(\\beta^*\\) into \\[\\begin{align*} \\widehat{\\alpha\\beta} &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha^*\\beta^* &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\\\ \\Rightarrow \\alpha^* &amp;= \\frac{1}{n\\beta^*}\\sum\\limits_{i=1}^{n}x_i \\approx \\hat\\alpha \\end{align*}\\] Where \\(\\alpha^*\\) denotes an approximation to \\(\\hat\\alpha\\). This method of estimation is prone to error because \\(\\beta^*\\) is found through two levels of estimation and \\(\\alpha^*\\) is found through three levels of estimation. Surely, this process inflates the error of estimation. At this point, however, I have no information to indicate how badly the error of estimation is inflated, nor have I performed any investigation into this problem. 14.6 Theorems for the Gamma Distribution 14.6.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}x\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx = 1\\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\frac{x^{\\alpha-1}e^{-\\frac{x}{\\beta}}} {\\Gamma(\\alpha)\\beta^\\alpha}dx &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha} \\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx \\\\ ^{[1]} &amp;= \\frac{1}{\\Gamma(\\alpha)\\beta^\\alpha}[\\Gamma(\\alpha)\\beta^\\alpha] \\\\ &amp;= \\frac{\\Gamma(\\alpha)\\beta^\\alpha}{\\Gamma(\\alpha)\\beta^\\alpha} \\\\ &amp;= 1 \\end{align*}\\] \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}dx = \\beta^\\alpha\\Gamma(\\alpha)\\) 14.6.2 Sum of Gamma Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be Gamma distributed random variables with parameters \\(\\alpha_i\\) and \\(\\beta\\), that is \\(X_i\\sim\\)Gamma\\((\\alpha_i,\\beta)\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\).\\ Then \\(Y\\sim\\)Gamma\\((\\sum\\limits_{i=1}^{n}\\alpha_i,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY})=E(e^{t(X_1+X_2+\\cdots+X_n)} \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-\\alpha_1}(1-\\beta t)^{-\\alpha_2}\\cdots (1-\\beta t)^{-\\alpha_n}=(1-\\beta t)^{-\\sum\\limits_{i=1}^{n}\\alpha_i} \\end{align*}\\] Which is the moment generating function of a Gamma random variable with parameters \\(\\sum\\limits_{i=1}^{n}\\alpha_i\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((\\sum\\limits_{i=1}^{n}\\alpha_i,\\beta)\\). 14.6.3 Sum of Exponential Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from an Exponential distribution with parameter \\(\\beta\\), i.e. \\(X_i\\sim\\)Exponential\\((\\beta)\\). Let \\(Y = \\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Gamma\\((n,\\beta)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= (1-\\beta t)^{-1}(1-\\beta t)^{-1}\\cdots(1-\\beta t)^{-1} \\\\ &amp;= (1-\\beta t)^{-n} \\end{align*}\\] Which is the moment generating function for a Gamma random variable with parameters \\(n\\) and \\(\\beta\\). Thus \\(Y\\sim\\)Gamma\\((n,\\beta)\\). "],
["geometric-distribution.html", "15 Geometric Distribution 15.1 Probability Mass Function 15.2 Cumulative Distribution Function 15.3 Expected Values 15.4 Moment Generating Function 15.5 Maximum Likelihood Estimator 15.6 Theorems for the Geometric Distribution", " 15 Geometric Distribution 15.1 Probability Mass Function A random variable \\(X\\) is said to have a Geometric Distribution with parameter \\(p\\) if its probability mass function is \\[p(x) = \\left\\{ \\begin{array}{ll} p(1-p)^{x-1}, &amp; x=1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] where \\(p\\) is the probability of a success on any given trial and \\(x\\) is the number of trials until the first success. 15.2 Cumulative Distribution Function The cumulative probability of \\(x\\) is computed as the \\(x^{th}\\) partial sum of the Geometric Series See 16.1. \\[\\begin{align*} P(x) &amp;= \\sum\\limits_{i=1}^{x}p(1-p)^{x-1} \\\\ &amp;= \\frac{p-p(1-p)^{x-1}}{1-(1-p)} \\\\ &amp;= \\frac{p[1-(1-p)^{x-1}]}{p} \\\\ &amp;= 1-(1-p)^{x-1} \\end{align*}\\] So \\[P(x) = \\left\\{ \\begin{array}{ll} 1-(1-p)^{x-1},&amp; x=1,2,3,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] A recursive form of the cdf can be derived and has some usefulness in computer applications. With it, one need only initiate the first value and additional cumulative probabilities can be calculated. It is derived as follows: \\[\\begin{align*} P(X=x+1) &amp;= p(1-p)^x \\\\ &amp;= (1-p)p(1-p)^{x-1} \\\\ &amp;= (1-p)P(X=x) \\end{align*}\\] Figure 15.1: The figures on the left and right display the Geometric probability and cumulative distirubtion functions, respectively, for the combinations of \\(p=.3\\). 15.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=1}^{\\infty}x\\cdot p(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}x\\cdot (1-p)^{x-1} \\\\ ^{[1]} &amp;= p\\sum\\limits_{x=1}^{\\infty}x\\cdot q^{x-1} \\\\ ^{[2]} &amp;= p\\frac{d}{dq}\\Big(\\sum\\limits_{x=1}^{\\infty}q^x\\Big) \\\\ &amp;= p\\frac{d}{dq}\\Big(\\sum\\limits_{x=1}^{\\infty}q\\cdot q^{x-1}\\Big) \\\\ ^{[3]} &amp;= p\\frac{d}{dq}\\Big(\\frac{q}{1-q}\\Big) \\\\ ^{[4]} &amp;= p\\Big(\\frac{(1-q)\\cdot 1-q(-1)}{(1-q)^2}\\Big) \\\\ &amp;= p\\Big(\\frac{1-q+q}{(1-q)^2}\\Big) \\\\ &amp;= p\\Big(\\frac{1}{(1-q)^2}\\Big) \\\\ ^{[5]} &amp;= p\\cdot\\frac{1}{p^2} &amp;= \\frac{p}{p^2} &amp;= \\frac{1}{p} \\end{align*}\\] Let \\(1-p = 1\\) \\(a\\cdot x^{a-1}=\\frac{d}{dx}(x^a)\\) \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=\\frac{a}{1-r}\\) (Geometric Series 16.1). Product Rule for Differentiation: \\(\\frac{d}{dx}(\\frac{f(x)}{g(x)}) = \\frac{g\\prime(x)\\cdot f(x)-f\\prime(x)\\cdot g(x)}{g^2(x)}\\) \\(1-p=q \\ \\ \\Rightarrow p=1-q\\) \\[\\begin{align*} E[X(X-1)] ^{[1]} &amp;= \\sum\\limits_{x=2}^{\\infty}x(x-1)p(1-p)^{x-1} \\\\ &amp;= p(1-p)\\sum\\limits_{x=2}^{\\infty}x(x-1)(1-p)^{x-2} \\\\ ^{[2]} &amp;= pq\\sum\\limits_{x=2}^{\\infty}x(x-1)q^{x-2} \\\\ ^{[3]} &amp;= pq\\frac{d^2}{dq^2}\\Big(\\sum\\limits_{x=2}^{\\infty}q^x\\Big) \\\\ &amp;= pq\\frac{d^2}{dq^2}\\Big(\\sum\\limits_{x=2}^{\\infty}q\\cdot q^{x-1}\\Big) \\\\ ^{[4]} &amp;= pq\\frac{d^2}{dq^2}\\Big(\\frac{2q-1}{1-q}\\Big) \\\\ ^{[5]} &amp;= pq\\frac{d}{dq}(-(1-q)^{-2}) \\\\ ^{[6]} &amp;= pq\\frac{2}{(1-q)^3} \\\\ &amp;= \\frac{2pq}{(1-q)^3} \\\\ ^{[7]} &amp;= \\frac{2p(1-p)}{p^3} \\\\ &amp; =\\frac{2(1-p)}{p^2} \\end{align*}\\] We start the summand at \\(x=2\\) because the term for \\(x=1\\) is 0. Let \\(1-p = 1\\) \\(a(a-1)x^{a-2}=\\frac{d^2}{dx^2}x^a\\) \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1} = \\frac{a}{1-r}=1+a+ar+ar^2+\\cdots.\\) The current series leaves out the first term,\\ \\(\\sum\\limits_{k=2}^{\\infty}ar^{k-1}=(\\sum\\limits_{k=1}^{\\infty}ar^{k-1})-1 = \\frac{a}{1-r}-1=\\frac{1}{1-r}-\\frac{1-r}{1-r}=\\frac{a-(1-r)}{1-r}=\\frac{a+r-1}{r-1}\\) \\(\\frac{d}{dx}(\\frac{2x-1}{1-x})=\\frac{-(2x-1)-2(1-x)}{(1-x)^2} = \\frac{2x+1-2+2x}{(1-x)^2}=\\frac{-1}{(1-x)^2}=-(1-x)^{-2}\\) \\(\\frac{d}{dx}-(1-x)^{-2}=2(1-x)^{-3}=\\frac{2}{(1-x)^3}\\) See note number 5. \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\frac{1}{p} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= [E(X^2)-E(X)]+E(X)-E(X)^2 \\\\ &amp;= E(X^2-X)+E(X)-E(X)^2 \\\\ &amp;= E[(X(X-1)]+E(X)-E(X)^2 \\\\ &amp;= \\frac{2(1-p)}{p^2} + \\frac{1}{p} - \\frac{1}{p^2} \\\\ &amp;= \\frac{2(1-p)}{p^2} + \\frac{p}{p^2} - \\frac{1}{p^2} \\\\ &amp;= \\frac{2(1-p)+p-1}{p^2} \\\\ &amp;= \\frac{2-2p+p-1}{p^2} \\\\ &amp;= \\frac{2-1+p-2p}{p^2} \\\\ &amp;= \\frac{1-p}{p^2} \\end{align*}\\] 15.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{x=1}^{\\infty}e^{tx}p(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}e^{tx}(1-p)^{x-1} \\\\ &amp;= p\\sum\\limits_{x=1}^{\\infty}e^{t^x}(1-p)^{x-1} \\\\ &amp;= pe^t\\sum\\limits_{x=1}^{\\infty}e^{t^{(x-1)}} \\\\ &amp;= pe^t\\sum\\limits_{x=1}^{\\infty}[(1-p)e^t]^{x-1} \\\\ ^{[1]} &amp;= pe^t\\frac{1}{1-(1-p)e^t} \\\\ &amp;= \\frac{pe^t}{1-(1-p)e^t} \\end{align*}\\] \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=\\frac{a}{1-r}\\), (Geometric Series 16.1) \\[\\begin{align*} M_X^{(1)}(t) &amp;= \\frac{[1-(1-p)e^t]pe^t-pe^t[-(1-p)e^t]}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t[1-(1-p)e^t+(1-pe^t)]}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t(1)}{\\big(1-(1-p)e^t\\big)^2} \\\\ &amp;= \\frac{pe^t}{\\big(1-(1-p)e^t\\big)^2} \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= \\frac{(1-(1-p)e^t)^2pe^t-pe^t[-2(1-(1-p)e^t)(1-p)e^t}{\\big(1-(1-p)e^t\\big)^4} \\\\ &amp;= \\frac{pe^t[(1-(1-p)e^t)^2+2(1-(1-p)e^t(1-p)e^t]}{\\big(1-(1-p)e^t\\big)^4} \\\\ \\\\ \\\\ E(X) &amp;= M_X^{(1)}(0) \\\\ &amp;= \\frac{pe^0}{\\big(1-(1-p)e^0\\big)^2} \\\\ &amp;= \\frac{p}{\\big(1-(1-p)\\big)^2} \\\\ &amp;= \\frac{p}{(1-1+p)^2} \\\\ &amp;= \\frac{p}{p^2} \\\\ &amp;= \\frac{1}{p}\\\\ \\\\ \\\\ E(X^2) &amp;= M_X^{(2)}(0) \\\\ &amp;= \\frac{pe^0[(1-(1-p)e^0)^2+2(1-(1-p)e^0)(1-p)e^0]}{(1-(1-p)e^0)^4} \\\\ &amp;= \\frac{p[(1-(1-p))^2+2(1-(1-p))(1-p)]}{(1-(1-p))^4} \\\\ &amp;= \\frac{p[(1-1+p)^2+2(1-1+p)(1-p)]}{(1-1+p)^4} \\\\ &amp;= \\frac{p[p^2=2p(1-p)]}{p^4} \\\\ &amp;= \\frac{p(p^2+2p-2p^2)}{p^4} \\\\ &amp;= \\frac{p(2p-p^2)}{p^4} \\\\ &amp;= \\frac{p^2(2-p)}{p^4} \\\\ &amp;= \\frac{2-p}{p^2} \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\frac{1}{p}\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\frac{2-p}{p^2} - \\frac{1}{p^2} \\\\ &amp;= \\frac{1-p}{p^2} \\end{align*}\\] 15.5 Maximum Likelihood Estimator Let \\(x_1, x_2 , \\ldots , x_n\\) be a random sample from a Geometric distribution with parameter \\(p\\). 15.5.1 Likelihood Function \\[\\begin{align*} L(\\theta) &amp;= P(x_1,x_2,\\ldots,x_n|\\theta) \\\\ &amp;= p(1-p)^{x-1} \\end{align*}\\] 15.5.2 Log-likelihood Function \\[\\begin{align*} \\ell(\\theta) &amp;= \\ln p(1-p)^{x-1} \\\\ &amp;= \\ln p+\\ln(1-p)^{x-1} \\\\ &amp;= \\ln p+(x-1)\\ln(1-p) \\end{align*}\\] 15.5.3 MLE for \\(p\\) \\[\\begin{align*} \\frac{d\\ell}{dp} &amp;= \\frac{1}{p} + \\frac{-(x-1)}{1-p} \\\\ &amp;= \\frac{1}{p} + \\frac{1-x}{1-p}\\\\ \\\\ \\\\ 0 &amp;= \\frac{1}{p}+\\frac{1-x}{1-p}\\\\ \\Rightarrow \\frac{1-x}{1-p} &amp;= -\\frac{1}{p}\\\\ \\Rightarrow 1-x &amp;= \\frac{-1+p}{p}\\\\ \\Rightarrow -x &amp;= \\frac{-1+p}{p}-1\\\\ \\Rightarrow x=1-\\frac{-1+p}{p} &amp;= \\frac{p}{p}-\\frac{-1+p}{p} \\\\ &amp;= \\frac{p+1-p}{p} \\\\ &amp;= \\frac{1}{p}\\\\ \\Rightarrow p &amp;= \\frac{1}{x} \\end{align*}\\] So \\[\\hat p=\\frac{1}{x}\\] is the Maximum Likelihood Estimator for \\(p\\). 15.6 Theorems for the Geometric Distribution 15.6.1 Validity of the Distribution \\[\\sum\\limits_{i=1}^{\\infty}p(1-p)^{x-1} = 1\\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{\\infty}p(1-p)^{x-1} &amp;= \\frac{p}{1-(1-p)} \\\\ &amp;= \\frac{p}{p} \\\\ &amp;= 1 \\end{align*}\\] \\(S=\\lim\\limits_{k\\rightarrow\\infty}S_k =\\lim\\limits_{k\\rightarrow\\infty}\\frac{a-ar^k}{1-r}\\) (Geometric Series 16.1) 15.6.2 Sum of Geometric Random Variables Let \\(X_1,X_2,\\ldots,X_n\\) be independent random variables from a Geometric Distribution with parameter \\(p\\), that is, \\(X_i\\sim\\)Geometric\\((p)\\), \\(i=1,2,3,\\ldots\\). Let \\(Y=\\sum\\limits_{i=1}^{n}X_i\\). Then \\(Y\\sim\\)Negative Binomial\\((n,p)\\). Proof: \\[\\begin{align*} M_Y(t) &amp;= E(e^{tY}) \\\\ &amp;= E(e^{t(X_1+X_2+\\cdots+X_n)}) \\\\ &amp;= E(e^{tX_1}e^{tX_2}\\cdots e^{tX_n}) \\\\ &amp;= E(e^{tX_1})E(e^{tX_2})\\cdots E(e^{tX_n}) \\\\ &amp;=\\frac{pe^t}{1-(1-p)e^t}\\cdot\\frac{pe^t}{1-(1-p)e^t}\\cdot\\ \\cdots \\ \\cdot\\frac{pe^t}{1-(1-p)e^t} \\\\ &amp;= \\Big(\\frac{pe^t}{1-(1-p)e^t}\\Big)^n \\end{align*}\\] Which is the mgf of a Negative Binomial random variable with parameters \\(n\\) and \\(p\\). Thus \\(Y\\sim\\)Negative Binomial\\((n,p)\\). 15.6.3 Lemma Let \\(X\\) be a Geometric random variable with parameter \\(p\\). Then \\(P(X&gt;a)=(1-p)^a\\). Proof: \\[\\begin{align*} P(X&gt;a) &amp;= 1-P(X\\leq a)=1-\\sum\\limits_{i=1}^{a}p(1-p)^i-1 \\\\ ^{[1]} &amp;= 1-\\frac{p-p(1-p)^a}{1-(1-p)} \\\\ &amp;= 1-\\frac{p\\big(1-(1-p)^a\\big)}{1-1+p} \\\\ &amp;= 1-\\frac{p\\big(1-(1-p)^a\\big)}{p} \\\\ &amp;= 1-\\big(1-(1-p)^a\\big) \\\\ &amp;= 1-1+(1-p)^a \\\\ &amp;= (1-p)^a \\end{align*}\\] \\(S_k=\\lim\\limits_{k\\rightarrow\\infty}\\frac{a-ar^k}{1-r}\\) (Geometric Series 16.1) 15.6.4 Memoryless Property \\[P(X\\geq k+j|X\\geq k)=P(X\\geq k)\\] Proof: \\[\\begin{align*} P(X &gt; a+b) ^{[1]} &amp;= (1-p)^{a+b}=(1-p)^a(1-p)^b \\\\ \\\\ \\\\ P(X &gt; k+j|X &gt; k) &amp;=\\frac{P(X &gt; k+j\\cap X &gt; k)}{P(X &gt; k)} \\\\ ^{[2]} &amp;= \\frac{P(X &gt; k+j)}{P(X &gt; k)} \\\\ &amp;= \\frac{(1-p)^k(1-p)^j}{(1-p)^k} \\\\ &amp;= (1-p)^j \\\\ &amp;= P(X &gt; j) \\end{align*}\\] Geometric Distribution 15.6.3 Since \\(j\\) must be a positive integer in the Geometric Distribution, it is certain that \\\\((k+j)\\cap k=k+j\\). "],
["geometric-series.html", "16 Geometric Series 16.1 Partial and Infinite Summations 16.2 Proofs of Convergence", " 16 Geometric Series A Geometric Series is a series of the form \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) where \\(a\\neq 0,\\ r\\neq 0,1\\). Expanding the series gives \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}=a+ar+ar^2+ar^3+\\cdots\\). 16.1 Partial and Infinite Summations Let \\(S_k\\) denote the sum of a series over \\(k\\) terms (or the \\(k^{th}\\) partial sum). For the Geometric Series\\ \\[\\begin{align*} S_k &amp;= \\sum\\limits_{k=1}^{k}ar^{k-1} \\\\ &amp;= a + ar + ar^2 + ar^3 + \\cdots + ar^{k-1} \\\\ &amp;= ar^0 + ar^2 + ar^2 + \\cdots + ar^{k-1} \\\\ &amp;= a + ar + ar^2 + \\cdots + ar^{k-1} \\end{align*}\\] Notice that \\(rS_n = ar + ar^2+ a r^3 + \\cdots + ar^k\\). So \\[\\begin{align*} S_k - rS_n &amp;= (a + ar + \\cdots + ar^{k-1}) - (ar + ar^2 + \\cdots + ar^k) \\\\ &amp;= a + ar - ar + ar^2 - ar^2 + \\cdots + ar^{k-2} - ar^{k-2} + ar^{k-1} - ar^{k-1} - ar^k \\\\ &amp;= a - ar^k \\end{align*}\\] Also \\[\\begin{align*} S_k - rS_k &amp;= S_k(1-r) \\\\ &amp;= a - ar^k \\\\ \\Rightarrow S_k &amp;= \\frac{a-ar^k}{1-r} \\end{align*}\\] 16.2 Proofs of Convergence \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) converges when \\(|r|&lt;1\\) and diverges when \\(|r|&gt;1\\). Proof: Recall that the \\(k^{th}\\) partial sum of the Geometric Series is \\[ S_k = \\frac{a-ar^k}{1-r} \\] And let \\(S\\) denote the sum of the infinite series, i.e. the sum as \\(k\\rightarrow\\infty\\). Case 1: \\(|r|&lt;1\\) \\[\\begin{align*} S &amp;= \\lim\\limits_{k\\rightarrow\\infty}S_k \\\\ &amp;= \\lim\\limits_{k\\rightarrow\\infty} \\frac{a-ar^k}{1-r} \\\\ &amp;= \\frac{a-\\lim\\limits_{k\\rightarrow\\infty}ar^k}{1-r} \\\\ &amp;= \\frac{a}{1-r} \\end{align*}\\] So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) converges when \\(|r|&lt;1\\) and \\(S=\\frac{a}{1-r}\\) Case 2: So \\(\\sum\\limits_{k=1}^{\\infty}ar^{k-1}\\) diverges whern \\(|r|&gt;1\\). "],
["hypergeometric-distribution.html", "17 Hypergeometric Distribution 17.1 Probability Mass Function 17.2 Cumulative Mass Function 17.3 Expected Values 17.4 Moment Generating Function 17.5 Theorems for the Hypergeometric Distribution", " 17 Hypergeometric Distribution 17.1 Probability Mass Function A random variable \\(X\\) is said to follow a Hypergeometric Distribution if its probability mass function is \\[p(x) = \\left\\{ \\begin{array}{ll} \\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}}, &amp; x=0,1,2,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] where \\(N\\) is the number of objects available to choose from \\(n\\) is the number of objects chosen from \\(N\\) \\(r\\) is the number of objects in \\(N\\) that posses a desired characteristic (successes) \\(x\\) is the number of objects in \\(n\\) that possess the desired characterstic 17.2 Cumulative Mass Function \\[P(x) = \\left\\{ \\begin{array}{ll} \\sum\\limits_{i=0}^{x}\\frac{{r\\choose i}{N-r\\choose n-i}}{{N\\choose n}}, &amp; x=0,1,2,\\ldots\\\\ 0 &amp; otherwise \\end{array} \\right. \\] 17.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\sum\\limits_{x=0}^{n}x\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ &amp;= \\sum\\limits_{x=0}^{n}x{r\\choose x}\\frac{{N-r\\choose n-x}}{{N\\choose n}} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n}x\\frac{r}{x}{r-1\\choose x-1}\\frac{{N-r\\choose n-x}} {\\frac{N}{n}{N-1\\choose n-1}} \\\\ &amp;= \\frac{rn}{N}\\sum\\limits_{x=0}^{n} \\frac{{r-1\\choose x-1}{N-r\\choose n-x}}{{N-1\\choose n-1}} \\\\ ^{[2]} &amp;= \\frac{rn}{N}\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\frac{rn}{N}\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\\\ &amp;= \\frac{\\frac{rn}{N}\\cdot n}{N}r\\sum\\limits_{y=0}^{n-1} \\frac{{r-1\\choose y}{N-r\\choose n-y-1}}{{N-1\\choose n-1}} \\\\ ^{[3]} &amp;= \\frac{rn}{N}\\cdot 1 \\\\ &amp;= \\frac{rn}{N} \\end{align*}\\] For any integer \\(a\\) such that \\(0\\leq a\\leq k,\\ {n\\choose k}=\\frac{n(n-1)\\cdots(n-a+1)}{k(k-1)\\cdots(k-a+1)}{n-a\\choose k-a}\\) (Theorem @ref(Combinations1.3)). Let \\(y=x-1\\) \\(\\Rightarrow x=y+1\\). \\(\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}}=1\\)\\ with \\(N_1=r,\\ N_2=N-r,\\ i=x\\). (Theorem @ref(BinomialTheor3.2) \\[\\begin{align*} E[X(X-1)] &amp;= \\sum\\limits_{x=0}^{n}x(x-1)\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ ^{[1]} &amp;= \\sum\\limits_{x=0}^{n}\\frac{x(x-1)r(r-1)}{x(x-1)}\\frac{{r-2\\choose x-2} {N-r\\choose n-x}}{\\frac{N(N-1)}{n(n-1)}{N-2\\choose n-2}} \\\\ &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\sum\\limits_{x=0}^{n} \\frac{{r-2\\choose x-2}{N-r\\choose n-x}}{{N-2\\choose n-2}} \\\\ ^{[2]} &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\sum\\limits_{y=0}^{n-2} \\frac{{r-2\\choose y}{N-r\\choose n-y-2}}{{N-2\\choose n-2}} \\\\ ^{[3]} &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)}\\cdot 1 \\\\ &amp;=\\frac{r(r-1)n(n-1)}{N(N-1)} \\end{align*}\\] For any integer \\(a\\) such that \\(0\\leq a\\leq k,\\ {n\\choose k}=\\frac{n(n-1)\\cdots(n-a+1)}{k(k-1)\\cdots(k-a+1)}{n-a\\choose k-a}\\) (Theorem @ref(Combinations1.3)). Let \\(y=x-1\\) \\(\\Rightarrow x=y+1\\). \\(\\frac{\\sum\\limits_{i=1}^{n}{N_1\\choose i}{N_2\\choose n-i}}{{N_1+N_2\\choose n}}=1\\)\\ with \\(N_1=r,\\ N_2=N-r,\\ i=x\\). (Theorem @ref(BinomialTheor3.2) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\frac{rn}{N} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= (E(X^2) - E(X) + E(X) - E(X)^2 \\\\ &amp;= E(X^2-X) + E(X) - E(X)^2 \\\\ &amp;= E[X(X-1)] + E(X) - E(X)^2\\\\ &amp;= \\frac{r(r-1)n(n-1)}{N(N-1)} + \\frac{rn}{N} - \\frac{r^2n^2}{N^2} \\\\ &amp;= \\frac{r(r-1)n(n-1)N}{N^2(N-1)} + \\frac{rnN(N-1)}{N^2(N-1)} - \\frac{r^2n^2(N-1)}{N^2(N-1)} \\\\ &amp;= \\frac{(r^2-r)(n^2-n)N rn(N^2-N)-r^2n^2(N-1)}{N^2(N-1)} \\\\ &amp;= \\frac{(r^2n^2N-r^2n^2N-rn^2N+rnN+rnN^2-rnN-r^2n^2N+r^2n^2}{N^2(N-1)} \\\\ &amp;= \\frac{-r^2nN-rn^2N+rnN^2+r^2n^2}{N^2(N-1)} \\\\ &amp;= \\frac{nr(-rN-nN+N^2+rn}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N^2-nN-rN+rn}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N-r)(N-n)}{N^2(N-1)} \\\\ &amp;= \\frac{nr(N-r)(N-n)}{N\\cdot N(N-1)} \\\\ &amp;= \\frac{nr}{N}\\cdot\\frac{N-r}{N}\\cdot\\frac{N-n}{N-1} \\end{align*}\\] 17.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX}) \\\\ &amp;= \\sum\\limits_{x=0}^{n}e^{tx}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} \\\\ &amp;= \\frac{1}{{N\\choose n}}\\sum\\limits_{x=0}^{n}e^{tx}{r\\choose x}{N-r\\choose n-x} \\\\ &amp;= \\frac{1}{{N\\choose n}}[e^{0t}{r\\choose 0}{N-r\\choose n-0} + e^{1t}{r\\choose 1}{N-r\\choose n-1} + e^{2t}{r\\choose 2}{N-r\\choose n-2} + \\cdots + e^{nt}{r\\choose n}{N-r\\choose n-n}] \\\\ &amp;= \\frac{1}{{N\\choose n}}[{N-r\\choose n-0}+e^{t}{r\\choose 1}{N-r\\choose n-1} + e^{2t}{r\\choose 2}{N-r\\choose n-2}+\\cdots+e^{nt}{r\\choose n}{N-r\\choose n-n}] \\end{align*}\\] This mgf does not reduce to any form which can be differentiated, and we cannot use it to generate moments for the distribution. 17.5 Theorems for the Hypergeometric Distribution 17.5.1 Validity of the Distribution \\[ \\sum\\limits_{x=0}^{n}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} = 1 \\] Proof: Theorem @ref(BinomialTheor3.1) states \\[ {N_1\\choose 0}{N_2\\choose n}+{N_1\\choose 2}{N_2\\choose n-1}+\\cdots +{N_1\\choose n-1}{N_2\\choose 1}+{N_1\\choose n}{N_2\\choose 0} \\\\ = \\sum\\limits_{x=0}^{n}{N_1\\choose x}{N_2\\choose n-x} \\\\ = {N_1+N_2\\choose n} \\] Using \\(N_1 = r\\) and \\(N_2 = N-r\\) we have \\[\\begin{align*} \\sum\\limits_{x=0}^{n}\\frac{{r\\choose x}{N-r\\choose n-x}}{{N\\choose n}} &amp;= \\frac{{r+N-r\\choose n}}{{N\\choose n}} \\\\ &amp;= \\frac{{N\\choose n}}{{N\\choose n}} \\\\ &amp;= 1 \\end{align*}\\] "],
["integration-techniques-and-theorems.html", "18 Integration: Techniques and Theorems 18.1 Elementary Theorems", " 18 Integration: Techniques and Theorems 18.1 Elementary Theorems 18.1.1 Integration of Even Functions about Zero Suppose \\(f\\) is an integratable function, and let \\(F(x)=\\int\\limits_{0}^{x_0}f(x)dx\\). Then \\(\\int\\limits_{-x_0}^{0}f(x)dx=\\int\\limits_{0}^{x_0}f(x)dx\\) if and only if \\(f\\) is an even function.\\ Proof: First, let \\(f\\) be an even function. Then, by Theorem 13.4.5, the anti-derivative \\(F\\) is an odd function. \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= F(0) + F(x_0) \\\\ ^{[1]} &amp;= F(x_0) \\\\ \\\\ \\\\ \\int\\limits_{0}^{x_0}f(x)dx &amp;= F(x_0) - F(0) \\\\ &amp;= F(x_0) \\end{align*}\\] \\(F(0)=\\int\\limits_{0}^{0}f(x)dx=0\\). So \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(x_0) \\\\ &amp;= \\int\\limits_{0}^{x_0}f(x)dx \\end{align*}\\] Now suppose \\[ \\int\\limits_{-x_0}^{0}f(x)dx = \\int\\limits_{0}^{x_0}f(x)dx \\] Then \\[\\begin{align*} \\int\\limits_{-x_0}^{0}f(x)dx &amp;= F(0) - F(-x_0) \\\\ &amp;= -F(-x_0) \\end{align*}\\] and \\[\\begin{align*} \\int\\limits_{0}^{x_0}f(x)dx = F(x_0) - F(0) \\\\ = F(x_0)$ \\end{align*}\\] So \\[\\begin{align*} -F(-x_0) &amp;= F(x_0) \\\\ \\Rightarrow F(-x_0) &amp;= -F(x_0) \\end{align*}\\] This satisfies the definition of an odd function. So by Theorem 13.4.4, \\(f\\) must be an even function. 18.1.2 Corollary If \\(f\\) is a continuous and even function and \\(t\\in\\Re\\), then \\[ \\int\\limits_{-t}^{t}f(x)dx = 2\\int\\limits_{0}^{t}f(x)dx \\] Furthermore, \\[\\int\\limits_{-\\infty}^{\\infty}f(x)dx = 2\\int\\limits_{0}^{\\infty}f(x)dx\\]. Proof: Since \\(f(x)\\) is even and by Theorem 18.1.1 \\[\\begin{align*} \\int\\limits_{-t}^{0}f(-x)dx &amp;= \\int\\limits_{-t}^{0}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] It follows that \\[\\begin{align*} \\int\\limits_{-t}^{t}f(x)dx &amp;= \\int\\limits_{-t}^{0}f(-x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= \\int\\limits_{0}^{t}f(x)dx + \\int\\limits_{0}^{t}f(x)dx \\\\ &amp;= 2\\int\\limits_{0}^{t}f(x)dx \\end{align*}\\] The second statement is proven by taking the limits as \\(t\\rightarrow\\infty\\). 18.1.3 Integrals of Horizontal Translations Let \\(x\\) be any real number and \\(a,b,\\) and \\(c\\) be constants. Also, let \\(f(x)\\) be continuous on the interval \\((a,b)\\). Then \\[\\int\\limits_{a}^{b}f(x)dx = \\int\\limits_{a+c}^{b+c} f(x+c)dx\\] Proof: The proof of this theorem is completed by applying a change of variable to \\[\\int\\limits_{a}^{b}f(x)dx\\] We let \\[\\begin{align*} y &amp;= x+c \\\\ \\Rightarrow x &amp;= y-c \\end{align*}\\] So \\(dx=dy\\). \\[\\begin{align*} x &amp;= a &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= a+c\\\\ x &amp;= b &amp; \\Rightarrow \\ \\ \\ \\ y &amp;= b+c \\end{align*}\\]. Thus \\[\\begin{align*} \\int\\limits_{a}^{b}f(x)dx &amp;= \\int\\limits_{a+c}^{b+c}f(y)dy \\\\ &amp;= \\int\\limits_{a+c}^{b+c}f(x+c)dx \\end{align*}\\] "],
["logistic-regression.html", "19 Logistic Regression 19.1 The Logit Transformation 19.2 Retrieving the Modelled Probability", " 19 Logistic Regression 19.1 The Logit Transformation Logistic regression intends to model the probability that a given response will occur based on the characteristics in the predictor variables. The response, therefore, is a probability; a value between zero and one. This is in contrast to typical linear regression where the response lies range of real numbers. This is further complicated by the fact that in the observed data, a subject does not have a probabilistic response, but the response is the dichotomous occurrence of an event. The nature of the response variable in logistic regression, therefore, necessitates that a transformation be applied. 19.1.1 Obtaining the Logit Transformation Let us call the response for our logistic model the probability \\(p\\) that an event will occur. Since \\(p\\) is a probability, by definition, it’s domain is from 0 to 1. Ideally, we would like to have a response whose domain is \\(\\Re\\). First, let us consider the transformation \\(\\frac{p}{1-p}\\) (also called the odds of \\(p\\)) and it’s limits as \\(p\\) approaches 0 and 1. \\[\\begin{align*} \\lim\\limits_{p\\rightarrow 0} \\frac{p}{1-p} = \\frac{0}{1-0} = 0 \\end{align*}\\] and \\[\\begin{align*} \\lim\\limits_{p\\rightarrow 1} \\frac{p}{1-p} = \\infty \\end{align*}\\] So the domain of \\(\\frac{p}{1-p}\\) is \\((0,\\infty)\\). This is handy, as we do know that the \\(\\ln\\) function takes a variable on the domain \\((0,\\infty)\\) and maps it onto the range \\((-\\infty,\\infty)\\). Thus, the equation for our logistic model the transformation (called the logit, or log-odds): \\[\\ln\\bigg(\\frac{p}{1-p}\\bigg) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n\\] 19.2 Retrieving the Modelled Probability While the logit transformation allows us to perform the logistic regression, the resulting measure tells us about the risk of an event associated with a predictor, but does not tell us directly about the probability of the event occuring. If we need to know the probability of the event ocurring, we must back transform the results of our regression equation. Essentially, we extract \\(\\hat p\\) from the modelled log-odds. This is done as by: \\[\\begin{align*} \\ln\\bigg(\\frac{\\hat{p}}{1-\\hat{p}}\\bigg) &amp;= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n \\\\ ^{[1]} &amp;= B \\\\ \\Rightarrow \\frac{\\hat{p}}{1-\\hat{p}} &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= (1-\\hat{p})\\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= \\exp(B) - \\hat{p} \\cdot \\exp(B) \\\\ \\Rightarrow \\hat{p} + \\hat{p} \\cdot \\exp(B) &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p}(1+\\exp(B)) &amp;= \\exp(B) \\\\ \\Rightarrow \\hat{p} &amp;= \\frac{\\exp(B)}{1 + \\exp(B)} \\\\ \\Rightarrow \\hat{p} &amp;= \\frac{\\exp(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n)} {1 + \\exp(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n)} \\end{align*}\\] Let \\(B = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n\\) "],
["mantel-haenszel-test-of-linear-trend.html", "20 Mantel-Haenszel Test of Linear Trend", " 20 Mantel-Haenszel Test of Linear Trend The Mantel-Haenszel Test is a method for testing independence of categorical variables on an ordinal scale. See Agresti for more discussion. Let \\(X\\) be a categorical variable of ordinal type with \\(R\\) levels. Let \\(Y\\) be a categorical variable of ordinal type with \\(C\\) levels. Suppose we take a sample of size \\(n\\) and take a measurement on each item in the sample with respect to \\(X\\) and \\(Y\\). The presence of a progresive between \\(X\\) and \\(Y\\) can be tested using the correlation coefficient \\(\\rho\\). We may begin by taking the estimate of \\(\\rho\\) \\[\\begin{align*} r &amp;= \\frac{\\widehat{Cov}(X,Y)}{\\sqrt{s_X^2 s_Y^2}} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}(x_i-\\bar x)(y_j-\\bar y)p(x_i,y_j)} {\\sqrt{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2p(x_i) \\sum\\limits_{j=1}^{n}(y_j-\\bar y)^2p(y_j)}} \\end{align*}\\] But since \\(X\\) and \\(Y\\) are categorical, we cannot sensibly perform any of the operations. Instead, we define the variables \\(U\\) and \\(V\\) to be the ordinal scoring of \\(X\\) and \\(Y\\) respecitively. In other words, \\(U_i\\) is the score for the category of \\(X_i\\) and \\(V_i\\) is the score for the category of \\(Y_i\\). Using this replacement we get \\[ r = \\frac{\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}(u_i-\\bar u)(v_j-\\bar v)p(u_i,v_j)} {\\sqrt{\\sum\\limits_{i=1}^{n}(u_i-\\bar u)^2p(u_i) \\sum\\limits_{j=1}^{n}(v_j-\\bar v)^2p(v_j)}} \\] To obtain the values of \\(\\bar u\\) and \\(\\bar v\\), we consider the following table. Recall that there are \\(R\\) levels of the variable \\(X\\) and \\(C\\) levels of the variable \\(Y\\). u1 Category of V Category of U 1 2 … C Total U 1 n11 n12 … n1c n1+ U1 2 n21 n22 … n2c n2+ U2 … … … … … … … R nr1 nr2 … nrc nr+ Ur Total n+1 n+2 … n+c n++ V V1 V2 … Vc In the table, \\(n_{rc},\\ r=1,2,\\ldots,R,\\ c=1,2,\\ldots,C\\) is the number of observations in the sample with scores \\(r\\) and \\(c\\). From the table we can understand the marginal distributions of \\(U\\) and \\(V\\), and we see that for \\(r=1,2,\\ldots,R,\\ c=1,2,\\ldots,C\\) \\[\\begin{align*} p(u_r) &amp;= \\frac{n_{r+}}{n} \\\\ \\\\ p(v_c) &amp;= \\frac{n_{+ c}}{n} \\\\ \\\\ p(u_r,v_c) &amp;= \\frac{n_{rc}}{n} \\\\ \\\\ \\bar u &amp;= \\sum\\limits_{r=1}^{R}u_i\\frac{n_{r+}}{n} \\\\ \\\\ \\bar v &amp;= \\sum\\limits_{c=1}^{C}v_i\\frac{n_{+ c}}{n} \\end{align*}\\] With these observations, we can derive the value of \\(r\\) as \\[\\begin{align*} r &amp;= \\frac{\\widehat{Cov}(U,V)}{\\sqrt{s_U^2s_V^2}} \\\\ &amp;= \\frac{\\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_r-\\bar u)(v_c-\\bar v)n_{rc}}{n-1}} {\\sqrt{\\frac{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2}{n-1} \\frac{\\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}{n-1}}} \\\\ &amp;= \\frac{\\frac{1}{n-1}\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_r-\\bar u)(v_c-\\bar v)n_{rc}} {\\frac{1}{n-1}\\sqrt{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2 \\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}(u_r-\\bar u)(v_c-\\bar v)n_{rc}} {\\sqrt{\\sum\\limits_{r=1}^{R}(u_r-\\bar u)^2 \\sum\\limits_{c=1}^{C}(v_c-\\bar v)^2}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_rv_c-u_r\\bar v-\\bar uv_c+\\bar u\\bar v)n_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C} (u_rv_cn_{rc}-u_r\\bar vn_{rc}-\\bar uv_cn_{rc}+\\bar u\\bar vn_{rc})} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_r\\bar vn_{rc} - \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}\\bar uv_cn_{rc} + \\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}\\bar u\\bar vn_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\bar v\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rn_{rc} - \\bar u\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}v_cn_{rc} + \\bar u\\bar v\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}n_{rc}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\bar v\\sum\\limits_{r=1}^{R}u_rn_{r+} - \\bar u\\sum\\limits_{c=1}^{C}v_cn_{+ c} + \\bar u\\bar vn} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\sum\\limits_{r=1}^{R} u_rn_{r+}}{n} - \\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n} + n\\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n^2}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{2\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n} + \\frac{\\sum\\limits_{r=1}^{R}u_rn_{r+}\\sum\\limits_{c=1}^{C} v_cn_{+ c}}{n}} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\\\ &amp;= \\frac{\\sum\\limits_{r=1}^{R}\\sum\\limits_{c=1}^{C}u_rv_cn_{rc} - \\frac{1}{n}(\\sum\\limits_{r=1}^{R}u_rn_{r+}) (\\sum\\limits_{c=1}^{C}v_cn_{+ c})} {\\sqrt{\\bigg(\\sum\\limits_{r=1}^{R}u_r^2n_{r+} - \\frac{1}{n}\\Big(\\sum\\limits_{r=1}^{R}u_rn_{r+}\\Big)^2\\bigg) \\bigg(\\sum\\limits_{c=1}^{C}v_c^2n_{+ c} - \\frac{1}{n}\\Big(\\sum\\limits_{c=1}^{C}v_cn_{+ c}\\Big)^2\\bigg)}} \\end{align*}\\] "],
["mcnemar-test.html", "21 McNemar Test 21.1 Sample Size Calculations for Paired Design 21.2 Power Calculation for Paired Design", " 21 McNemar Test This chapter only represents work that needed to be done for a specific application. Some of the formulas and equations provided are not necessarily coherent with the articles originally published on the topic. The majority of the work was derived from Connor’s 1987 paper. This chapter could benefit from a great deal of improvement and additional explanation. The McNemar Test compares proportions of related samples in which the outcome for each sample is a binary response. Th response is the same in each sample. Related samples may mean subjects from one sample are matched with subjects with similiar qualities (subjects are related, but outcomes are not); or it may mean that subjects are paired with themselves, as in a pre-post design (outcomes are related because they are taken on the same subject).\\ Trial 2 1 0 Trial 1 1 p11 p10 p1 0 p01 p00 1 - p1 p2 1 - p2 The table demonstrates the possible outcomes of such a experiment. Suppose \\(Y_i\\) deontes the outcome of Trial 1 and \\(Y_j\\) denotes the outcome of Trial 2. \\(Y_{ij}\\) denotes the outcome of the first and second trials, that is \\(Y_{ij} = Y_i \\cap Y_j\\). Then: \\[\\begin{align*} p_{11} &amp;= P(Y_i = 1 \\cap Y_j = 1) \\\\ p_{10} &amp;= P(Y_i = 1 \\cap Y_j = 0) \\\\ p_{01} &amp;= P(Y_i = 0 \\cap Y_j = 1) \\\\ p_{00} &amp;= P(Y_i = 0 \\cap Y_j = 0) \\\\ \\end{align*}\\] Furthermore \\[\\begin{align*} p_1 &amp;= p_{11} + p_{10} &amp;= P(Y_i = 1) \\\\ p_2 &amp;= p_{11} + p_{01} &amp;= P(Y_j = 1) \\end{align*}\\] In upcoming sections, the values of the difference and sum of \\(p_1\\) and \\(p_2\\) will be important, so we define \\[\\begin{align*} \\delta &amp;= p_1 - p_2 &amp;= (p_{11} + p_{10}) - (p_{11} + p_{01}) &amp;= p_{10} - p_{01} \\\\ \\\\ \\\\ \\psi &amp;= p_1 + p_2 &amp;= (p_{11} + p_{10}) + (p_{11} + p_{01}) &amp;= 2p_{11} + p_{10} + p_{01} \\end{align*}\\] 21.1 Sample Size Calculations for Paired Design Three methods of calculating power for McNemar’s Test have been presented. Miettenen proposed a method of estimating the power in 1968. Duffy provided the exact power in 1984. Connor provided an additional method of estimating the power in 1987. 21.1.1 Miettenen’s Sample Size Calculation Miettenen was the first to provide a popular power calculation for McNemar’s test with a paired-design. Duffy would later show that this calculation tends to under-estimate the power. Subsequently, sample sizes derived from this calculation are generally lower than is needed to obtain the designed power. Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). Now suppose we wish to determine the sample size \\(n_m\\) (for Miettenen method) required to find a change in proportion from \\(p_1\\) to \\(p_2\\) with significance \\(\\alpha\\) and power \\(1-\\beta\\). The required sample size is calculated by: \\[ n_m = \\frac{\\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2}{\\delta^2} \\] 21.1.2 Connor’s Sample Size Calculation Connor proposed a method for sample size calculation in addition to Miettenen’s. Connor’s method tends to over-estimate the power. Subsequently, sample sizes derived from this calculation are generally higher than is actually needed to obtain the designed power. Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). Now suppose we wish to determine the sample size \\(n_c\\) (for Miettenen method) required to find a change in proportion from \\(p_1\\) to \\(p_2\\) with significance \\(\\alpha\\) and power \\(1-\\beta\\). The required sample size is calculated by: \\[ n_c = \\frac{\\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2}{\\delta^2} \\] 21.2 Power Calculation for Paired Design The following power calculations are derived in a backward fashion. In the application I had at the time, I needed to calculate sample sizes, and also wanted to allow functionality in my R function to obtain power with a supplied sample size. Since I had the sample size equations, I solved for the power. Normally this would be done the other way around, ie, take the power function and solve for \\(n\\). In the future, this should be revised appropriately. 21.2.1 Power Calculation for Miettenen Method Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). The power function can be found from the sample size equation by: \\[\\begin{align*} n_m &amp;= \\frac{\\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2} {\\delta^2} \\\\ \\Rightarrow n_m\\delta^2 &amp;= \\Big( Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\Big)^2 \\\\ \\Rightarrow \\sqrt{n_m}\\delta &amp;= Z_\\alpha \\psi^{1/2} + Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\\\ \\Rightarrow \\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2} &amp;= Z_\\beta \\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} \\\\ \\Rightarrow \\frac{\\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2}}{\\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2}} &amp;= Z_\\beta \\\\ \\Rightarrow \\Phi^{-1}\\bigg(\\frac{\\sqrt{n_m}\\delta - Z_\\alpha \\psi^{1/2}} {\\big(\\psi - \\delta^2 (3+\\psi) / (4\\psi) \\big)^{1/2} }\\bigg) &amp;= 1 - \\beta \\end{align*}\\] 21.2.2 Power Calculation for Connor Method Let \\(\\alpha\\) be the probability of Type I Error, and let \\(\\beta\\) be the probability of Type II Error. Furthermore, let \\(Z_\\alpha = \\Phi(1-\\alpha)\\) and \\(Z_\\beta = \\Phi(1-\\beta)\\). The power function can be found from the sample size equation by: \\[\\begin{align*} n_c &amp;= \\frac{\\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2}{\\delta^2} \\\\ \\Rightarrow n_c\\delta^2 &amp;= \\big( Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\big)^2 \\\\ \\Rightarrow \\sqrt{n_c\\delta} &amp;= Z_\\alpha \\psi^{1/2} + Z_\\beta (\\psi - \\delta^2)^{1/2} \\\\ \\Rightarrow \\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2} &amp;= Z_\\beta (\\psi - \\delta^2)^{1/2} \\\\ \\Rightarrow \\frac{\\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2}}{(\\psi - \\delta^2)^{1/2}} &amp;= Z_\\beta \\\\ \\Rightarrow \\Phi^{-1}\\bigg(\\frac{\\sqrt{n_c\\delta} - Z_\\alpha \\psi^{1/2}}{(\\psi - \\delta^2)^{1/2}}\\bigg) &amp;= 1 - \\beta \\end{align*}\\] "],
["moments-and-moment-generating-functions.html", "22 Moments and Moment Generating Functions 22.1 Definitions of Moments 22.2 Moment Generating Functions", " 22 Moments and Moment Generating Functions 22.1 Definitions of Moments 22.1.1 Definition: General Definition of Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about some point \\(c\\) is defined to be \\(E[(X-c)^k]\\). There are two moments that are of particular use in statistics. First, the moment of \\(X\\) about the origin; second, the moment of \\(X\\) about the mean. 22.1.2 Definition: Ordinary Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about the origin is defined to be \\(E[(X-0)^k] = E(X^k)\\). 22.1.3 Definition: Central Moments The \\(k^{th}\\) moment of a random variable \\(X\\) about the mean \\(\\mu\\) is defined to be \\(E[(X-\\mu)^k]\\). Using these definitions we can derive the first three central moments as follows: \\[\\begin{align*} E[(X-\\mu)^1] &amp;= E(X - \\mu) \\\\ &amp;= E(X) - \\mu \\\\ &amp;= E(X) - E(X) \\\\ \\\\ E[(X-\\mu)^2] &amp;= E[(X-\\mu)(X-\\mu)] \\\\ &amp;= E(X^2-\\mu X-\\mu X+\\mu^2) \\\\ &amp;= E(X^2-2\\mu X+\\mu^2) \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\\\ \\\\ \\\\ E[(X-\\mu)^3] &amp;= E[(X-\\mu)(X-\\mu)(X-\\mu)] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)(X-\\mu)] \\\\ &amp;= E(X^3-\\mu X^2-2\\mu X^2+2\\mu^2X+\\mu^2X+\\mu^3) \\\\ &amp;= E(X^3-3\\mu X^2+3\\mu^2X-\\mu^3) \\\\ &amp;= E(X^3) - E(3\\mu X^2) + E(3\\mu^2X) - E(\\mu^3) \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 3\\mu^2E(X) - \\mu^3 \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 3\\mu^3 - \\mu^3 \\\\ &amp;= E(X^3) - 3\\mu E(X^2) + 2\\mu^3 \\end{align*}\\] It should be noticed that with all of these results, the moment about the mean can be evaluated by finding the ordinary moments. Thus, if we can find a consistent way to generate ordinary moments , we may use these results to find various parameters of a distribution. 22.2 Moment Generating Functions 22.2.1 Definition: Moment Generating Function The moment generating function of a random variable, denoted \\(M_X(t)\\), is defined to be: \\[ M_X(t) = E(e^{tX}) \\] The moment generating function of \\(X\\) is said to exist if for any positive constant \\(c,\\ M_X(t)\\) is finite for \\(|t|&lt;c\\). The definition can be expanded to \\[\\begin{align*} M_X(t) &amp;= E(e^{tX} \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}e^{tx_i}p(x_i) \\\\ ^{[1]} &amp;= \\sum\\limits_{i=1}^{\\infty}[\\frac{(tx_i)^0}{0!}+\\frac{(tx_i)^1}{1!} + \\frac{(tx_i)^2}{2!}+\\frac{(tx_i)^3}{3!}+\\cdots]p(x_i) \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}[1+tx_i+\\frac{(tx_i)^2}{2!} + \\frac{(tx_i)^3}{3!}+\\cdots]p(x_i) \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}[p(x_i)+tx_ip(x_i) + \\frac{(tx_i)^2}{2!}p(x_i)+\\frac{(tx_i)^3}{3!}p(x_i) + \\cdots] \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}p(x_i)+\\sum\\limits_{i=1}^{\\infty}tx_ip(x_i) + \\sum\\limits_{i=1}^{\\infty}\\frac{(tx_i)^2}{2!}p(x_i) + \\sum\\limits_{i=1}^{\\infty}\\frac{(tx_i)^3}{3!}p(x_i) + \\cdots \\\\ &amp;= \\sum\\limits_{i=1}^{\\infty}p(x_i)+t\\sum\\limits_{i=1}^{\\infty}x_ip(x_i) + \\frac{t^2}{2!}\\sum\\limits_{i=1}^{\\infty}x_i^2p(x_i) + \\frac{t^3}{3!}\\sum\\limits_{i=1}^{\\infty}x_i^3p(x_i)+\\cdots \\\\ &amp;= 1 + tE(X) \\\\ &amp;= \\frac{t^2}{2!}E(X^2) + \\frac{t^3}{3!} + \\cdots \\end{align*}\\] Taylor Series Expansion: \\(e^x=\\frac{x^0}{0!}+\\frac{x^1}{1!}+\\frac{x^2}{2!}\\cdots = 1+x+\\frac{x^2}{2!}+\\cdots\\) 22.2.2 Theorem: Extraction of Moments from Moment Generating Functions Let \\(M_X^{(k)}(t)\\) denote the \\(k^{th}\\) derivative of \\(M_X(t)\\) with respect to \\(t\\). Then \\(M_X^{(k)}(0)=E(X^k)\\). Proof: \\[\\begin{align*} M_X(t) &amp;= 1 + tE(X) \\\\ &amp;= \\frac{t^2}{2!}E(X^2) + \\frac{t^3}{3!} + \\cdots \\\\ \\\\ \\\\ M_X^{(1)}(t) &amp;= 0 + E(X) + \\frac{2t}{2!}E(X^2) + \\frac{3t^2}{3!}E(X^3) + \\cdots \\\\ &amp;= E(X) + tE(X^2) + \\frac{t^2}{2!}E(X^3) + \\cdots \\\\ \\\\ \\\\ M_X^{(2)}(t) &amp;= 0 + E(X^2) + \\frac{2t}{2!}E(X^3) + \\frac{3t^2}{3!}E(X^4) + \\cdots \\\\ &amp;= E(X^2) + tE(X^3) + \\frac{t^2}{2!}E(X^4) + \\cdots \\\\ \\vdots \\\\ \\\\ \\\\ M_X^{(k)}(t) &amp;= 0 + E(X^k) + \\frac{2t}{2!}E(X^{k+1}) + \\frac{3t^2}{3!}E(X^{k+2}) + \\cdots \\\\ &amp;= E(X^k) + tE(X^{k+1}) + \\frac{t^2}{2!}E(X^{k+2}) + \\cdots \\\\ \\\\ \\\\ M_X^{(1)}(0) &amp;= 0 + E(X) + \\frac{2\\cdot 0}{2!}E(X^2) + \\frac{3\\cdot 0t^2}{3!}E(X^3) + \\cdots \\\\ &amp;= E(X)\\\\ \\\\ \\\\ M_X^{(2)}(0) &amp;= 0 + E(X^2) + \\frac{2\\cdot 0}{2!}E(X^3) + \\frac{3\\cdot 0^2}{3!}E(X^4) + \\cdots \\\\ &amp;= E(X^2) \\\\ \\\\ \\\\ \\vdots \\\\ \\\\ \\\\ M_X^{(0)}(t) &amp;= 0 + E(X^k) + \\frac{2\\cdot 0}{2!}E(X^{k+1}) + \\frac{3\\cdot 0^2}{3!}E(X^{k+2}) + \\cdots \\\\ &amp;= E(X^k) \\end{align*}\\] "],
["multinomial-distribution.html", "23 Multinomial Distribution 23.1 Cumulative Distribution Function 23.2 Expected Values", " 23 Multinomial Distribution Let \\(E_1,E_2,\\ldots,E_k\\) be mutually exclusive and exhaustive events and define a multinomial experiment to have the following characteristics: The experiment consists of \\(N\\) indepedendent trials. The outcome of each trial belongs to exactly one \\(E_j,\\ j=1,2,\\ldots,k\\). The probability that an outcome belongs to event \\(E_j\\) is \\(p_j\\). Let \\(X_{ij}=\\left\\{ \\begin{array}{ll} 1&amp; \\rm if\\ the\\ outcome\\ of\\ the\\ \\it i^{th}\\ \\rm trial\\ belongs\\ to\\ \\it E_j.\\\\ 0&amp; otherwise \\end{array} \\right.\\) and let \\(n_j=\\sum\\limits_{i=1}^{N}X_{ij}\\). Under these conditions, \\(N=\\sum\\limits_{j=1}^{k}n_j\\). By Lemma 8.0.1 the number of ways to partition \\(N\\) into the \\(k\\) events, without respect to order, is \\(\\frac{N!}{n_1!n_2!\\cdots n_k!}\\). So the probabilitiy of any particular outcome of the experiment is \\[p(n_1,n_2,\\ldots,n_{k-1}) = \\frac{N!}{n_1!n_2!\\cdots n_{k-1}!n^\\prime!} p_1^{n_1}p_2^{n_2}\\cdots p_{(k-1)}p^{\\prime n^\\prime} \\] where \\(n^\\prime = N - n_1 - n_2 - \\cdots - n_{k-1}\\) and \\(p^\\prime = 1 - p_1 - p_2 - \\cdots - p_{k-1}\\). In other words, the entire distribution is defined by the first \\(k-1\\) terms. 23.1 Cumulative Distribution Function \\[P(n_1,n_2,\\ldots,n_{k-1}) = \\sum\\limits_{n_1=0}^{N} \\sum\\limits_{n_2=0}^{N-n_1} \\cdots \\sum\\limits_{n_{k-1}=0}^{N^\\prime} \\frac{N!}{n_1!n_2!\\cdots n_{k-1}!n^\\prime!} p_1^{n_1}p_2^{n_2}\\cdots p_{k-1}^{n_{k-1}}p^{\\prime n^\\prime}\\] where \\(N^\\prime=N-n_1-n_2-\\cdots n_{k-1}\\). 23.2 Expected Values Since this is a multivariate distribution, we discuss finding the expected values for each variate \\(n_j\\) as opposed to an overall mean.\\ \\(n_j\\) is a random variable from a multinomial distribution that specifies how many of the \\(N\\) observations were of type \\(j\\). Each of the \\(N\\) observations willf all into exactly one type, so we can conclude that an observation is either of type \\(j\\) or it isn’t. Also, it is of type \\(j\\) with probability \\(p_j\\), and each trial is independent. Thus, we may consider \\(n_j\\) a binomial random variable and \\(E(n_j)=Np_j\\) and \\(V(n_j)=Np_j(1-p_j)\\). Now we must derive the Covariance of \\(n_j\\). We begin by defnining the random variables for \\(j\\neq m\\): \\[X_i=\\left\\{ \\begin{array}{ll} 1 &amp; \\rm if\\ trial \\it\\ i\\ \\rm results\\ in\\ type\\ \\it j.\\\\ 0 &amp; otherwise \\end{array} \\right. \\] \\[Y_i=\\left\\{ \\begin{array}{ll} 1 &amp; \\rm if\\ trial \\it\\ i\\ \\rm results\\ in\\ type\\ \\it m.\\\\ 0 &amp; otherwise \\end{array} \\right. \\] and let \\(n_j=\\sum\\limits_{i=1}^{n}X_i\\) and \\(n_m=\\sum\\limits_{i=1}^{n}Y_i\\). Since \\(X_i\\) and \\(Y_i\\) cannot simultaneously equal 1, \\(X_i\\cdot Y_i=0\\) for all \\(i\\). We thus have the following results so far: \\[E(X_i\\cdot Y_i) = 0\\] \\[E(X_i) = p_j\\] \\[E(Y_i) = p_m\\] \\(Cov(X_i,Y_i) = 0\\) if \\(i\\neq j\\) because the trials are independent \\(Cov(X_i,Y_i) = E(X_i\\cdot Y_i) - E(X_i)E(Y_i) = 0 - p_j p_m = -p_j p_m\\). (\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\) (Theorem 10.2.2) Using these results we find the Covariance of \\(n_j\\) and \\(n_m\\). \\[\\begin{align*} Cov(n_j,n_m) &amp;= \\sum\\limits_{j=1}^{N}\\sum\\limits_{m=1}^{N}Cov(X_i,Y_i) \\\\ &amp;= \\sum\\limits_{i=1}^{N}Cov(X_iY_i) + \\sum\\sum\\limits_{i\\neq j}Cov(X_i,Y_i) \\\\ &amp;= \\sum\\limits_{i=1}^{n}-p_jp_m + \\sum\\sum\\limits_{i\\neq j}0=-np_jp_m \\end{align*}\\] The Expected Values of the \\(p_j\\)’s can be found by \\[\\begin{align*} E(\\hat p_j) &amp;= E(\\frac{n_j}{N}) \\\\ &amp;= \\frac{1}{N}E(n_j) \\\\ &amp;= \\frac{1}{N}Np_j \\\\ &amp;= p_j \\\\ \\\\ \\\\ V(\\hat p_j) &amp;= V(\\frac{n_j}{N}) \\\\ &amp;= \\frac{1}{N^2}V(n_j) \\\\ &amp;= \\frac{1}{N^2}Np_j(1-p_j) \\\\ &amp;= \\frac{p_j(1-p_j)}{N} \\\\ \\\\ \\\\ Cov(\\hat p_j,\\hat p_m) &amp;= Cov(\\frac{n_j}{N},\\frac{n_m}{N}) \\\\ &amp;= \\frac{1}{N^2}Cov(n_j,n_m) \\\\ &amp;= \\frac{1}{N^2}(-Np_jp_m) \\\\ &amp;= \\frac{-p_jp_m}{N} \\end{align*}\\] "],
["real-number-system.html", "24 Real Number System 24.1 Historical Note 24.2 The Field of Real Numbers 24.3 Proof that the Field of Rationals is not Complete 24.4 Preliminary Results in the Field of Real Numbers", " 24 Real Number System This chapter was prepared by Steve MacDonald. 24.1 Historical Note The first axiom system known in the history of mathematics was Euclid’s aximoatic development of plane geometry. Euclid’s treatment began with some primitive or undefined terms and some assumed statements, which he called axioms and postulates. For Euclid an axiom was a general “self-evident” truth, such as “The whole is greater than any of its parts” or “equals added to equals are equal,” whereas a postulate was an assumed statement about the relationships among the primitives of his system, such as “Two points determine exactly one line.” From these undefined terms and basic assumptions a whole body of other statements, called theorems was deduced. For centuries Euclidean Geometry, which was assumed to be the true description of physical reality, remained the only mathematical systems with such an axiomatic foundation. Then in the nineteenth century, spurred by Lobatchevsky and others who discovered that by modifying the postulates another logically consistent geometry could be constructed, matematicians began to apply this deductive approach to other branches of mathematics. Not only did this work do much to organize and clarify such familiar disciplines as number theory, analysis, and algebra, but it helped develop new areas of mathematics such as topology. Note that today we do not make Euclid’s distinction between axiom and postulate, using the terms synonymously. As an example of this deductive approach, we now want to give an axiomatic description of the real numbers system and thus place a logical foundation under many of the “rules” you learned in high school algebra. 24.2 The Field of Real Numbers 24.2.1 Definition: The Field of Real Numbers The Field of Real Numbers is a set \\(\\Re\\) of objects called numbers together with two well-defined binary operations, called addition, denoted by +, and multiplication, denoted by \\(\\cdot\\) or juxtaposition, satisfying the Field Axioms. (By well-defined, we mean that if \\(s=s^\\prime\\) and \\(t=t^\\prime\\), then \\(s+t=s^\\prime + t^\\prime\\).) 24.2.2 Field Axioms (Closure for addition) For each pair \\(x,y,\\in\\Re\\), there exists a unique object in \\(\\Re\\), called the sum of \\(x\\) and \\(y\\) denoted by \\(x+y\\). (Associative law for addition) For all \\(x,y,z\\in\\Re,\\ (x+y)=z=x+(y+z)\\). (Additive identity) There exists and object \\(0\\in\\Re\\) such that for all \\(x\\in\\Re,\\ x+0=x=0+x\\). (Additive inverse) For each \\(x\\in\\Re\\), there exists some \\(y\\in\\Re\\) such that \\(x+y=0=y+x\\). We will usually denote the additive inverse of \\(x\\) by \\(-x\\). (Commutative law of addition) For all \\(x,y\\in\\Re,\\ x+y=y+x\\). (Closure for multiplication) For each pair \\(,y,\\in\\Re\\), there exists a unique object in \\(\\Re\\), called the product of \\(x\\) and \\(y\\) and denoted by \\(x\\cdot y\\) or \\(xy\\). (Associative law of multiplication) For all \\(x,y,z\\in\\Re,\\ x\\cdot(y\\cdot z)=(x\\cdot y)\\cdot z\\). (Multiplicative identity) There exists an object \\(1\\in\\Re\\) such that for all \\(x\\in\\Re,\\ 1\\cdot x=x=x\\cdot 1\\). (Multiplicative inverse) For each \\(x\\in\\Re\\) such that \\(x\\neq 0\\), there exists an object \\(y\\in\\Re\\) such that \\(x\\cdot y=1=y\\cdot x\\). We will usually denote the multiplicative inverse of \\(x\\) by \\(x^{-1}\\). (Commutative law of multiplication) For all \\(x,y\\in\\Re,\\ x\\cdot y=y\\cdot x\\). (Distributive law of multiplication over addition) For all \\(x,y,z\\in\\Re, x\\cdot(y+z)=x\\cdot y+x\\cdot z\\). (Positive Elements) There exists a nonempty subset \\(\\Re^+\\subset\\Re\\) closed under \\(+\\) and \\(\\cdot\\). That is, for all \\(x,y\\in\\Re^+,\\ x+y\\in\\Re\\) and \\(x\\cdot y\\in\\Re^+\\). (Trichotomy) For any \\(x\\in\\Re\\), exactly one of these three cases holds: \\(x\\in\\Re^+,\\ -x\\in\\Re^+\\), or \\(x=0\\). 24.2.3 Definiton: Less Than (or Equal To) Let \\(x,y\\in\\Re\\). We say that \\(x\\) is less than \\(y\\), written \\(x&lt;y\\), provided \\(y+-x\\in\\Re^+\\). We say that \\(x\\) is less than or equal \\(y\\) iff and only iff \\(x&lt;y\\) or \\(x=y\\). (Also, \\(x\\) is said to be greater than \\(y\\) if \\(y&lt;x\\).) 24.2.4 Definition: Bounded Above (and Below) A set \\(A\\) of real numbers is said to be bounded above if there exists some \\(b\\in\\Re\\) such that \\(x\\leq b,\\ \\forall x\\in A\\). In this case, \\(b\\) is called an upper bound for \\(A\\). (Bounded below and lower bound are defined similarly.) 24.2.5 Definition: Least Upper (and Lower) Bound Let \\(A\\) be a set of real numbers bounded above. An element \\(\\beta\\in\\Re\\) is called the least upper bound, often written lub for \\(A\\) iff and only if \\(\\beta\\) is an upper bound for \\(A\\) and \\(\\beta\\leq b\\) for every \\(b\\) which is an upper bound for \\(A\\). (A greatest lower bound is defined similarly.) 24.2.6 Completeness Axiom Every nonempty subset of \\(\\Re\\) having an upper bound has a least upper bound. 24.3 Proof that the Field of Rationals is not Complete Let \\(A=\\left\\{p\\in Q^+|p^2&lt;2\\right\\}\\) and \\(B=\\left\\{p\\in Q^+|p^2&gt;2\\right\\}\\). We claim that \\(A\\) has no largest element and that \\(B\\) has no smallest element; i.e., given that \\(p\\in A\\), we can find some \\(q\\in A\\) with \\(q&gt;p\\); and given any \\(p\\in B\\), we can find some \\(q\\in B\\) with \\(q&lt;p\\). For any \\(p\\in Q^+\\), let \\[q &amp;=p-\\frac{p^2-2}{p+2} &amp;=\\frac{p^2+2p-p^2+2}{p+2} &amp;=\\frac{2p+2}{p+2}\\] Then \\[q^2-2 &amp;=\\frac{4p^2+8p+r-2p^2-8p-8}{(p+2)^2} &amp;=\\frac{2p^2-4}{(p+2)^2} &amp;=\\frac{2(p+2)}{(p+2)^2}\\] Now if \\(p\\in A,\\ p^2-2&lt;0\\), so \\(-\\frac{p^2-2}{p+2}&gt;0\\), whence \\(q=p-\\frac{p^2-2}{p+2}&gt;p\\). But \\(q^2-2=\\frac{2(p^2-2)}{(p+2)^2}&lt;0\\) implies that \\(q^2&lt;2\\). Thus \\(q\\in A\\) and \\(q&gt;p\\). On the other hand, if \\(p\\in B\\), then \\(p^2-2&gt;0\\), so \\(q=p-\\frac{p^2-2}{p+2}&lt;p\\). But \\(q^2-2=\\frac{2(p^2-2)}{(p+2)^2}&gt;0\\) implies that \\(q\\in B\\). Here \\(q\\in B\\) and \\(q&lt;p\\). Now it is clear that every member of \\(B\\) is an upper bound for \\(A\\), and every member of \\(A\\) is a lower bound for set \\(B\\). It is also clear from the above demonstration that among the rationals Q, the nonempty set \\(A\\) has no least upper bound; and the nonempty set \\(B\\) has no greatest lower bound among the rationals . Therefore we have shown that the ordered field of rational numbers does not satisfy the conditions of the Completeness Axiom . Thus it is the Completeness Axiom that distinguishes the ordered field of real numbers from the ordered field of rational numbers. 24.4 Preliminary Results in the Field of Real Numbers 24.4.1 Theorem: Uniqueness of Identities Identity elements are unique. Proof: Suppose \\(u \\star a=a\\star u=a\\) and \\(e\\star a=a\\star e=a,\\ \\forall a\\in\\Re\\). Then \\(u=u\\star e=e\\). 24.4.2 Theorem 2: Uniqueness of Inverses If \\(\\star\\) is an associative operation, inverse elements for \\(\\star\\) are unique. Proof: Suppose \\(a\\star a_1=a_1\\star a=e\\) and \\(a\\star a_2=a_2\\star a=e\\), where \\(e\\) is the identity element for the operation \\(\\star\\). Then \\(a_1=a_1\\star e=a_1\\star(a\\star a_2)=(a_1\\star a)\\star a_2)=e\\star a_2=a_2\\). 24.4.3 Theorem: Left Cancellation Law If \\(a\\) has an inverse \\(a^\\prime\\) with respect to the associative operation \\(\\star\\), and \\(a\\star b=a\\star c\\), then \\(b=c\\). Proof: Suppose \\(a\\star b=a\\star c\\). Then \\[ a^\\prime\\star(a\\star b) &amp;= a^\\prime\\star(a\\star c)\\\\ \\Rightarrow(a^\\prime\\star a)\\star b &amp;= (a^\\prime\\star a)\\star c\\\\ \\Rightarrow e\\star b &amp;= e\\star c\\\\ \\Rightarrow b &amp;= c \\] 24.4.4 Corollary: Right Cancellation In the field \\((\\Re,+,\\cdot),\\ a+b=a+c\\Rightarrow b=c\\), and if \\(a\\neq 0\\) and \\(ab=ac\\), then \\(b=c\\). Proof: The Corollary is proved using commutativity and Left Cancellation. 24.4.5 Lemma \\[-0=0\\]. Proof: By axiom 4 \\(-0+0=0\\). Because 0 is the additive identity, \\(-0+0=-0\\). Therefore \\(-0=-0+0=0\\). 24.4.6 Theorem \\(\\forall a\\in\\Re,\\ a&gt;0\\) if and only if \\(a\\in\\Re^+\\). Proof: Suppose \\(0&lt;a\\). Then \\(a-0\\in\\Re^+\\), but \\(a-0=a\\), so \\(a\\in\\Re^+\\). Conversely, suppose \\(a\\in\\Re^+\\). Then \\(a-0=a\\in\\Re^+\\), so \\(0&lt;a\\). 24.4.7 Theorem \\(\\forall x\\in\\Re,\\ x\\cdot 0=0\\). Proof: \\[ 0 + x\\cdot 0 &amp;= x\\cdot 0\\\\ &amp;= x\\cdot(0+0)\\\\ &amp;= x\\cdot 0 + x\\cdot 0\\\\ Rightarrow 0 &amp;= x\\cdot 0 \\] 24.4.8 Theorem \\(\\forall x\\in\\Re,\\ -(-x)=x\\). Proof: \\(-(-x)+(-x) = 0\\) and \\(0 = x+(-x)\\). So \\[-(-x)+(-x) &amp;= x+(-x) \\Rightarrow -(-x) &amp;= x \\] 24.4.9 Theorem \\[\\forall x,y\\in\\Re,\\ x\\cdot(-y) = -(x\\cdot y)=(-x)\\cdot y\\] Proof: \\[ x(-y)+xy &amp;= x(-y+y) &amp;= x\\cdot 0 \\\\ &amp;= 0 \\\\ &amp;= -(xy)+xy\\\\ \\Rightarrow x(-y)+xy &amp;= -(xy)+xy\\\\ \\Rightarrow x(-y) &amp;= -(xy) \\] Similarly, \\[ (-x)y+xy &amp;= (-x+x)y \\\\ &amp;= 0\\cdot y \\\\ &amp;= 0 \\\\ &amp;= -(xy)+xy\\\\ \\Rightarrow (-x)y &amp;= -(xy) \\] By transitivity, \\((-x)y = -(xy) = x(-y)\\). 24.4.10 Theorem \\[\\forall x\\in\\Re,\\ (-1)\\cdot x=-x\\] Proof: By Theorem 24.4.9 \\[ (-1)x &amp;= -(1x) &amp;= -x \\] 24.4.11 Corollary \\[\\forall x\\in\\Re,\\ (-1)\\cdot(-x)=x\\] Proof: By 24.4.10 \\[(-1)(-x) = -(-x) = x \\] 24.4.12 Theorem \\[\\forall x\\in\\Re,\\ (-x)(-x)=x\\cdot x\\] Proof: By Corollary 24.4.11 \\[(-x)(-x) &amp;= -(x(-x)) \\\\ &amp;= -((-x)x) \\\\ &amp;= -(-(x\\cdot x)) \\\\ &amp;= (x\\cdot x) \\\\ &amp;= x\\cdot x \\] 24.4.13 Theorem Let \\(x\\) and \\(y\\) be any real numbers. Then exactly one of the following is true: \\(x&gt;y\\) \\(x=y\\) \\(x&lt;y\\) Proof: By Axiom 1, \\(\\Re\\) is closed under addition. Thus, since \\(x,y\\in\\Re\\) , \\(x+(-y)\\in\\Re\\). By Trichotomy, \\(x+(-y)\\in\\Re,\\ -(x+(-y))\\in\\Re\\), or \\(x+(-y)=0\\). \\(x+(-y)\\in\\Re\\ \\Rightarrow x&gt;y\\) (Definition 24.2.3) \\(-(x+(-y))=(-x)+y\\in\\Re\\ \\Rightarrow x&lt;y\\) (Axiom 4) \\(x+(-y)=0\\ \\Rightarrow x=y\\) (Definition 24.2.3). 24.4.14 Theorem \\[\\forall a,b,c\\in\\Re,\\ (a&lt;b \\wedge b&lt;c)\\Rightarrow a&lt;c\\] Proof: \\[ a&lt;b &amp;\\wedge b&lt;c \\\\ \\Rightarrow b-a&gt;0 &amp;\\wedge c-b&gt;0 \\\\ \\Rightarrow^{[1]}(b-a)+(c-b)&gt;0 \\\\ \\Rightarrow (c-b)+(b-a)&gt;0 \\\\ \\Rightarrow c-b+b-a&gt;0 \\\\ \\Rightarrow c-a&gt;0 \\\\ \\Rightarrow a&lt;c \\] Axiom 12 \\(\\forall a,b,c \\in\\Re,\\ (a&lt;b \\ \\Rightarrow a+c&lt;b+c)\\).\\ \\ Proof: \\ \\ \\(a&lt;b\\ \\Rightarrow\\ b-a&gt;0\\\\\\\\ \\indent\\Rightarrow b-c+c-a&gt;0\\ \\Rightarrow b+c-a-c&gt;0\\\\\\\\ \\indent\\Rightarrow (b+c)-(a+c)&gt;0\\ \\Rightarrow a+c&lt;b+c\\ \\) \\(\\forall a,b,c\\in\\Re,\\ (a&lt;b\\wedge c&gt;0)\\Rightarrow ac&lt;bc\\).\\ \\ Proof: \\ \\ $a0 c(b-a)&gt;0 bc-ac&gt;0 ac0 c(b-a)&lt;0 bc-ba&lt;0 bc&lt;ba$. If \\(a&gt;b&gt;0\\) and \\(c&gt;d&gt;0\\), then \\(ac&gt;bd\\).\\ \\ Proof: \\ \\ \\(a&gt;b\\wedge c&gt;d\\ \\Rightarrow a-b&gt;0\\wedge c-d&gt;0\\\\ \\indent\\Rightarrow c(a-b)&gt;0\\wedge b(c-d)&gt;0\\ \\Rightarrow ac-bc&gt;0\\wedge bc-bd&gt;0\\\\ \\indent\\Rightarrow ac-bc+bc-bd&gt;0\\ \\Rightarrow ac-bd&gt;0\\ \\Rightarrow ac&gt;bd\\). In a field containing at least two elements, \\(1\\in\\Re^+\\).\\ \\ _Proof: If \\(x&gt;0\\), then \\(x^{-1}&gt;0\\).\\ \\ Proof: \\ \\ Since \\(x^{-1}\\) has an inverse \\(x\\), we know that \\(x^{-1}\\neq 0\\). Hence, by Axiom 13, either \\(x^{-1}&gt;0\\) or \\(x^{-1}&lt;0\\). Suppose \\(x^{-1}&lt;0\\). Then \\(-x^{-1}\\in\\Re^+\\), and since \\(\\Re^+\\) is closed under multiplication, \\((-x^{-1})\\cdot x\\in\\Re^+\\). Now by Theorem 7, \\((-x^{-1})\\cdot x=-(x^{-1}\\cdot x)=-1\\), so this would imply that \\(-1\\in\\Re^+\\), in contradction to Theorem . Since \\(x^{-1}&lt;0\\) must be be false, we conclude that \\(x^{-1}&gt;0\\) whenever \\(x&gt;0\\). "],
["skew-normal-distribution.html", "25 Skew-Normal Distribution 25.1 Preliminary Theorems 25.2 Lemma: A Symmetry Theorem 25.3 Lemma 25.4 Expected Values 25.5 Estimation of \\(\\lambda\\)", " 25 Skew-Normal Distribution 25.1 Preliminary Theorems 25.2 Lemma: A Symmetry Theorem Suppose the pdf of \\(X\\), \\(f_X\\) is symmetric about 0. Let \\(w(\\cdot)\\) be any odd function. Then the pdf of \\(Y=w(X)\\), \\(f_Y\\), is also symmetric about 0. Proof: Recall that if a pdf is symmetric about zero, it must demonstrate the property \\(P(T\\leq t)=P(T\\geq-t)\\). Since \\(f_X\\) is symmetric, we know \\[\\begin{align*} P(X\\leq x) &amp;= P(X\\geq-x) \\\\ \\Rightarrow P\\big[ w(X)\\leq w(x)\\big] &amp;= P\\big[ w(X)\\geq w(-x)\\big] ^{[1]} &amp;= P\\big[ w(X)\\geq-w(x)\\big] \\\\ &amp;= P(Y\\leq y) \\\\ &amp;= P(Y\\geq-y) \\end{align*}\\] Thus \\(f_Y\\) is symmetric about 0. By the definition of an odd function \\(f(-x)=-f(x)\\). 25.3 Lemma Let \\(f_0\\) be a one-dimensional probability density function symmetric over 0. Also, let \\(G\\) be a one dimensional probability distribution function such that \\(G^\\prime\\) exists and is a density function symmetric over 0. Then \\[ \\begin{array}{rl} f(x) = 2 f_0(x) G\\big(w(x)\\big) &amp; (-\\infty&lt;x&lt;\\infty)\\\\ \\end{array} \\] is a probability density function for any odd fuction \\(w(\\cdot)\\).\\ Proof: Let \\(X\\sim f_0\\) and \\(Y\\sim G^\\prime\\). Now consider the random variable \\(X-w(Y)|Y\\). When \\(Y\\) is fixed, \\(X-w(Y)\\) is an odd function of \\(X\\) and, by Lemma , \\(X-w(Y)\\) is symmetric over 0. Thus, \\[\\begin{align*} \\frac{1}{2} &amp;= P\\big(X-w(Y)\\leq0|Y\\big) \\\\ ^{[1]} &amp;= E\\Big[ P\\big(X-w(Y)\\leq0|Y\\big) \\Big] \\\\ \\Rightarrow \\frac{1}{2} &amp;= E\\big[ P\\big( X\\leq w(Y)|Y\\big) \\big]\\\\ \\Rightarrow \\frac{1}{2} &amp;= \\int\\limits_{-\\infty}^{\\infty} P\\big(X \\leq w(Y)|Y\\big) p(x) dx\\\\ \\Rightarrow \\frac{1}{2} &amp;= \\int\\limits_{-\\infty}^{\\infty} G\\big( w(Y) \\big) f_0(x)dx\\\\ \\Rightarrow 1 &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} f_0(X) G\\big( w(Y) \\big) dx \\end{align*}\\] So \\(f(x) = 2 f_0(x)G\\big(w(y)\\big)\\) is a valid density function for all \\(x,\\ x\\in\\Re\\). The expected value of \\(P(X\\leq 0)=\\frac{1}{2}\\) when \\(X\\) is distributed symmetric over 0. 25.4 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{-\\infty}^{\\infty} x \\cdot 2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\Bigg[ \\int\\limits_{-\\infty}^{\\alpha x} \\frac{1}{\\sqrt{s\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}dt \\Bigg]dx \\\\ &amp;= 2\\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{2}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\end{align*}\\] But \\(\\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}\\) cannot be integrated in closed form, so the solution must be found with numerical methods. \\[\\begin{align*} E(X^2) &amp;= \\int\\limits_{-\\infty}^{\\infty} x^2 \\cdot 2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x^2 f(x) \\Phi(\\alpha x)dx \\\\ &amp;= 2 \\int\\limits_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\Bigg[ \\int\\limits_{-\\infty}^{\\alpha x} \\frac{1}{\\sqrt{s\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}dt \\Bigg]dx \\\\ &amp;= 2\\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\frac{1}{\\sqrt{2\\pi}} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{2}{2\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\\\ &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\alpha x} x^2 \\exp\\bigg\\{ -\\frac{x^2}{2} \\bigg\\} \\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\} dt dx \\end{align*}\\] But \\(\\exp\\bigg\\{ -\\frac{t^2}{2} \\bigg\\}\\) cannot be integrated in closed form, so the solution must be found with numerical methods. 25.5 Estimation of \\(\\lambda\\) Using the Moment Generating Function, it was shown that the skew of the Skew-Normal distribution can be calculated as \\[ S=sign(\\lambda) \\bigg(2 - \\frac{\\pi}{2}\\bigg) \\Bigg(\\frac{\\lambda^2} {\\frac{pi}{2}+(\\frac{pi}{2}-1)\\lambda^2}\\Bigg)^{\\frac{3}{2}} \\] where \\(S\\) denotes the skew of the distribution. Given a value of skew for the distribution, a link can be made back to \\(\\lambda\\). We begin by noticing that the following process is identical regardless of the sign of \\(\\lambda\\). It is presented here as if \\(\\lambda&gt;0\\) \\[\\begin{align*} S &amp;= \\bigg( 2-\\frac{\\pi}{2} \\bigg) \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg)^{3/2} \\\\ \\Rightarrow \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg) &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg)^{3/2} \\\\ \\Rightarrow \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3} &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg) \\\\ \\Rightarrow T &amp;= \\Bigg( \\frac{\\lambda^2} {\\frac{\\pi}{2}+\\big(\\frac{\\pi}{2}-1\\big) \\lambda^2} \\Bigg) \\\\ \\Rightarrow \\bigg( \\frac{\\pi}{2} + \\Big(\\frac{\\pi}{2}-1\\Big) \\lambda^2 \\Bigg)T &amp;= \\lambda^2 \\\\ \\Rightarrow \\frac{\\pi}{2}T + \\Big( \\frac{\\pi}{2}-1 \\Big) \\lambda^2 T &amp;= \\lambda^2 \\\\ \\Rightarrow \\frac{\\pi}{2}T &amp;= \\lambda^2 - \\Big( \\frac{\\pi}{2}-1 \\Big) \\lambda^2 T \\\\ \\Rightarrow \\frac{\\pi}{2}T &amp;= \\lambda^2 \\bigg(1 - \\Big( \\frac{\\pi}{2}-1 \\Big) T \\bigg) \\\\ \\Rightarrow \\lambda^2 &amp;= \\frac{ \\frac{\\pi}{2}T } { 1-\\Big( \\frac{\\pi}{2}-1 \\Big) T } \\\\ \\Rightarrow \\lambda^2 &amp;= \\frac{ \\frac{\\pi}{2} \\Big( \\frac{S}{2-\\frac{\\pi}{2}} \\Big)^{2/3} } { 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3}\\\\ } \\\\ \\Rightarrow \\lambda &amp;= \\frac{ \\sqrt{\\frac{\\pi}{2}} \\Big( \\frac{S}{2-\\frac{\\pi}{2}} \\Big)^{1/3} } { \\sqrt{ 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3}} } \\end{align*}\\] Let \\(T=\\big( \\frac{S}{2-\\frac{\\pi}{2}} \\big)^{2/3}\\) This equation is only defined for certain values of \\(S\\). In particular, \\(S\\) cannot be a number such that the denominator is 0, nor can the that which appears under the radical be negative. These two restrictions can be collapsed, and the equation is defined so long as \\[\\begin{align*} 1-\\Big( \\frac{\\pi}{2}-1 \\Big) \\bigg(\\frac{S}{2-\\frac{\\pi}{2}}\\bigg)^{2/3} &gt; 0 \\\\ \\Rightarrow 1 &gt; \\Big( \\frac{\\pi}{2}-1 \\Big) \\bigg(\\frac{S}{2-\\frac{\\pi}{2}}\\bigg)^{2/3} \\\\ \\Rightarrow \\Big( \\frac{\\pi}{2}-1 \\Big)^{-1} &gt; \\bigg( \\frac{S}{2-\\frac{\\pi}{2}} \\bigg)^{2/3} \\\\ \\Rightarrow \\frac{ \\Big( 2-\\frac{\\pi}{2} \\Big)^{2/3} } { \\frac{\\pi}{2}-1 } &gt; S^{2/3} \\\\ \\Rightarrow \\frac{ 2-\\frac{\\pi}{2} } { \\Big( \\frac{\\pi}{2}-1 \\Big)^{3/2} } &gt; S \\\\ \\Rightarrow -.9952 &lt; S &lt; .9952 \\end{align*}\\] We notice that the endpoints of this interval are approximations. Ideally, the interval would span from -1 to 1, as most estimators of skew provide a value in that interval–values close to negative one denoting a strong left skew; values close to one denoting a strong right skew; 0 denoting perfect symmetry. Although this relationship is not perfect, it is quite close to what we would like, and can be practically implemented. "],
["somers-d.html", "26 Somers’ D 26.1 Theorems for Somers’ \\(D\\)", " 26 Somers’ D Somers’ \\(D\\) has an asymptotically \\(Normal\\) Distribution . It may take any value between -1 and 1. It is used to measure classification agreement between a predictor and outcome variable. Somers’ \\(D\\) is related to a form of a concordance index. Concrodance in measured between 0 and 1 and can effectively be calculated by rescaling Somers’ \\(D\\). The rescaling can be accomplished by: \\[ C = \\frac{D+1}{2} \\] 26.1 Theorems for Somers’ \\(D\\) 26.1.1 Theorem: Distribution of Somers’ Derived Concordance Let \\(D \\sim\\) Normal\\((\\mu, \\sigma^2)\\). Then $C \\(Normal\\)(,)$. Proof: \\[ D \\sim Normal(\\mu,\\sigma^2) \\Rightarrow (D+1) \\sim Normal(\\mu+1,\\sigma^2) \\\\ (D+1) \\sim Normal(\\mu+1,\\sigma^2) \\Rightarrow \\frac{D+1}{2} \\sim Normal(\\frac{\\mu+1}{2}, \\frac{\\sigma^2}{4}) \\] By definition, \\(C=\\frac{D+1}{2}\\), so \\(C \\sim Normal(\\frac{\\mu+1}{2},\\frac{\\sigma^2}{4})\\). Note: when the dependent variable is a binary response, the Concordance Index is equal to the area under the Receiver Operator Characteristic (ROC) curve, or AUC. "],
["summation.html", "27 Summation 27.1 Theorems of Summation", " 27 Summation 27.1 Theorems of Summation 27.1.1 Theorem If \\(c\\) is a constant then \\[\\sum\\limits_{i=1}^{n}c = nc\\] Proof: \\[ \\sum\\limits_{i=1}^{n}c = \\underbrace{c+c+\\cdots+c}_{n\\ \\rm terms} = nc \\] 27.1.2 Theorem If \\(a_1,a_2,\\ldots,a_n\\) are real numbers and \\(c\\) is a constant, then \\[ \\sum\\limits_{i=1}^{n}ca_i = c\\sum\\limits_{i=1}^{n}a_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}ca_i &amp;= ca_1 + ca_2 + \\cdots + ca_n \\\\ &amp;= c(a_1+a_2+\\cdots+a_n) \\\\ &amp;= c\\sum\\limits_{i=1}^{n}a_i \\end{align*}\\] 27.1.3 Theorem If \\(a_1,_2,\\ldots,a_n\\) are real numbers and \\(b_1,b_2,\\ldots,b_n\\) are real numbers, then \\[ \\sum\\limits_{i=1}^{n}(a_i+b_i) = \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}(a_i+b_i) &amp;= a_1 + b_1 + a_2 + b_2 + \\cdots + a_n + b_n \\\\ &amp;= a_1 + a_2 + \\cdots + a_n + b_1 + b_2 + \\cdots + b_n \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i + \\sum\\limits_{i=1}^{n}b_i \\end{align*}\\] 27.1.4 Theorem If \\(a_i\\) and \\(b_j\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j = a_{+} b_{+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b_j &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_j\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_i b_{+} \\\\ &amp;= b_{+} \\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= a_{+} b_{+} \\end{align*}\\] 27.1.5 Theorem If \\(a_i\\) is a real number for \\(i=1,2,\\ldots,n\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b = m a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_i b &amp;= \\sum\\limits_{i=1}^{n}m a_i b \\\\ &amp;= m b\\sum\\limits_{i=1}^{n}a_i \\\\ &amp;= m a_{+} b \\end{align*}\\] 27.1.6 Theorem If \\(a_j\\) is a real number for \\(j=1,2,\\ldots,m\\) and \\(b\\) is a real number, then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b = n a_{+} b \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_j b &amp;= \\sum\\limits_{i=1}^{n}\\bigg( b \\sum\\limits_{j=1}^{m} a_j \\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_{+}b \\\\ &amp;= n a_{+} b \\end{align*}\\] 27.1.7 Theorem If \\(a_i\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} = \\sum\\limits_{i=1}^{n}a_ib_{i+} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_ib_{ij} &amp;= \\sum\\limits_{i=1}^{n}\\bigg(a_i\\sum\\limits_{j=1}^{m}b_{ij}\\bigg) \\\\ &amp;= \\sum\\limits_{i=1}^{n}a_ib_{i+} \\end{align*}\\] 27.1.8 Theorem If \\(a_j\\) and \\(b_{ij}\\) are real numbers for \\(i=1,2,\\ldots,n\\), \\(j=1,2,\\ldots,m\\), then \\[\\ \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} = \\sum\\limits_{i=1}^{n}a_jb_{+ j} \\] Proof: \\[\\begin{align*} \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{m}a_jb_{ij} &amp;= a_1b_{11}+a_2b_{12}+\\cdots+a_mb_{1m} \\\\ &amp; \\ \\ \\ \\ +a_1b_{21}+a_2b_{22}+\\cdots+a_mb_{2m} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_1b_{n1}+a_1b_{n1}+\\cdots+a_1b_{nm} \\\\ &amp;= a_1b_{11}+a_1b_{21}+\\cdots+a_1b_{n1} \\\\ &amp; \\ \\ \\ \\ +a_2b_{12}+a_2b_{22}+\\cdots+a_2b_{n2} \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_mb_{1m}+a_mb_{2m}+\\cdots+a_nb_{nm} \\\\ &amp;= a_1(b_{11}+b_{21}+\\cdots+b_{n1}) \\\\ &amp; \\ \\ \\ \\ +a_2(b_{12}+b_{22}+\\cdots+b_{n2}) \\\\ &amp; \\ \\ \\ \\ \\vdots \\\\ &amp; \\ \\ \\ \\ +a_m(b_{1m}+b_{2m}+\\cdots+b_{nm}) \\\\ &amp;= a_1b_{+ 1}+a_2b_{+ 2}+\\cdots+a_mb_{+ m} \\\\ &amp;=\\sum\\limits_{j=1}^{m}a_jb_{+ j} \\end{align*}\\] "],
["method-of-transformations.html", "28 Method of Transformations 28.1 Example: Cauchy Distribution", " 28 Method of Transformations Suppose we wish to find the distribution funciton for the random variable \\(Y\\) that is either a strictly increasing or strictly decreasing function (Such a function is sure to have an inverse, whereas a function like \\(Y=X^2\\) does not have an inverse).. If we know the distribution of \\(X\\), we may use the following to determine the cdf of \\(Y\\). \\[\\begin{align*} P(Y\\leq y) &amp;= P(h(X)\\leq y) \\\\ &amp;= P(h^{-1}(h(y))\\leq h^{-1}(x)) \\\\ &amp;= P(Y\\leq h^{-1}(x)) \\\\ \\Rightarrow F_Y(y) &amp;= F_X(h^{-1}(x)) \\end{align*}\\] The pdf can now be found by taking the deriviative of the cdf. \\[\\begin{align*} f_Y(y) &amp;= \\frac{d(F_Y(y))}{d y} \\\\ &amp;= \\frac{d F_Y(h^{-1}(y))}{d y} \\\\ &amp;= f_X(h^{-1}(y))\\frac{d(h^{-1}(y))}{dy} \\end{align*}\\] 28.1 Example: Cauchy Distribution Let \\(X\\) have the Uniform pdf \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{\\pi}, &amp; \\frac{-\\pi}{2}&lt;x&lt;\\frac{\\pi}{2}\\\\ 0 &amp; otherwise \\end{array}\\right. \\] Let \\(Y = \\tan (X)\\). The pdf of \\(Y\\) can be found as follows: \\[\\begin{align*} h(x) &amp;= \\tan (x) \\\\ \\Rightarrow h^{-1}(x) &amp;= \\tan^{-1}(x) \\\\ \\Rightarrow \\frac{d h^{-1}(x)}{d x} &amp;= \\frac{1}{1+x^2} \\\\ \\Rightarrow f_Y(y) &amp;= f_X(h^{-1}(y))\\frac{d(h^{-1}(y))}{d y} \\\\ &amp;= f_X(tan^{-1}(x))\\frac{1}{1+y^2} \\\\ &amp;= \\frac{1}{\\pi}\\frac{1}{1+y^2} \\\\ &amp;= \\frac{1}{\\pi(1+y^2)} \\end{align*}\\] The domain of \\(Y\\) is transformed \\[ \\frac{-\\pi}{2} &lt; x &lt; \\frac{-\\pi}{2} \\\\ \\Rightarrow \\tan\\big(\\frac{-\\pi}{2}\\big) &lt; \\tan(x) &lt; \\tan\\big(\\frac{-\\pi}{2}\\big) \\\\ \\Rightarrow -\\infty &lt; y &lt; \\infty \\] Thus the pdf of the \\(Y\\), known as the Cauchy distribution, is \\[f_Y(y) = \\frac{1}{\\pi(1+y^2)},\\ \\ -\\infty&lt;y&lt;\\infty \\] "],
["uniform-distribution.html", "29 Uniform Distribution 29.1 Probability Density Function 29.2 Cumulative Density Function 29.3 Expected Values 29.4 Moment Generating Function 29.5 Theorems for the Uniform Distribution 29.6 Validity of the Distribution", " 29 Uniform Distribution 29.1 Probability Density Function A random variable \\(X\\) is said to have a Uniform Distribution with parameters \\(a\\) and \\(b\\) if its pdf is \\[f(x)=\\left\\{ \\begin{array}{ll} \\frac{1}{b-a}, &amp; a\\leq x \\leq b\\\\ 0 &amp; elsewhere \\end{array} \\right. \\] 29.2 Cumulative Density Function \\[\\begin{align*} F(x) &amp;= \\int\\limits_{a}^{x}\\frac{1}{b-a}dt \\\\ &amp;= \\frac{t}{b-a}|_{a}^{x} \\\\ &amp;= \\frac{x}{b-a}-\\frac{a}{b-a} \\\\ &amp;= \\frac{x-a}{b-a} \\end{align*}\\] \\[F(x)=\\left\\{ \\begin{array}{lll} 0 &amp; x&lt;a\\\\ \\frac{x-a}{b-a},&amp; a\\leq x\\leq b\\\\ 1 &amp; elsewhere \\end{array}\\right. \\] Figure 29.1: The figures on the left and right display the Uniform probability and cumulative distirubtion functions, respectively, for \\(a=0, b=5\\). 29.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{a}^{b}x\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}x\\ dx \\\\ &amp;= \\frac{1}{b-a}\\cdot \\Big[\\frac{x^2}{2}\\Big]_a^b \\\\ &amp;= \\frac{1}{b-a}\\cdot\\Big[\\frac{b^2}{2}-\\frac{a^2}{2}\\Big] \\\\ &amp;= \\frac{1}{b-a}\\cdot \\frac{b^2-a^2}{2} \\\\ &amp;= \\frac{b^2-a^2}{2(b-a)} \\\\ &amp;= \\frac{(b-a)(b+a)}{2(b-a)} \\\\ &amp;= \\frac{b+a}{2} \\\\ \\\\ \\\\ E(X^2) &amp;= \\int\\limits_{a}^{b}x^2\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}x^2\\ \\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{x^3}{3}\\Big]_a^b \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{b^3-a^3}{3}\\Big] \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{(b-a)(b^2+ab+a^2)}{3}\\Big] \\\\ &amp;= \\frac{(b-a)(b^2+ab+a^2)}{3(b-a)} \\\\ &amp;= \\frac{(b^2+ab+a^2)}{3} \\\\ \\\\ \\\\ \\mu &amp;= E(X) \\\\ &amp;= \\frac{b+a}{1} \\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\frac{b^2+ab+a^2}{3} - \\frac{(b-a)^2}{4} \\\\ &amp;= \\frac{4(b^2+ab+a^2)-3(b+a)^2}{12} \\\\ &amp;= \\frac{4(b^2+ab+a^2-3(b^2+2ab+a^2)}{12} \\\\ &amp;= \\frac{4b^2+4ab+4a^2-3b^2-6ab-3a^2)}{12} \\\\ &amp;= \\frac{4b^2-3b^2+4ab-6ab+4a^2-3a^2}{12} \\\\ &amp;= \\frac{b^2-2a+a^2}{12}=\\frac{(b-a)^2}{12} \\end{align*}\\] 29.4 Moment Generating Function \\[\\begin{align*} M_X(t) &amp;= E(e^{tX})=\\int\\limits_{a}^{b}e^{tx}\\frac{1}{b-a}dx \\\\ &amp;= \\frac{1}{b-a}\\int\\limits_{a}^{b}e^{tx}dx \\\\ &amp;= \\frac{1}{b-a}\\Big[\\frac{e^{tb}-e^{ta}}{t}\\Big] \\\\ &amp;= \\frac{e^{t(b-a)}}{t(b-a)} \\end{align*}\\] \\(M_X^{(k)}(0)\\) will lead to an undefined operation (division by 0). Thus, in the case of the Uniform distribution, we are unable to use the method of moments to identify parameter values. 29.5 Theorems for the Uniform Distribution 29.6 Validity of the Distribution \\[\\int\\limits_{a}^{b}\\frac{1}{b-a} = 1\\] Proof: \\[\\begin{align*} \\int\\limits_{a}^{b}\\frac{1}{b-a} &amp;= \\frac{x}{b-a}\\Big|_a^b \\\\ &amp;= \\frac{b}{b-a}-\\frac{a}{b-a} \\\\ &amp;= \\frac{b-a}{b-a} \\\\ &amp;= 1 \\end{align*}\\] "],
["variance-parameter.html", "30 Variance Parameter 30.1 Defining Variance With Expected Values 30.2 Unbiased Estimator 30.3 Computational Formulae", " 30 Variance Parameter 30.1 Defining Variance With Expected Values In the case of a discrete random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\sum\\limits_{x=0}^{\\infty}(x-\\mu)^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2-2\\mu x+\\mu^2)p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}(x^2p(x)-2\\mu x\\cdot p(x)+\\mu^2p(x)) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\sum\\limits_{x=0}^{\\infty}2\\mu x\\cdot p(x) + \\sum\\limits_{x=0}^{\\infty}\\mu^2p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\sum\\limits_{x=0}^{\\infty}x\\cdot p(x) + \\mu^2\\sum\\limits_{x=0}^{\\infty}p(x) \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\sum\\limits_{x=0}^{\\infty}x^2p(x)-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2\\\\ \\end{align*}\\] In the case of a continuous random variable, the variance is \\[\\begin{align*} \\sigma^2 &amp;= \\int\\limits_{-\\infty}^{\\infty}(x-\\mu)^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2-2\\mu x+\\mu^2)f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}(x^2f(x)-2\\mu x\\cdot f(x)+\\mu^2f(x))dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\int\\limits_{-\\infty}^{\\infty}2\\mu x\\cdot f(x)dx + \\int\\limits_{-\\infty}^{\\infty}\\mu^2f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\int\\limits_{-\\infty}^{\\infty}x\\cdot f(x)dx + \\mu^2\\int\\limits_{-\\infty}^{\\infty}f(x)dx \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-2\\mu\\cdot\\mu+\\mu^2 \\\\ &amp;= \\int\\limits_{-\\infty}^{\\infty}x^2f(x)dx-\\mu^2 \\\\ &amp;= E(X^2)-E(X)^2 \\end{align*}\\] In general, these results may be summarized as follows: \\[\\begin{align*} \\sigma^2 &amp;= E[(X-\\mu)^2] \\\\ &amp;= E[(X^2-2\\mu X+\\mu^2)] \\\\ &amp;= E(X^2) - E(2\\mu X) + E(\\mu^2) \\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu\\cdot\\mu + \\mu^2 \\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align*}\\] 30.2 Unbiased Estimator \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg) &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2 \\frac{n(\\sum\\limits_{i=1}^{n}x_i)^2}{n^2}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2)-nE(\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n}\\Big[\\sum\\limits_{i=1}^{n}\\Big(\\sigma^2+\\mu^2\\Big) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big]\\\\\\\\ &amp;= \\frac{1}{n}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;=\\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n}\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Since \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n}\\Bigg)\\neq\\sigma^2\\) it is a biased estimator. Notice, however, that the bias can be eliminated by dividing by \\(n-1\\) instead of by \\(n\\) \\[\\begin{align*} E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg) &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - \\sum\\limits_{i=1}^{n}2\\bar x x_i+\\sum\\limits_{i=1}^{n}\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2 - 2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{(\\sum\\limits_{i=1}^{n}x_i)^2}{n}+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1} E\\Big(\\sum\\limits_{i=1}^{n}x_i^2- 2\\frac{n\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n^2} + n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-2n\\bar x^2+n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2-n\\bar x^2\\Big) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-E(n\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}E\\Big(\\sum\\limits_{i=1}^{n}x_i^2\\Big)-nE(\\bar x^2) \\\\ &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}E(x_i^2)-nE(\\bar x^2)\\Big] \\\\ ^{[1]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2)-nE(\\bar x^2)\\Big] \\\\ ^{[2]} &amp;= \\frac{1}{n-1}\\Big[\\sum\\limits_{i=1}^{n}(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\Big] \\\\ &amp;= \\frac{1}{n-1}(n\\sigma^2-n\\mu^2+\\sigma^2-n\\mu^2) \\\\ &amp;= \\frac{1}{n}(n\\sigma^2-\\sigma) \\\\ &amp;= \\frac{1}{n-1}(n-1)\\sigma^2 \\\\ &amp;= \\frac{n-1}{n-1}\\sigma^2 \\\\ &amp;=\\sigma^2 \\end{align*}\\] \\(V(X)=E(X^2)-E(X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(X^2)=V(X)+E(X)^2=\\sigma^2+\\mu^2\\) \\(V(\\bar X)=E(\\bar X^2)-E(\\bar X)^2\\) \\(\\ \\ \\ \\ \\Rightarrow E(\\bar X^2)=V(\\bar X)+E(\\bar X)^2 = \\frac{\\sigma^2}{n}+\\mu^2\\) By the Central Limit Theorem, \\(V(\\bar X)=\\frac{\\sigma^2}{n}\\) Thus \\(E\\Bigg(\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\Bigg)\\) is an unbiased estimator of \\(\\sigma^2\\), and we define the estimator \\[s^2= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1}\\] 30.3 Computational Formulae 30.3.1 Computational Formula for \\(\\sigma\\)^2 \\[\\begin{align*} \\sigma^2 &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{N}(x_i-\\mu)^2}{N} &amp;= \\frac{\\sum\\limits_{i=1}^{N}(x_i^2-2\\mu x_i+\\mu^2)}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-\\sum\\limits_{i=1}^{N}2\\mu x_i + \\sum\\limits_{i=1}^{N}\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\mu\\sum\\limits_{i=1}^{N}x_i+N\\mu^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2 -2\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\sum\\limits_{i=1}^{N}x_i + N\\Big(\\frac{\\sum\\limits_{i=1}^{N}x_i}{N}\\Big)^2}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N} x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N} + \\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{N}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{N}x_i\\Big)^2}{N}}{N} \\end{align*}\\] 30.3.2 Computational Formula for \\(s\\)^2 \\[\\begin{align*} s^2 &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] Proof: \\[\\begin{align*} \\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{n-1} &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x_i^2-2\\bar x x_i+\\bar x^2)}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\sum\\limits_{i=1}^{n}2\\bar x x_i + \\sum\\limits_{i=1}^{n}\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\bar x\\sum\\limits_{i=1}^{n}x_i+n\\bar x^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\sum\\limits_{i=1}^{n}x_i + n\\Big(\\frac{\\sum\\limits_{i=1}^{n}x_i}{n}\\Big)^2}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-2\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n} + \\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_i^2-\\frac{\\Big(\\sum\\limits_{i=1}^{n}x_i\\Big)^2}{n}}{n-1} \\end{align*}\\] "],
["weibull-distribution.html", "31 Weibull Distribution 31.1 Probability Distribution Function 31.2 Cumulative Distribution Function 31.3 Expected Values 31.4 Theorems for the Weibull Distribution", " 31 Weibull Distribution 31.1 Probability Distribution Function A random variable \\(X\\) is said to have a Weibull Distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its probability density function is: \\[f(x)=\\left\\{ \\begin{array}{ll} \\alpha\\beta x^{\\beta-1}e^{-\\alpha x^{\\beta}},&amp;0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 31.2 Cumulative Distribution Function \\[\\begin{align*} \\int\\limits_{0}^{x}\\alpha\\beta t^{\\beta-1}e^{-\\alpha t^\\beta}dt &amp;= \\alpha\\beta\\int\\limits_{0}^{x}t^{\\beta-1}e^{-\\alpha t^\\beta}dt \\\\ &amp;= \\alpha\\beta\\Big[\\frac{-1}{\\alpha\\beta}e^{-\\alpha t^\\beta}\\Big]_0^x \\\\ &amp;= \\alpha\\beta\\Big[\\frac{-1}{\\alpha\\beta}e^{-\\alpha x^\\beta} + \\frac{1}{\\alpha\\beta}\\Big] \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\big(-e^{-\\alpha x^\\beta}+1\\big) \\\\ &amp;= 1-e^{-\\alpha x^\\beta} \\end{align*}\\] Using this result, we can write the Cumulative Distribution Function as \\[F(x)=\\left\\{ \\begin{array}{ll} 1-e^{-\\alpha x^\\beta},&amp; 0&lt;x,\\ 0&lt;\\alpha,\\ 0&lt;\\beta\\\\ 0 &amp; otherwise \\end{array}\\right. \\] 31.3 Expected Values \\[\\begin{align*} E(X) &amp;= \\int\\limits_{0}^{\\infty}x\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^{\\beta}}dx \\\\ &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x x^{\\beta-1}e^{-\\alpha x^{\\beta}}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty} \\Big(\\big(\\frac{y}{\\alpha}\\big)^\\frac{1}{\\beta}\\Big)^\\beta e^{-y}\\frac{1}{\\alpha\\beta} \\Big(\\frac{y}{\\alpha}\\Big)^{\\frac{1}{\\beta}-1}dy \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty} \\Big(\\frac{y}{\\alpha}\\Big)^\\frac{\\beta+1}{\\beta} \\Big(\\frac{y}{\\alpha}\\Big)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\Big(\\frac{y}{\\alpha}\\Big) ^{\\frac{\\beta+1}{\\beta}-\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\Big(\\frac{y}{\\alpha}\\Big) ^{\\frac{\\beta+1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\frac{1}{\\alpha^{\\frac{\\beta+1}{\\beta}-\\frac{\\beta}{\\beta}}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+1}{\\beta}-1}e^{-y} \\\\ &amp;= \\alpha^{-\\frac{1}{\\beta}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+1}{\\beta}-1}e^{-y} \\\\ ^{[2]} &amp;= \\alpha^{-\\frac{1}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big) \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x=(\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx=\\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx =\\Gamma(\\alpha)\\) \\[\\begin{align*} E(X^2) &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^2x^{\\beta-1}e^{-\\alpha x^\\beta}dx \\\\ &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^{\\beta+1}e^{-\\alpha x^\\beta}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}\\Big(\\big(\\frac{y}{\\alpha}\\big)^\\frac{1}{\\beta}\\Big)^{\\beta+1} e^{-y}\\frac{1}{\\alpha\\beta}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}dy \\\\ &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty}\\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{\\beta+1}{\\beta}} \\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\bigg(\\frac{y}{\\alpha}\\bigg)^{\\frac{\\beta+1}{\\beta}+\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\frac{1}{\\alpha^{\\frac{\\beta+2}{\\beta}-\\frac{\\beta}{\\beta}}} \\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+2}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\int\\limits_{0}^{\\infty}y^{\\frac{\\beta+2}{\\beta}-1}e^{-y}dy \\\\ ^{[2]} = \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x=(\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx=\\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx =\\Gamma(\\alpha)\\) \\[\\begin{align*} \\mu &amp;= E(X) \\\\ &amp;= \\alpha^{-\\frac{1}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)\\\\ \\\\ \\\\ \\sigma^2 &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) - \\alpha^{-\\frac{2}{\\beta}}\\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)^2 \\\\ &amp;= \\alpha^{-\\frac{2}{\\beta}}\\Big[\\Gamma\\Big(\\frac{\\beta+2}{\\beta}\\Big) - \\Gamma\\Big(\\frac{\\beta+1}{\\beta}\\Big)^2\\Big] \\end{align*}\\] 31.4 Theorems for the Weibull Distribution 31.4.1 Validity of the Distribution \\[ \\int\\limits_{0}^{\\infty}\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^\\beta}dx = 1 \\] Proof: \\[\\begin{align*} \\int\\limits_{0}^{\\infty}\\alpha\\beta x^{\\beta-1}e^{-\\alpha x^\\beta}dx &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}x^{\\beta-1}e^{-\\alpha x^\\beta}dx \\\\ ^{[1]} &amp;= \\alpha\\beta\\int\\limits_{0}^{\\infty}\\Big(\\big(\\frac{y}{\\alpha}\\big)^ \\frac{1}{\\beta}\\Big)^{\\beta-1} e^{-y}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}\\frac{1}{\\alpha\\beta}dy &amp;= \\frac{\\alpha\\beta}{\\alpha\\beta}\\int\\limits_{0}^{\\infty}\\big(\\frac{y}{\\alpha}\\big)^\\frac{\\beta}{-1} \\big(\\frac{y}{\\alpha}\\big)^{\\frac{1}{\\beta}-1}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\big(\\frac{y}{\\alpha}\\big)^{\\frac{\\beta-1}{\\beta}+\\frac{1-\\beta}{\\beta}}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}\\frac{y^0}{\\alpha^0}e^{-y}dy \\\\ &amp;= \\int\\limits_{0}^{\\infty}y^{1-1}e^{-y}dy \\\\ ^{[2]} &amp;= \\Gamma(1)=1 \\end{align*}\\] \\(y=\\alpha x^\\beta\\ \\Rightarrow x = (\\frac{y}{\\alpha})^\\frac{1}{\\beta}\\ \\Rightarrow dx = \\frac{1}{\\alpha\\beta}(\\frac{y}{\\alpha})^{\\frac{1}{\\beta}-1}\\) \\(\\int\\limits_{0}^{\\infty}x^{\\alpha-1}e^{-x}dx = \\Gamma(\\alpha)\\) "]
]
