# Binomial Distribution

## Probability Mass Function
A random variable is said to follow a Binomial distribution with parameters $n$ and $p$ if its probability mass function is:

\[p(x)=
	\left\{
		\begin{array}{ll}
			{n \choose x} p^x (1-p)^{n-x},	& x=0,1,2,\ldots,n\\
			0 				& \mathrm{otherwise}
		\end{array}
	\right. \]
	
Where $n$ is the number of trials performed and $p$ is the probability of a success on each individual trial.

## Cumulative Mass Function

\[ P(x)=
	\left\{
		\begin{array} {lll}
			0							& x<0\\
			\sum\limits_{i=0}^{x} {n \choose i} p^i (1-p)^{n-i} 	& 0 \leq x=0,1,2,\ldots,n\\
			1 							& n\leq x
		\end{array}
	\right. \]
	
A recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:

$$\begin{align*} 
F(x+1)
	&= {n\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\
	&= \frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\
  &= \frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \cdot p^x \frac{(1-p)^{n-x}}{(1-p)} \\
  &= \frac{(n-x)n!}{(x+1)x!(n-x)!} \cdot \frac{p}{1-p} p^x (1-p)^{n-x} \\
  &= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\
  &= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot {n\choose x} p^x (1-p)^{n-x} \\
	&= \frac{p}{1-p} \cdot \frac{n-x}{x+1} \cdot F(x)
\end{align*}$$

```{r, echo = FALSE, fig.cap = 'The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $n=10$ and $p=.4$.'}
Binomial <- 
  data.frame(x = 0:10) %>%
  mutate(dbinom = dbinom(x, size = 10, p = 0.4),
         pbinom = pbinom(x, size = 10, p = 0.4)) %>%
  gather(cumulative, prob, -x) %>%
  mutate(cumulative = factor(cumulative,
                             levels = c("dbinom", "pbinom"),
                             labels = c("Probability Mass", "Cumulative Mass")))

ggplot(data = Binomial,
       mapping = aes(x = x)) + 
  geom_bar(mapping = aes(y = prob),
           stat = "identity",
           fill = palette[1]) + 
  facet_grid(~ cumulative) + 
  scale_x_continuous(breaks = 0:10) + 
  ylab("P(x)") + 
  theme_bw()
```

## Expected Values
$$\begin{align*}
E(X)
    	    &= \sum\limits_{x=0}^n x \cdot p(x) \\
    	    &= \sum\limits_{x=0}^n x {n\choose x} p^x (1-p)^{n-x} \\
	^{[1]}  &= \sum\limits_{x=0}^n x {n\choose x} p^x q^{n-x} \\
          &= 0 \cdot {n\choose 0}p^0q^n+1 \cdot {n\choose 1}p^1q^{n-1} 
    		        + \cdots + n{n\choose n}p^nq^{n-n}\\
     	    &= 0 + 1{n\choose 1}p^1q^{n-1} + 2{n\choose 2}p^2q^{n-2} 
    		        + \cdots + n{n\choose n}p^nq^{n-n}\\
    	    &= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\
    	    &= np [q^{n-1} + (n-1)pq^{n-2} + \cdots + p^{n-1}]\\
        	&= np \Big[{n-1\choose 0}p^0q^{n-1} + {n-1\choose 1}p^1q^{(n-1)-1}
        		+ \cdots + {n-1\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\Big]\\
        	&= np (\sum\limits_{x=0}^{n-1}{n-1\choose x}p^xq^{(n-1)-x}) \\
  ^{[2]}	&= np(p+q)^{n-1} \\
  ^{[1]}	&= np(p+(1-p))^{n-1} \\
    	    &= np(p+1-p)^{n-1} \\
        	&= np(1)^{n-1} \\
        	&= np(1) \\
        	&= np
\end{align*}$$

> 1. Let $q = (1 - p)$
> 2. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$

$$\begin{align*}
E(X^2)
	        &= \sum\limits_{x=0}^{n} x^2 p(x) \\
	        &= \sum\limits_{x=0}^{n} x^2 {n\choose x} p^x (1-p)^{n-x} \\
	^{[1]}  &= \sum\limits_{x=0}^{n} x^2 {n\choose x} p^x q^{n-x} \\
          &= 0^2 \frac{n!}{0!(n-0)!} p^0q^n + 1^2 \frac{n!}{1!(n-1)!} p^1q^{n-1}
		          + \cdots + n^2 \frac{n!}{n!(n-n)!} p^nq^{n-n} \\
        	&= 0 + 1 \frac{n!}{(n-1)!} pq^{n-1} + 2 \frac{n!}{1\cdot(n-2)!} p^2q^{n-2}
        		+ \cdots + n \frac{n!}{(n-1)!(n-n)!} p^n \\
          &= np \Big[1 \frac{(n-1)!}{(n-1)!} p^0q^{n-1}
        		+ 2 \frac{(n-1)!}{1(n-2)!} p^2q^{n-2}
        		+ \cdots + n \frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\Big] \\
          &= np \Big[1 \frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + 
                 	\cdots + n \frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\Big] \\
          &= np \sum\limits_{x=1}^{n} x {n-1\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\
  ^{[2]}  &= \sum\limits_{y=0}^{m} (y+1) {m \choose y} p^yq^{m-y} \\
          &= np \Big[ \sum\limits_{y=0}^{m} y {m \choose y} p^yq^{m-y} + {m \choose y} p^yq^{m-y}\Big] \\
          &= np \Big[ \sum\limits_{y=0}^{m} y {m \choose y} p^yq^{m-y} 
          		+ \sum\limits_{y=0}^{m} {m \choose y} p^yq^{m-y}\Big] \\
  ^{[3]}  &= np(mp+1) \\
        	&= np[(n-1)p+1] \\
        	&=np(np-p+1) \\
        	&=n^2p^2 - np^2 + np
\end{align*}$$
	
>1. $q = (1 - p)$
>2. Let $y = x - 1$ and $n = m + 1$  
>    $\Rightarrow$ $x = y + 1$ and $m = n - 1$
>3. $\sum\limits_{y=0}^{m}y{m \choose y}p^yq^{m-y}$
>		is of the form of the expected value of $Y$, and $E(Y)=mp=(n-1)p$.
>		$\sum\limits_{y=0}^{m}{m \choose y}p^yq^{m-y}$
>		is the sum of all probabilities over the domain of $Y$ which is 1.


$$\begin{align*}
\mu
	&= E(X) \\
	&= np \\
\\
\\
\sigma^2
	&= E(X^2) - E(X)^2 \\
	&= n^2p^2 - np^2 + np - n^2p^2 \\
	&= -np^2 + np \\
	&= np(-p-1) \\
  &= np(1-p)
\end{align*}$$

## Moment Generating Function

$$
\begin{align*} 
M_X(t)
          &= E(e^{tX})=\sum\limits_{x=0}^{n}e^{tx}p(x) \\
        	&= \sum\limits_{x=0}^{n}e^{tx}{n\choose x}p^x(1-p)^{n-x} \\
          &= \sum\limits_{x=0}^{n}{n\choose x}e^{tx}p^x(1-p)^{n-x} \\
	        &= \sum\limits_{x=0}^{n}{n\choose x}(pe^{tx})^x(1-p)^{n-x} \\
  ^{[1]}  &= [(1-p)+pe^t]^n
\end{align*}
$$
	
> 1. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$

$$
\begin{align*}
M_X^{(1)}(t) &= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\
\\
M_X^{(2)}(t) &= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\
             &= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\
\\
\\
E(X)
  &= M_X^{(1)}(0) \\
  &= n[(1-p)+pe^0]^{n-1}pe^0 \\
  &= n[1-p+p^{n-1}p\\
  &= n(1)^{n-1}p
  &= np\\
\\
\\
E(X^2)
  &= M_X^{(2)}(0) \\
  &= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\cdot0}[(1-p) + pe^0]^{n-2} \\
  &= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\
  &= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\
  &= np+n(n-1)p^2 \\
  &= np+(n^2-n)p^2 \\
  &= np + n^2 + n^2p^2 - np^2 \\
\\
\\
\mu
  &= E(X) \\
  &= np \\
\\
\\
\sigma^2
  &= E(X^2) - E(X)^2 \\
  &= np + n^2p^2 - np^2 - n^2p^2 \\
  &= np - np^2\\
  &= np(1-p)
\end{align*}$$



## Maximum Likelihood Estimator

Since $n$ is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for $n$.  The mean and variance can both be estimated from the single parameter $p$.

Let $X$ be a Binomial random variable with parameter $p$ and $n$ outcomes $(x_1,x_2,\ldots,x_n)$.  Let $x_i=0$ for a failure and $x_i=1$ for a success.  In other words, $X$ is the sum of $n$ Bernoulli trials with equal probability of success and $X=\sum\limits_{i=1}^{n}x_i$.

### Likelihood Function
$$\begin{align*} 
L(\theta)
	&= L(x_1,x_2,\ldots,x_n|\theta) \\
	&= P(x_1|\theta) P(x_2|\theta) \cdots P(x_n|\theta) \\
  &= [\theta^{x_1}(1-\theta)^{1-x_1}] [\theta^{x_2}(1-\theta)^{1-x_2}] \cdots
        [\theta^{x_n}(1-\theta)^{1-x_n}]\\
  &= \exp_\theta\bigg\{\sum\limits_{i=1}^{n}x_i\bigg\}
        \exp_{(1-\theta)}\bigg\{n-\sum\limits_{i=1}^{n}x_i\bigg\} \\
  &= \theta^X(1-\theta)^{n-X}
\end{align*}$$

### Log-likelihood Function
$$\begin{align*}
\ell(\theta)
	&= \ln L(\theta) \\
	&= \ln\big(\theta^X(1-\theta)^{n-X}\big) \\
	&= X\ln(\theta)+(n-X)\ln(1-\theta)
\end{align*}$$

### MLE for p

$$\begin{align*} 
                \frac{d\ell(p)}{d p} 
                            &= \frac{X}{p}-\frac{n-X}{1-p} \\
\\
\\
                       0   &= \frac{X}{p}-\frac{n-X}{1-p} \\
  \Rightarrow  \frac{X}{p} &= \frac{n-X}{1-p} \\
  \Rightarrow  (1-p)X      &= p(n-X) \\
  \Rightarrow  X-pX        &= np-pX \\
  \Rightarrow  X           &= np \\
  \Rightarrow  \frac{X}{n} &= p \\
\end{align*}$$

So $\displaystyle \hat p = \frac{X}{n} = \frac{1}{n}\sum\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $p$.


## Theorems for the Binomial Distribution

### Validity of the Distribution

$$\begin{align*}
\sum\limits_{x=0}^n{n\choose x}p^x(1-p)^{n-x}	= 1
\end{align*}$$

_Proof:_

$$\begin{align*}
\sum\limits_{x=0}^n {n\choose x} p^x (1-p)^{n-x} \\
	^{[1]}  &= \big(p + (1-p)\big)^n \\
	        &= (1)^n \\
	        &= 1 
\end{align*}$$

> 1. By the Binomial Theorem (\@ref(binomial-theorem-traditional)), $\sum\limits_{x=0}^n{n\choose x}a^xb^{n-x}=(a+b)^n$


### Sum of Binomial Random Variables

Let $X_1,X_2,\ldots,X_k$ be independent random variables where $X_i$ comes from a Binomial distribution with parameters $n_i$ and $p$.  That is $X_i\sim(n_i,p)$.

Let $Y = \sum\limits_{i=1}{k} X_i$.  Then $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,p)$.

_Proof:_

$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{t(x_1 + X_2 + \cdots + X_k)} \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_k}) \\ 
  &= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_k}) \\
  &= \prod\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\
	&= [(1-p)+pe^t]^{\sum\limits_{i=1}^{k}n_i}
\end{align*}$$

Which is the mgf of a Binomial random variable with parameters $\sum\limits_{i=1}^{k}n_i$ and $p$.  
Thus $Y\sim$Binomial$(\sum\limits_{i=1}^{k}n_i,p)$.

### Sum of Bernoulli Random Variables

Let $X_1,X_2,\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y = \sum\limits_{i=1}^{n}X_i$.  
Then $Y\sim$Binomial$(n,p)$

_Proof:_
$$\begin{align*}
M_Y(t)
	&= E(e^{tY}) \\
	&= E(e^{tX_1} e^{tX_2} \cdots e^{tX_n}) \\
	&= E(e^{tX_1}) E(e^{tX_2}) \cdots E(e^{tX_n}) \\
  &= (pe^t+(1-p))(pe^t+(1-p))\cdots (pe^t+(1-p)) \\
	&= (pe^t+(1-p))^n
\end{align*}$$

Which is the mgf of a Binomial random variable with parameters $n$ and $p$.  Thus, $Y\sim$ Binomial$(n,p)$. 
