{
    "collab_server" : "",
    "contents" : "# Binomial Distribution\n\n## Probability Mass Function\nA random variable is said to follow a Binomial distribution with parameters $n$ and $p$ if its probability mass function is:\n\n\\[p(x)=\n\t\\left\\{\n\t\t\\begin{array}{ll}\n\t\t\t{n \\choose x} p^x (1-p)^{n-x},\t& x=0,1,2,\\ldots,n\\\\\n\t\t\t0 \t\t\t\t& \\mathrm{otherwise}\n\t\t\\end{array}\n\t\\right. \\]\n\t\nWhere $n$ is the number of trials performed and $p$ is the probability of a success on each individual trial.\n\n## Cumulative Mass Function\n\n\\[ P(x)=\n\t\\left\\{\n\t\t\\begin{array} {lll}\n\t\t\t0\t\t\t\t\t\t\t& x<0\\\\\n\t\t\t\\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} \t& 0 \\leq x=0,1,2,\\ldots,n\\\\\n\t\t\t1 \t\t\t\t\t\t\t& n\\leq x\n\t\t\\end{array}\n\t\\right. \\]\n\t\nA recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:\n\n$$\\begin{align*} \nF(x+1)\n\t&= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)} \\\\\n\t&= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)} \\\\\n  &= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1} \\\\\n  &= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)} \\\\\n  &= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x} \\\\\n  &= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \\\\\n  &= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x} \\\\\n\t&= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x)\n\\end{align*}$$\n\n```{r, echo = FALSE, fig.cap = 'The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $n=10$ and $p=.4$.'}\nBinomial <- \n  data.frame(x = 0:10) %>%\n  mutate(dbinom = dbinom(x, size = 10, p = 0.4),\n         pbinom = pbinom(x, size = 10, p = 0.4)) %>%\n  gather(cumulative, prob, -x) %>%\n  mutate(cumulative = factor(cumulative,\n                             levels = c(\"dbinom\", \"pbinom\"),\n                             labels = c(\"Probability Mass\", \"Cumulative Mass\")))\n\nggplot(data = Binomial,\n       mapping = aes(x = x)) + \n  geom_bar(mapping = aes(y = prob),\n           stat = \"identity\",\n           fill = pallette[1]) + \n  facet_grid(~ cumulative) + \n  scale_x_continuous(breaks = 0:10) + \n  ylab(\"P(x)\") + \n  theme_bw()\n```\n\n## Expected Values\n$$\\begin{align*}\nE(X)\n    \t    &= \\sum\\limits_{x=0}^n x \\cdot p(x) \\\\\n    \t    &= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x} \\\\\n\t^{[1]}  &= \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x} \\\\\n          &= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} \n    \t\t        + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\\n     \t    &= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} \n    \t\t        + \\cdots + n{n\\choose n}p^nq^{n-n}\\\\\n    \t    &= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\\n    \t    &= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\\n        \t&= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1}\n        \t\t+ \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\\n        \t&= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x}) \\\\\n  ^{[2]}\t&= np(p+q)^{n-1} \\\\\n  ^{[1]}\t&= np(p+(1-p))^{n-1} \\\\\n    \t    &= np(p+1-p)^{n-1} \\\\\n        \t&= np(1)^{n-1} \\\\\n        \t&= np(1) \\\\\n        \t&= np\n\\end{align*}$$\n\n> 1. Let $q = (1 - p)$\n> 2. By the Binomial Theorem (\\@ref(binomial-theorem-traditional)), $\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n$\n\n$$\\begin{align*}\nE(X^2)\n\t        &= \\sum\\limits_{x=0}^{n} x^2 p(x) \\\\\n\t        &= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x} \\\\\n\t^{[1]}  &= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x} \\\\\n          &= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1}\n\t\t          + \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n} \\\\\n        \t&= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2}\n        \t\t+ \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n \\\\\n          &= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1}\n        \t\t+ 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2}\n        \t\t+ \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big] \\\\\n          &= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \n                 \t\\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big] \\\\\n          &= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)} \\\\\n  ^{[2]}  &= \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y} \\\\\n          &= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big] \\\\\n          &= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} \n          \t\t+ \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big] \\\\\n  ^{[3]}  &= np(mp+1) \\\\\n        \t&= np[(n-1)p+1] \\\\\n        \t&=np(np-p+1) \\\\\n        \t&=n^2p^2 - np^2 + np\n\\end{align*}$$\n\t\n>1. $q = (1 - p)$\n>2. Let $y = x - 1$ and $n = m + 1$  \n>    $\\Rightarrow$ $x = y + 1$ and $m = n - 1$\n>3. $\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}$\n>\t\tis of the form of the expected value of $Y$, and $E(Y)=mp=(n-1)p$.\n>\t\t$\\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}$\n>\t\tis the sum of all probabilities over the domain of $Y$ which is 1.\n\n\n$$\\begin{align*}\n\\mu\n\t&= E(X) \\\\\n\t&= np \\\\\n\\\\\n\\\\\n\\sigma^2\n\t&= E(X^2) - E(X)^2 \\\\\n\t&= n^2p^2 - np^2 + np - n^2p^2 \\\\\n\t&= -np^2 + np \\\\\n\t&= np(-p-1) \\\\\n  &= np(1-p)\n\\end{align*}$$\n\n## Moment Generating Function\n\n$$\n\\begin{align*} \nM_X(t)\n          &= E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x) \\\\\n        \t&= \\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x} \\\\\n          &= \\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x} \\\\\n\t        &= \\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x} \\\\\n  ^{[1]}  &= [(1-p)+pe^t]^n\n\\end{align*}\n$$\n\t\n> 1. By the Binomial Theorem (\\@ref(binomial-theorem-traditional)), $\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n$\n\n$$\n\\begin{align*}\nM_X^{(1)}(t) &= n[(1 - p) + pe^t] ^ {n - 1} pe^t\\\\\n\\\\\nM_X^{(2)}(t) &= n[(1-p) + pe^t] ^ {n-1} pe^t + n(n-1)[(1-p) + pe^t] ^ {n-2}(pe^t)^2\\\\\n             &= npe^t[(1-p) + pe^t] ^ {n-1} + n(n-1)pe^{2t}[(1-p) + pe^t] ^ {n-2}\\\\\n\\\\\n\\\\\nE(X)\n  &= M_X^{(1)}(0) \\\\\n  &= n[(1-p)+pe^0]^{n-1}pe^0 \\\\\n  &= n[1-p+p^{n-1}p\\\\\n  &= n(1)^{n-1}p\n  &= np\\\\\n\\\\\n\\\\\nE(X^2)\n  &= M_X^{(2)}(0) \\\\\n  &= npe^0 [(1-p) + pe^0]^{n-1} + n(n-2) pe^{2\\cdot0}[(1-p) + pe^0]^{n-2} \\\\\n  &= np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2} \\\\\n  &= np (1)^{n-1} + n(n-1) p^2 (1)^{n-2} \\\\\n  &= np+n(n-1)p^2 \\\\\n  &= np+(n^2-n)p^2 \\\\\n  &= np + n^2 + n^2p^2 - np^2 \\\\\n\\\\\n\\\\\n\\mu\n  &= E(X) \\\\\n  &= np \\\\\n\\\\\n\\\\\n\\sigma^2\n  &= E(X^2) - E(X)^2 \\\\\n  &= np + n^2p^2 - np^2 - n^2p^2 \\\\\n  &= np - np^2\\\\\n  &= np(1-p)\n\\end{align*}$$\n\n\n\n## Maximum Likelihood Estimator\n\nSince $n$ is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for $n$.  The mean and variance can both be estimated from the single parameter $p$.\n\nLet $X$ be a Binomial random variable with parameter $p$ and $n$ outcomes $(x_1,x_2,\\ldots,x_n)$.  Let $x_i=0$ for a failure and $x_i=1$ for a success.  In other words, $X$ is the sum of $n$ Bernoulli trials with equal probability of success and $X=\\sum\\limits_{i=1}^{n}x_i$.\n\n### Likelihood Function\n$$\\begin{align*} \nL(\\theta)\n\t&= L(x_1,x_2,\\ldots,x_n|\\theta) \\\\\n\t&= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta) \\\\\n  &= [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots\n        [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\\n  &= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\}\n        \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\\\\n  &= \\theta^X(1-\\theta)^{n-X}\n\\end{align*}$$\n\n### Log-likelihood Function\n$$\\begin{align*}\n\\ell(\\theta)\n\t&= \\ln L(\\theta) \\\\\n\t&= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big) \\\\\n\t&= X\\ln(\\theta)+(n-X)\\ln(1-\\theta)\n\\end{align*}$$\n\n### MLE for p\n\n$$\\begin{align*} \n                \\frac{d\\ell(p)}{d p} \n                            &= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\\n                       0   &= \\frac{X}{p}-\\frac{n-X}{1-p} \\\\\n  \\Rightarrow  \\frac{X}{p} &= \\frac{n-X}{1-p} \\\\\n  \\Rightarrow  (1-p)X      &= p(n-X) \\\\\n  \\Rightarrow  X-pX        &= np-pX \\\\\n  \\Rightarrow  X           &= np \\\\\n  \\Rightarrow  \\frac{X}{n} &= p \\\\\n\\end{align*}$$\n\nSo $\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $p$.\n\n\n## Theorems for the Binomial Distribution\n\n### Validity of the Distribution\n\n$$\\begin{align*}\n\\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x}\t= 1\n\\end{align*}$$\n\n_Proof:_\n\n$$\\begin{align*}\n\\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x} \\\\\n\t^{[1]}  &= \\big(p + (1-p)\\big)^n \\\\\n\t        &= (1)^n \\\\\n\t        &= 1 \n\\end{align*}$$\n\n> 1. By the Binomial Theorem (\\@ref(binomial-theorem-traditional)), $\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n$\n\n\n### Sum of Binomial Random Variables\n\nLet $X_1,X_2,\\ldots,X_k$ be independent random variables where $X_i$ comes from a Binomial distribution with parameters $n_i$ and $p$.  That is $X_i\\sim(n_i,p)$.\n\nLet $Y = \\sum\\limits_{i=1}{k} X_i$.  Then $Y\\sim$Binomial$(\\sum\\limits_{i=1}^{k}n_i,p)$.\n\n_Proof:_\n\n$$\\begin{align*}\nM_Y(t)\n\t&= E(e^{tY}) \\\\\n\t&= E(e^{t(x_1 + X_2 + \\cdots + X_k)} \\\\\n\t&= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k}) \\\\ \n  &= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k}) \\\\\n  &= \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i} \\\\\n\t&= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i}\n\\end{align*}$$\n\nWhich is the mgf of a Binomial random variable with parameters $\\sum\\limits_{i=1}^{k}n_i$ and $p$.  \nThus $Y\\sim$Binomial$(\\sum\\limits_{i=1}^{k}n_i,p)$.\n\n### Sum of Bernoulli Random Variables\n\nLet $X_1,X_2,\\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y = \\sum\\limits_{i=1}^{n}X_i$.  \nThen $Y\\sim$Binomial$(n,p)$\n\n_Proof:_\n$$\\begin{align*}\nM_Y(t)\n\t&= E(e^{tY}) \\\\\n\t&= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n}) \\\\\n\t&= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n}) \\\\\n  &= (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p)) \\\\\n\t&= (pe^t+(1-p))^n\n\\end{align*}$$\n\nWhich is the mgf of a Binomial random variable with parameters $n$ and $p$.  Thus, $Y\\sim$ Binomial$(n,p)$. \n",
    "created" : 1471006421687.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2735781300",
    "id" : "219FDCB4",
    "lastKnownWriteTime" : 1471006644,
    "last_content_update" : 1471006644372,
    "path" : "~/GitHub/ItCanBeShown/Binomial_Distribution.Rmd",
    "project_path" : "Binomial_Distribution.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}