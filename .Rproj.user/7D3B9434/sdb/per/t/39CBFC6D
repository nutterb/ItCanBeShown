{
    "collab_server" : "",
    "contents" : "\\chapter{The Binomial Distribution}\n\t\\label{Binomial}\n\t\\markboth{Binomial Distribution}{Probability Distributions and Theory}\nA random variable is said to follow a Binomial distribution with parameters $n$ and $p$ if its probability mass function is:\\\\\n\\[p(x)=\n\t\\left\\{\n\t\t\\begin{array}{ll}\n\t\t\t{n \\choose x} p^x (1-p)^{n-x},\t& x=0,1,2,\\ldots,n\\\\\n\t\t\t0 \t\t\t\t& \\mathrm{otherwise}\n\t\t\\end{array}\n\t\\right. \\]\nWhere $n$ is the number of trials performed and $p$ is the probability of a success on each individual trial.\n\n%New Section\n\\section{Cumulative Distribution Function}\n\t\\label{Binomial1}\n\\[ P(x)=\n\t\\left\\{\n\t\t\\begin{array} {lll}\n\t\t\t0\t\t\t\t\t\t\t& x<0\\\\\n\t\t\t\\sum\\limits_{i=0}^{x} {n \\choose i} p^i (1-p)^{n-i} \t& 0 \\leq x=0,1,2,\\ldots,n\\\\\n\t\t\t1 \t\t\t\t\t\t\t& n\\leq x\n\t\t\\end{array}\n\t\\right. \\]\nA recursive form of the cdf can be derived and has some usefulness in computer applications.  With it, one need only initiate the first value and additional cumulative probabilities can be calculated.  It is derived as follows:\\\\\n$\\displaystyle \nF(x+1)\n\t= {n\\choose x+1} p^{x+1} (1-p)^{n-(x+1)}\n\t= \\frac{n!}{(x+1)!(n-(x+1))!} p^{x+1} (1-p)^{n-(x+1)}\\\\\n\\indent\t= \\frac{n!}{(x+1)!(n-x-1)!} p^{x+1} (1-p)^{n-x-1}\\\\\n\\indent\t= \\frac{(n-x)n!}{(x+1)x!(n-x)(n-x-1)!} p \\cdot p^x \\frac{(1-p)^{n-x}}{(1-p)}\\\\\n\\indent\t= \\frac{(n-x)n!}{(x+1)x!(n-x)!} \\cdot \\frac{p}{1-p} p^x (1-p)^{n-x}\\\\\n\\indent\t= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\\\\\n\\indent\t= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot {n\\choose x} p^x (1-p)^{n-x}\n\t= \\frac{p}{1-p} \\cdot \\frac{n-x}{x+1} \\cdot F(x)$\\\\\n\n\\begin{figure}\n\t\\begin{center}\n\\includegraphics[height=2in,width=2in]{Figures/BinomialFigure1.jpg}\\includegraphics[height=2in,width=2in]{Figures/BinomialFigure2.jpg}\n\t\t\\label{BinomialFigure1}\n\t\t\\caption[Binomial PDF and CDF]{The graphs on the left and right show a Binomial Probability Distribution and Cumulative Distribution Function, respectively, with $n=10$ and $p=.4$.}\n\t\\end{center}\n\\end{figure}\n%New Section\n\\section{Expected Values}\n\t\\label{Binomial2}\n$\\displaystyle\nE(X)\n\t= \\sum\\limits_{x=0}^n x \\cdot p(x)\n\t= \\sum\\limits_{x=0}^n x {n\\choose x} p^x (1-p)^{n-x}\n\t= \\footnote[1]{Let $q=(1-p)$}\n\t\\ \\ \\sum\\limits_{x=0}^n x {n\\choose x} p^x q^{n-x}\\\\\\\\\n\\indent\t= 0 \\cdot {n\\choose 0}p^0q^n+1 \\cdot {n\\choose 1}p^1q^{n-1} \n\t\t+ \\cdots + n{n\\choose n}p^nq^{n-n}\\\\\\\\\n\\indent\t= 0 + 1{n\\choose 1}p^1q^{n-1} + 2{n\\choose 2}p^2q^{n-2} \n\t\t+ \\cdots + n{n\\choose n}p^nq^{n-n}\\\\\\\\\n\\indent\t= np^1 q^{n-1} + n(n-1)p^2q^{n-2} + \\cdots + n(n-1)p^{n-1}q^{n-(n-1)} + n p^n\\\\\\\\\n\\indent\t= np [q^{n-1} + (n-1)pq^{n-2} + \\cdots + p^{n-1}]\\\\\\\\\n\\indent\t= np \\Big[{n-1\\choose 0}p^0q^{n-1} + {n-1\\choose 1}p^1q^{(n-1)-1}\n\t\t+ \\cdots + {n-1\\choose n-1}p^{n-1}q^{(n-1)-(n-1)}\\Big]\\\\\\\\\n\\indent\t= np (\\sum\\limits_{x=0}^{n-1}{n-1\\choose x}p^xq^{(n-1)-x})\n\t= \\footnote[2]{The Binomial Theorem states that \n\t\t$\\sum\\limits_{x=0}^n{n\\choose x}a^xb^{n-x}=(a+b)^n$}\n\t\\ \\ np(p+q)^{n-1}\n\t= \\footnote[3]{$q=(1-p)$}\n\t\\ \\ np(p+(1-p))^{n-1}\\\\\\\\\n\\indent\t= np(p+1-p)^{n-1}\n\t= np(1)^{n-1}\n\t= np(1)\n\t= np\\\\\\\\$\n\\\\\n$\\displaystyle \nE(X^2)\n\t= \\sum\\limits_{x=0}^{n} x^2 p(x)\n\t= \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x (1-p)^{n-x}\n\t= \\footnote[1]{$q=(1-p)$}\n\t\\ \\ \\sum\\limits_{x=0}^{n} x^2 {n\\choose x} p^x q^{n-x}\\\\\\\\\n\\indent\t= 0^2 \\frac{n!}{0!(n-0)!} p^0q^n + 1^2 \\frac{n!}{1!(n-1)!} p^1q^{n-1}\n\t\t+ \\cdots + n^2 \\frac{n!}{n!(n-n)!} p^nq^{n-n}\\\\\\\\\n\\indent\t= 0 + 1 \\frac{n!}{(n-1)!} pq^{n-1} + 2 \\frac{n!}{1\\cdot(n-2)!} p^2q^{n-2}\n\t\t+ \\cdots + n \\frac{n!}{(n-1)!(n-n)!} p^n\\\\\\\\\n\\indent= np \\Big[1 \\frac{(n-1)!}{(n-1)!} p^0q^{n-1}\n\t\t+ 2 \\frac{(n-1)!}{1(n-2)!} p^2q^{n-2}\n\t\t+ \\cdots + n \\frac{(n-1)!}{(n-1)!(n-n)!} p^{n-1}\\Big]\\\\\\\\\n\\indent\t= np \\Big[1 \\frac{(n-1)!}{(1-1)!((n-1)-(-1-1))!} p^{1-1} q^{n-1} + \\\\\\\\\n\\indent\\indent \t\\cdots + n \\frac{(n-1)!}{(n-1)!((n-1)-(n-1))!} p^{n-1} q^{(n-1)-(n-1)}\\Big]\\\\\\\\\n\\indent\t= np \\sum\\limits_{x=1}^{n} x {n-1\\choose x-1} p^{x-1}1^{(n-1)-(x-1)}\n\t= \\footnote[2]{Let $y=x-1$ and $n=m+1\\\\\n\t\t\\indent\\indent\\indent\\Rightarrow x=y+1$ and $m=n-1$}\n\t\\ \\ \\sum\\limits_{y=0}^{m} (y+1) {m \\choose y} p^yq^{m-y}\\\\\\\\\n\\indent = np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} + {m \\choose y} p^yq^{m-y}\\Big]\\\\\\\\\n\\indent\t= np \\Big[ \\sum\\limits_{y=0}^{m} y {m \\choose y} p^yq^{m-y} \n\t\t+ \\sum\\limits_{y=0}^{m} {m \\choose y} p^yq^{m-y}\\Big]\\\\\\\\\n\\indent\t= \\footnote[3]{$\\sum\\limits_{y=0}^{m}y{m \\choose y}p^yq^{m-y}$\n\t\tis of the form of the expected value of $Y$, and $E(Y)=mp=(n-1)p$\\\\\n\t\t$\\indent\\ \\sum\\limits_{y=0}^{m}{m \\choose y}p^yq^{m-y}$\n\t\tis the sum of all probabilities over te domain of $Y$ which is 1.}\n\t\\ \\ np(mp+1)\n\t= np[(n-1)p+1]\n\t=np(np-p+1) \n\t=n^2p^2 - np^2 + np\\\\\\\\$\n\\\\\n\\\\\n$\\mu\n\t= E(X)\n\t= np\\\\\\\\$\n$\\sigma^2\n\t= E(X^2) - E(X)^2\n\t= n^2p^2 - np^2 + np - n^2p^2\n\t= -np^2 + np\n\t= np(-p-1)\\\\\n\\indent\t= np(1-p)$\n\n%new Section\n\\newpage\n\\section{Moment Generating Function}\n\\label{Binomial3}\n$\\displaystyle M_X(t)=E(e^{tX})=\\sum\\limits_{x=0}^{n}e^{tx}p(x)\n\t=\\sum\\limits_{x=0}^{n}e^{tx}{n\\choose x}p^x(1-p)^{n-x}\\\\\\\\\n\\indent=\\sum\\limits_{x=0}^{n}{n\\choose x}e^{tx}p^x(1-p)^{n-x}\n\t=\\sum\\limits_{x=0}^{n}{n\\choose x}(pe^{tx})^x(1-p)^{n-x}\\\\\\\\\n\\indent=\\footnote[4]{By the Binomial Theorem (Theorem \\ref{BinomialTheor1.2})\n\t$\\sum\\limits_{x=0}^{n}{n\\choose x}b^xa^{n-x}=(a+b)^n$}\n\t\\ \\ [(1-p)+pe^t]^n$\\\\\\\\\n\\\\\n$M_X^{(1)}(t)=n[(1-p)+pe^t]^{n-1}pe^t\\\\\\\\$\n$M_X^{(2)}(t)=n[(1-p)+pe^t]^{n-1}pe^t+n(n-1)[(1-p)+pe^t]^{n-2}(pe^t)^2\\\\\\\\\n\\indent=npe^t[(1-p)+pe^t]^{n-1}+n(n-1)pe^{2t}[(1-p)+pe^t]^{n-2}\\\\\\\\$\n\\\\\n\\\\\n$E(X)=M_X^{(1)}(0)=n[(1-p)+pe^0]^{n-1}pe^0=n[1-p+p^{n-1}p\\\\\\\\\n\\indent=n(1)^{n-1}p=np\\\\$\n\\\\\n$E(X^2)=M_X^{(2)}(0)=npe^0[(1-p)+pe^0]^{n-1}\n\t+n(n-2)pe^{2\\cdot0}[(1-p)+pe^0]^{n-2}\\\\\\\\\n\\indent=np(1-p+p)^{n-2}+n(n-1)p^2(1-p+p^{n-2}\\\\\\\\\n\\indent=np(1)^{n-1}+n(n-1)p^2(1)^{n-2}=np+n(n-1)p^2=np+(n^2-n)p^2\\\\\\\\\n\\indent=np+n^2+n^2p^2-np^2\\\\\\\\$\n\\\\\n\\\\\n$\\mu=E(X)=np\\\\\\\\$\n$\\sigma^2=E(X^2)-E(X)^2=np+n^2p^2-np^2-n^2p^2=np-np^2\\\\\\\\\n\\indent=np(1-p)\\\\$\n\n%New Section\n\n\\section{Maximum Likelihood Estimator}\n\\label{Binomial4}\nSince $n$ is fixed in each Binomial experiment, and must therefore be given, it is unnecessary to develop an estimator for $n$.  The mean and variance can both be estimated from the single parameter $p$.\\\\\n\\\\\nLet $X$ be a Binomial random variable with parameter $p$ and $n$ outcomes $(x_1,x_2,\\ldots,x_n)$.  Let $x_i=0$ for a failure and $x_i=1$ for a success.  In other words, $X$ is the sum of $n$ Bernoulli trials with equal probability of success and $X=\\sum\\limits_{i=1}^{n}x_i$.\n\n\\subsection{Likelihood Function}\n\\label{Binomial4.1}\n$\\displaystyle L(\\theta)\n\t= L(x_1,x_2,\\ldots,x_n|\\theta)\n\t= P(x_1|\\theta) P(x_2|\\theta) \\cdots P(x_n|\\theta)\\\\\\\\\n\\indent = [\\theta^{x_1}(1-\\theta)^{1-x_1}] [\\theta^{x_2}(1-\\theta)^{1-x_2}] \\cdots [\\theta^{x_n}(1-\\theta)^{1-x_n}]\\\\\n\\indent\t= \\exp_\\theta\\bigg\\{\\sum\\limits_{i=1}^{n}x_i\\bigg\\} \\exp_{(1-\\theta)}\\bigg\\{n-\\sum\\limits_{i=1}^{n}x_i\\bigg\\}\\\\\n\\indent = \\theta^X(1-\\theta)^{n-X}$\n\n\\subsection{Log-likelihood Function}\n\\label{Binomial4.2}\n$\\ell(\\theta)\n\t= \\ln L(\\theta)\n\t= \\ln\\big(\\theta^X(1-\\theta)^{n-X}\\big)\n\t= X\\ln(\\theta)+(n-X)\\ln(1-\\theta)\\\\$\n\n\\subsection{MLE for $p$}\n\\label{Binomial4.3}\n$\\displaystyle \\frac{d\\ell(p)}{d p}\n\t= \\frac{X}{p}-\\frac{n-X}{1-p}\\\\\n\\\\\n0 = \\frac{X}{p}-\\frac{n-X}{1-p}\\\\\\\\\n\\indent\\Rightarrow\\frac{X}{p} = \\frac{n-X}{1-p}\\\\\\\\\n\\indent\\Rightarrow(1-p)X = p(n-X)\\\\\\\\\n\\indent\\Rightarrow X-pX = np-pX\\\\\\\\\n\\indent\\Rightarrow X = np\\\\\\\\\n\\indent\\Rightarrow\\frac{X}{n} = p\\\\\\\\$\n\\\\\nSo $\\displaystyle \\hat p = \\frac{X}{n} = \\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i$ is the maximum likelihood estimator for $p$.\n\n%New Section\n\\section{Theorems for the Binomial Distribution}\n\\label{Binomial5}\n\n%New Subsection\n\\subsection{Validity of the Distribution}\n\\label{Binomial5.1}\n$\\displaystyle \\sum\\limits_{x=0}^n{n\\choose x}p^x(1-p)^{n-x}\n\t= 1$\\\\\n\\\\\n\\it Proof: \\rm \\\\\n\\\\\n$\\displaystyle \\sum\\limits_{x=0}^n {n\\choose x} p^x (1-p)^{n-x}\n\t= \\footnote[1]{$\\sum\\limits_{x=0}^n {n\\choose x} a^x b^{n-x} \n\t\t= (a+b)^n$. See \\ref{BinomialTheor1.2}.}\n\t\\ \\ \\big(p + (1-p)\\big)^n\n\t= (1)^n\n\t= 1$ \\ \\rule{.05in}{.05in}\n\n%new Subsection\n\\subsection{Sum of Binomial Random Variables}\n\\label{Binomial5.2}\nLet $X_1,X_2,\\ldots,X_k$ be independent random variables where $X_i$ comes from a Binomial distribution with parameters $n_i$ and $p$.  That is \n\t$X_i\\sim(n_i,p)$.\\\\\nLet $Y = \\sum\\limits_{i=1}{k} X_i$.  Then $Y\\sim$Binomial$(\\sum\\limits_{i=1}^{k}n_i,p)$.\\\\\\\\\n\\\\\n\\it Proof: \\rm \\\\\n\\\\\n$\\displaystyle M_Y(t)\n\t= E(e^{tY})\n\t= E(e^{t(x_1 + X_2 + \\cdots + X_k)}\n\t= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_k})\\\\\\\\\n\\indent = E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_k})\\\\\n\\indent = \\prod\\limits_{i=1}^{k} [(1-p)+pe^t]^{n_i}\n\t= [(1-p)+pe^t]^{\\sum\\limits_{i=1}^{k}n_i}\\\\$\n\\\\\n\\\\\nWhich is the mgf of a Binomial random variable with parameters \n\t$\\sum\\limits_{i=1}^{k}n_i$ and \\nolinebreak[4]$p$.  \n\tThus $Y\\sim$Binomial$(\\sum\\limits_{i=1}^{k}n_i,p)$.\\rule{.05in}{.05in}\n\n%New Subsection\n\\newpage\n\\subsection{Sum of Bernoulli Random Variables}\n\\label{Binomial5.3}\nLet $X_1,X_2,\\ldots,X_n$ be independent and identically distributed random variables from a Bernoulli distribution with parameter $p$.  Let $Y = \\sum\\limits_{i=1}^{n}X_i$.\\\\\nThen $Y\\sim$Binomial$(n,p)$\\\\\n\\\\\n\\it Proof: \\rm\\\\\n$M_Y(t)\n\t= E(e^{tY})\n\t= E(e^{tX_1} e^{tX_2} \\cdots e^{tX_n})\n\t= E(e^{tX_1}) E(e^{tX_2}) \\cdots E(e^{tX_n})\\\\\n\\indent = (pe^t+(1-p))(pe^t+(1-p))\\cdots (pe^t+(1-p))\n\t= (pe^t+(1-p))^n$\\\\\n\\\\\nWhich is the mgf of a Binomial random variable with parameters $n$ and $p$.  Thus, $Y\\sim$ Binomial$(n,p)$. \\rule{.05in}{.05in}\n",
    "created" : 1470879464838.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3491326400",
    "id" : "39CBFC6D",
    "lastKnownWriteTime" : 1196884270,
    "last_content_update" : 1196884270,
    "path" : "C:/Users/Benjamin/Desktop/Stats Book/TeX Files/Binomial.tex",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "tex"
}